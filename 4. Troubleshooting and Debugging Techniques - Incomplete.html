<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>4. Troubleshooting and Debugging Techniques - Incomplete</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#troubleshooting-and-debugging-techniques">4. Troubleshooting and Debugging Techniques</a>
<ul>
<li><a href="#troubleshooting-concepts">Troubleshooting Concepts</a>
<ul>
<li><a href="#introduction-to-debugging">Introduction to Debugging</a></li>
<li><a href="#what-is-debugging">What is debugging?</a></li>
<li><a href="#problem-solving-steps">Problem Solving Steps</a></li>
<li><a href="#silently-crashing-application">Silently Crashing Application</a></li>
</ul>
</li>
<li><a href="#understanding-the-problem">Understanding the Problem</a>
<ul>
<li><a href="#it-doesnt-work">“It doesn’t work”</a></li>
<li><a href="#creating-a-reproduction-case">Creating a Reproduction Case</a></li>
<li><a href="#finding-the-root-cause">Finding the Root Cause</a></li>
<li><a href="#dealing-with-intermittent-issues">Dealing with Intermittent Issues</a></li>
</ul>
</li>
<li><a href="#binary-searching-a-problem">Binary Searching a Problem</a>
<ul>
<li><a href="#what-is-binary-search">What is binary search?</a></li>
<li><a href="#applying-binary-search-in-troubleshooting">Applying Binary Search in Troubleshooting</a></li>
<li><a href="#finding-invalid-data">Finding Invalid Data</a></li>
<li><a href="#module-1-review">Module 1 Review</a></li>
</ul>
</li>
<li><a href="#understanding-slowness">Understanding Slowness</a>
<ul>
<li><a href="#intro-to-module-2-slowness">Intro to Module 2: Slowness</a></li>
<li><a href="#why-is-my-computer-slow">Why is my computer slow?</a></li>
<li><a href="#how-computers-use-resources">How Computers Use Resources</a></li>
<li><a href="#possible-causes-of-slowness">Possible Causes of Slowness</a></li>
<li><a href="#slow-web-server-example">Slow Web Server Example</a></li>
<li><a href="#monitoring-tools">Monitoring Tools</a></li>
</ul>
</li>
<li><a href="#slow-code">Slow Code</a>
<ul>
<li><a href="#writing-efficient-code">Writing Efficient Code</a></li>
<li><a href="#using-the-right-data-structures">Using the Right Data Structures</a></li>
<li><a href="#expensive-loops">Expensive Loops</a></li>
<li><a href="#keeping-local-results">Keeping Local Results</a></li>
<li><a href="#slow-script-with-expensive-loop">Slow Script with Expensive Loop</a></li>
<li><a href="#more-about-improving-our-code">More About Improving Our Code</a></li>
</ul>
</li>
<li><a href="#when-slowness-problems-get-complex">When Slowness Problems Get Complex</a>
<ul>
<li><a href="#parallelizing-operations">Parallelizing Operations</a></li>
<li><a href="#slowly-growing-in-complexity">Slowly Growing in Complexity</a></li>
<li><a href="#dealing-with-complex-slow-systems">Dealing with Complex Slow Systems</a></li>
<li><a href="#using-threads-to-make-things-go-faster">Using Threads to Make Things Go Faster</a></li>
<li><a href="#more-about-complex-slow-systems">More About Complex Slow Systems</a></li>
<li><a href="#module-2-wrap-up-slowness">Module 2 Wrap Up: Slowness</a></li>
</ul>
</li>
<li><a href="#why-programs-crash">Why Programs Crash</a>
<ul>
<li><a href="#intro-to-module-3-crashing-programs">Intro to Module 3: Crashing Programs</a></li>
<li><a href="#systems-that-crash">Systems That Crash</a></li>
<li><a href="#understanding-crashing-applications">Understanding Crashing Applications</a></li>
<li><a href="#what-to-do-when-you-cant-fix-the-program">What to do when you can’t fix the program?</a></li>
<li><a href="#internal-server-error-example">Internal Server Error Example</a></li>
<li><a href="#resources-for-understanding-crashes">Resources for Understanding Crashes</a></li>
</ul>
</li>
<li><a href="#code-that-crashes">Code that Crashes</a>
<ul>
<li><a href="#accessing-invalid-memory">Accessing Invalid Memory</a></li>
<li><a href="#unhandled-errors-and-exceptions">Unhandled Errors and Exceptions</a></li>
<li><a href="#fixing-someone-elses-code">Fixing Someone Else’s Code</a></li>
<li><a href="#debugging-a-segmentation-fault">Debugging a Segmentation Fault</a></li>
<li><a href="#debugging-a-python-crash">Debugging a Python Crash</a></li>
<li><a href="#resources-for-debugging-crashes">Resources for Debugging Crashes</a></li>
</ul>
</li>
<li><a href="#handling-bigger-incidents">Handling Bigger Incidents</a>
<ul>
<li><a href="#crashes-in-complex-systems">Crashes in Complex Systems</a></li>
<li><a href="#communication-and-documentation-during-incidents">Communication and Documentation During Incidents</a></li>
<li><a href="#writing-effective-postmortems">Writing Effective Postmortems</a></li>
<li><a href="#module-3-crashing-apps-wrap-up">Module 3 Crashing Apps Wrap-up</a></li>
</ul>
</li>
<li><a href="#managing-computer-resources">Managing Computer Resources</a>
<ul>
<li><a href="#intro-to-module-4-managing-resources">Intro to Module 4: Managing Resources</a></li>
<li><a href="#memory-leaks-and-how-to-prevent-them">Memory Leaks and How to Prevent Them</a></li>
<li><a href="#managing-disk-space">Managing Disk Space</a></li>
<li><a href="#network-saturation">Network Saturation</a></li>
<li><a href="#dealing-with-memory-leaks">Dealing with Memory Leaks</a></li>
<li><a href="#more-about-managing-resources">More About Managing Resources</a></li>
</ul>
</li>
<li><a href="#managing-our-time">Managing our Time</a>
<ul>
<li><a href="#getting-to-the-important-tasks">Getting to the Important Tasks</a></li>
<li><a href="#prioritizing-tasks">Prioritizing Tasks</a></li>
<li><a href="#estimating-the-time-tasks-will-take">Estimating the Time Tasks Will Take</a></li>
<li><a href="#communicating-expectations">Communicating Expectations</a></li>
<li><a href="#more-about-making-the-best-use-of-our-time">More About Making the Best Use of Our Time</a></li>
</ul>
</li>
<li><a href="#making-our-future-lives-easier">Making Our Future Lives Easier</a>
<ul>
<li><a href="#dealing-with-hard-problems">Dealing with Hard Problems</a></li>
<li><a href="#proactive-practices">Proactive Practices</a></li>
<li><a href="#planning-future-resource-usage">Planning Future Resource Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="troubleshooting-and-debugging-techniques">4. Troubleshooting and Debugging Techniques</h1>
<h2 id="troubleshooting-concepts">Troubleshooting Concepts</h2>
<h3 id="introduction-to-debugging">Introduction to Debugging</h3>
<p>Whether it’s an application crashing, a hardware issue, or network outage, as IT specialists, we tend to run into problems that need solving pretty regularly.When facing these issues, we need to make sure that people affected by the problem can get back to doing their jobs as fast as possible. We also have to plan for how to prevent against the same problems from happening again in the future.</p>
<p>In this module, we’ll learn some <em>essential debugging techniques</em>. We’ll start with the basic process that we can use for tackling any technical problem. Then we’ll look at different ways that we can approach understanding what’s going on and finding the root cause of an issue, including using a process called <em><strong>binary search</strong></em> to troubleshoot problems. Along the way, we’ll talk about a bunch of examples of real-world problems and how to apply the techniques to solve them.</p>
<p>The techniques we’ll look at are reusable and we’ll let you solve almost any technical problem that you might face in the future. We’ll keep applying them throughout the course as we explore different issues that can affect us or the users we’re supporting in different ways. As with any other skills that you’ve learned throughout this program, the best way to get good at something is practice. So at the end of the module, you’ll have the opportunity to apply these techniques and try solving a technical issue yourself on a virtual machine running Linux. Finding the solution to a problem can sometimes take a really long time. While I don’t know if anyone loves troubleshooting, I definitely enjoy solving problems. Dealing with software is like trying to solve one giant puzzle. It can sometimes be frustrating when we can’t find the right pieces. But it’s super exciting when everything finally fits together. If everything worked right away, then it wouldn’t be fun anymore.</p>
<h3 id="what-is-debugging">What is debugging?</h3>
<p>Throughout this course, we’ll be talking about debugging and troubleshooting. So what’s the difference between them?</p>
<p><strong>We say that troubleshooting is the process of identifying, analyzing, and solving problems</strong>. We can use the term troubleshooting to refer to solving any kind of problem. In this course, we’ll focus on <em>troubleshooting IT-related problems</em>. They could be <em>problems caused by hardware, the operating system, or applications running on the computer</em>. They could also be caused by the <em>environment</em> and <em>configuration</em> of the software. The <em>services</em> the application is interacting with, or a wide range of other possible IT causes.</p>
<p><strong>On the flip side, debugging is the process of identifying, analyzing, and removing bugs in a system</strong>. We sometimes use troubleshooting and debugging interchangeably. But generally, we say <em>troubleshooting when we’re fixing problems in the system running the application</em>, and <em>debugging when we’re fixing the bugs in the actual code of the application</em>.</p>
<p>There are lots of tools that we can use to get more information about the system and what the programs in our system are doing. Tools like <strong>tcpdump</strong> and <strong>Wireshark</strong> can show us <em>ongoing network connections</em>, and help us analyze the traffic going over our cables. Tools like <strong>ps</strong>, <strong>top</strong>, or <strong>free</strong> can show us the number and types of resources used in the system. We can use a tool like <strong>strace</strong> to look at the system calls made by a program, or <strong>ltrace</strong> to look at the library calls made by the software. Don’t worry about memorizing them, we’ll talk about each in detail in our real-world examples.</p>
<p>When debugging the code of a program, we can combine these tools with specific debugging tools developed for the programming language used to write the application. <strong>Debuggers let us follow the code line by line, inspect changes in variable assignments, interrupt the program when a specific condition is met, and more</strong>. On top of that, <em>if we can modify the code, we can change it so that it provides more logging information</em>. This can help us understand what’s going on behind the scenes.</p>
<p>Both troubleshooting and debugging are a bit of an art. In those fortunate cases when you’ve seen the problem before, you might immediately know what the solution is. But <em>usually, figuring out the problem and its solution require some creativity</em>. We need to come up with new ideas of what could be failing, and ways to check for that. And once we know what’s failing, we need to imagine how to solve it. To take it a step further, once we’ve solved a problem, we can start thinking about how to prevent it from happening again. <em>When troubleshooting or debugging, we come across surprising situations</em>. Things aren’t working as expected, and we need to understand why, and figure out how to solve it. As we called out, in this course, we’ll look into a bunch of different techniques to understand and solve technical problems. While we’ll sometimes focus on system side and sometimes on the coding side, most of the techniques that we’ll cover can help us solve any technical problem.</p>
<h3 id="problem-solving-steps">Problem Solving Steps</h3>
<p>There’s a wide range of different technical problems that you might face as an IT specialist or systems administrator. But fortunately, there’s a set of steps that you can usually take to solve almost any technical problem.</p>
<ol>
<li>
<p><strong>The first step is getting information</strong>. This means <em>gathering</em> as much <em>information</em> as we need about the <em>current state</em> of things, what the <em>issue</em> is, <em>when</em> it happens, and what the <em>consequences</em> are, for example. To get this information, we can use any <em>existing documentation</em> that might help. This can be <em>internal documentation, manual pages, or even questions asked on the Internet</em>. One super important resource to solve a problem is the <em><strong>reproduction case</strong>, which is a clear description of how and when the problem appears</em>.</p>
</li>
<li>
<p><strong>The second step is finding the root cause of the problem</strong>. This is <em>usually the most difficult step</em>. Throughout this course, we’ll discuss a lot of possibilities on how to get there. But the key here is to get to the bottom of what’s going on, what triggered the problem, and how we can change that.</p>
</li>
<li>
<p><strong>The final step is performing the necessary remediation</strong>. Depending on the problem, this might include an <em>immediate remediation</em> to get the system back to health, and then a <em>medium or long-term remediation</em> to avoid the problem in the future.</p>
</li>
</ol>
<p>While these are three basic steps of problem-solving, <em>they don’t always happen sequentially</em>. It’s pretty common that while trying to find the root cause, we discover that we need even more info about the current state. So we gather more information until we find the answer we’re looking for, or we could understand the problem just enough to create a <em><strong>workaround</strong></em> that lets our users get back to work quickly, but we’d still need more time to get to the <em><strong>root cause</strong></em> and prevent the problem from happening again.</p>
<p>Preventing the problem from occurring can sometimes feel like a hassle, but it can actually save us and our users a lot of valuable time. This way we avoid having to solve the same problem over and over again. Throughout the whole process, <strong>it’s important that we document what we do</strong>. We should note down the <em>info that we get, the different things we tested to try, and figure out the root cause, finally, the steps we took to fix the issue</em>. This documentation might prove invaluable next time a similar issue pops up.</p>
<p>Imagine a user asks you for your help because their computer is unexpectedly shutting down. Computers shouldn’t shut down on their own, but the problem could be a hardware issue, a software issue, or even a configuration issue. So the first thing to do is to get more information. You’ll want to know things like when it happened, what the user was doing when it happened, and how regularly it’s happening. You’ll also want to look at the computer logs to check if there are any interesting errors. If any aren’t totally clear, you can look them up on the Internet to see what they mean. In our example, safe on a line in the logs that says the temperature threshold was exceeded and so the computer shutdown. That’s useful information, you know why the computer shut down but you don’t know why it overheated, so you’ll need to keep investigating. After not finding anything else interesting in the logs, you decide to check if it’s a hardware issue. When you open up the computer, you find that the fan that’s supposed to cool down the CPU is full of dirt, and so it isn’t spinning properly. That’s the root cause of the problem. Now, the short-term remediation is to clean up the fan so that it can spin again and the computer doesn’t overheat. But what’s the long-term remediation? In this case, it would be deploying monitoring on the computers to make sure you get notified early when they’re overheating. Long-term remediation would also include checking if you can reduce the amount of dust in the air so that there’s less chance of this happening again.</p>
<h3 id="silently-crashing-application">Silently Crashing Application</h3>
<p>Say a user contacts us to let us know that a certain application fails to open. As we called out earlier the first thing to do is to get more information about the conditions that caused the failure. What the error is that the user is getting and then check if we experience the same failure. By asking for these details, we discover that a new version of the software was recently released. And when we upgrade to this new version, we can reproduce the problem on our own computer like this.</p>
<p>We see that when we try to run the program it prints no error at all. It just exits immediately. We need to figure out what’s going on. Even if there’s no error message.</p>
<p>There are a bunch of tools that can help us better understand what’s going on with the system and with our applications. With the help of these tools, we can extend our knowledge of a particular problem view the actions of the program from a different point of view and get the info we need. Among these tools <strong>strace</strong> lets us look more deeply at what the program is doing. It will trace a system calls made by the program and tell us what the result of each of these calls was. So to figure out what’s up with our program that’s failing to open will s trace the failing application.</p>
<p>Whoa, that’s a lot of output the A strois command shows us all the system calls are program made. System calls are the calls that the programs running on our computer make to the running kernel. There are loads of different system calls and depending on what we’re trying to debug. We might be interested in some more than others. If you want to understand what these system calls are. You can read more about each of them in the corresponding manual pages. But before we jump into that, let’s make this output more manageable. We could pipe the output to the less command which we could use to scroll through a lot of texts or we could use the- 0 flag of the <strong>strace</strong> command to store the output into a file and then browse the contents of that file. The- 0 flag, lets us refer back to the file later if we need to so, let’s go with that one.</p>
<p>Okay, now we can read the generated file using whichever program we prefer. Let’s open it with less will go to the end of the file pressing Shift G then scroll up to see if we find anything suspicious.</p>
<p>All right close to the end of the log we can see that the application tries to open a directory called .config purple box, which doesn’t exist. Let’s look at this line in a bit more detail. The name of the system call is open at. One of the calls used to open files or directories. The content of the call shows the parameters passed including the path that’s being opened and a bunch of flax. In particular the 0 directory flag tells us that the program is trying to open this path as a directory. The number after the equal sign shows us the return code of the sys call. In this case it’s negative one. So the program is trying to open this directory and it turns out it doesn’t exist.</p>
<p>Since this is happening shortly before the program finishes it’s a likely candidate for the root cause of the issue. Let’s create the directory and try to start the program again. Purple box, success this time the program works just fine. Let’s recap what we did first. We got some information from a user telling us there was a change in the new version which was causing a problem. To investigate this we reproduce the problem on our own computer. Then we got more information on what was going on by using the <strong>strace</strong> tool which lets us see the system calls made by a program. We found a suspicious error. That’s that a directory didn’t exist. We created the directory to check out what would happen when it did exist and the program work correctly. So we’ve identified the root cause of the problem which is the missing directory. Now, we can go ahead and remediate the issue. The immediate remediation is to tell the user to create the directory so that they can get back to work quickly. The long-term remediation is to contact the developers of the software to let them know that the program will fail to start if the director is missing. This gives them a heads up about the problem so they can fix it in the next version. And what about the documentation we should note that this version of the software won’t start if that directory doesn’t exist. That will help others facing the same issue to quickly find out the solution.</p>
<p>In this example, we were able to use <strong>strace</strong> and quickly identify what the problem was but it won’t always be this easy. Throughout this course, we’ll keep looking at more tools and more ideas to help us work through what’s going on when it’s not immediately obvious. Coming up we’ve got the first practice quiz of the course. These quizzes will help make sure that all the concepts that we’ve covered thus far make sense. Remember you can always review the content if it wasn’t clear. I’ll meet you in the next video.</p>
<h2 id="understanding-the-problem">Understanding the Problem</h2>
<h3 id="it-doesnt-work">“It doesn’t work”</h3>
<p>As we called out, <em>the first step to solving a problem is getting enough information so that we can understand the current state of things</em>. To do this we’ll need to know <em>what the actual issue we’re solving is</em>. This starts when we first come across the issue, which can be through report by a ticketing system or by encountering the problem ourselves. When working with users, it’s pretty common to receive reports of failures that just boil down to, <em>“It doesn’t work.”</em> These reports usually don’t include a lot of useful information but <em>it’s still important that the problem gets reported and solved</em>. Which information is useful or not might depend on the problem. But there are some common questions that we can ask a user that simply report something doesn’t work:</p>
<ul>
<li>
<p>What were you trying to do?</p>
</li>
<li>
<p>What steps did you follow?</p>
</li>
<li>
<p>What was the expected result?</p>
</li>
<li>
<p>What was the actual result?</p>
</li>
</ul>
<p>If the ticketing system your company uses allows this, it’s a good idea to include these questions in the form that users have to fill out when reporting an issue. This way we save time and can start asking more specific questions right away. Otherwise, these are almost always going to be the first questions you ask.</p>
<p>Another thing to keep in mind is that <em>when debugging a problem</em>, <em>we want to consider the simplest explanations first</em> and <em>avoid jumping into complex or time-consuming solutions unless we really have to</em>. That’s why when a device doesn’t turn on, we first check if it’s correctly plugged in and that there’s electricity coming from the plug before taking it apart or replacing it with a new device.</p>
<p>Say you got a call from a user that tells you the internal website used by the sales team to track customer interactions doesn’t work. The user is super stressed because they need to access the information on the website for a meeting happening in a few minutes. So you tell them that you’ll look into the problem right away, but then you need more information. <em>What were they trying to do?</em> The user tells you that they’re trying to access the website. What steps did they follow? They tell you that they opened the website URL and entered their credentials. What was the expected result? They expected to see the sales system’s landing page. <em>What did they get instead?</em> The web page just keeps loading. It stays blank forever. Okay. So now you’ve gone from, “it doesn’t work,” to, “when I tried to log in, the page keeps loading and never shows the landing page.” That’s great! Now that you have a basic idea of what the problem is, it’s time to start figuring out the <em>root cause</em>. For that, you’ll apply a <em>process of elimination, starting with the simplest explanations first and testing those until you can isolate the root cause</em>.</p>
<p>For example, you check if you can reproduce the issue on your own computer. So you navigate to the website, enter your credentials, and sure enough, the page just keeps loading, never showing the landing page. This is enough information that you can tell the user that you’ll work on it and investigate on your own. There’s no need to keep them on the line. By reproducing the problem on your computer, you’ve taken a simple and quick action that rules out the user or the user’s computer as the cause of the problem. This cuts the troubleshooting process in half since you now know there’s a problem with the service and you can focus on solving that. Before jumping into the server that’s hosting the application, you run a few quick checks to verify if the problem is isolated to that specific website or not. You check if your Internet access is working successfully by accessing an external website which loads just fine. Then you check if other internal websites, like the inventory website or ticketing system are working okay. Doing this, you discover that while the ticketing system loads with no issues, the inventory website never finishes loading. It turns out both websites are hosted on the same server. Again, it’s important to highlight that doing these quick checks to verify that the Internet works correctly and which sites are affected by the problem, helps you isolate the root cause. <em>By looking at possible simple explanations first, you avoid losing time chasing the wrong problem</em>. At this point, you know that website’s running on a specific server or failing to load while the rest of the systems and the Internet are working correctly.</p>
<p>Next up, you need to check what’s going on on that server. The server running the websites is a Linux machine, so you’ll connect to it using <em>SSH</em>. You run the <strong>top</strong> command which shows the state of the computer and processes using the most CPU and see that the computer is super overloaded. The <em>load average</em> in the first line says 40. <em>The load average on Linux shows how much time a processor is busy in a given minute, with one meaning it was busy for the whole minute. So normally this number shouldn’t be above the amount of processors in the computer. A number higher than the amount of processors means the computer is overloaded.</em> You know this computer has four cores, so 40 is a really high number. You also see that most of the CPU time is spent in waiting. This means that processes are stuck waiting for the operating system to return from system calls. This usually happens when processes get stuck gathering data from the hard drive or the network. By looking at the list of processes, you realize that the backup system is currently running on the server, and it seems to be using a lot of processing time. Backing up the data on the system is super important. But currently, the whole system is unusable. So you decide to stop the backup system by calling kill-stop. This will suspend the execution of the program until you let it continue or decide to terminate it.</p>
<p>After doing this, you’re on top once again and you see that the load is going down, and so processes are no longer stuck waiting for I/O. Then you try logging into the website, and this time the landing page loads. Success! You let the user know that they can use the website once again.</p>
<p>At this point, you’ve applied the <em>immediate remediation</em>. We’ll talk about long-term remediation in a later section. Before moving on to the next topic, imagine that the following week another user calls you and tells you the sales website doesn’t work. Remembering the previous incident, you tell them you’ll fix it right away. You SSH onto the server and try to find the backup process to stop it, but it’s not running. Oops! You forgot to ask the user what they meant when they said it didn’t work. When you call back to ask them they tell you that they’re trying to generate a monthly sales report and they get an error saying the product category column doesn’t exist. Totally different problem, totally different actions to take. So <em><strong>remember to always have a clear picture of what the problem is before you start solving it</strong></em>.</p>
<h3 id="creating-a-reproduction-case">Creating a Reproduction Case</h3>
<p>When we’re dealing with an issue that’s tricky to debug, we want to have a <em>clear reproduction case</em> for the problem. <strong>A reproduction case is a way to verify if the problem is present or not</strong>. We want to make <em>the reproduction case as simple as possible</em>. That way, we can clearly understand when it happens, and it makes it really easy to check if the problem is fixed or not, when we try to solve it.</p>
<p>Sometimes, the reproduction case is pretty obvious. In our example where the program fail to start because of a missing directory, the reproduction case was to open the program without that directory on the computer. On our overloaded server example, the reproduction case for the failure was to try to login to the website and see the loading page.</p>
<p>But sometimes the reproduction case might be much more complex to discover. Imagine you’re trying to help a user with an application that won’t start. This time when you run the same version of the application on your computer, the application starts just fine. So you suspect that the problem has to do with something in the <em>user’s environment or configuration</em>. There could be a bunch of reasons why this could happen. It could be problems with the <em>network routing</em>, <em>old config files interfering</em> with a new version of the program, a <em>permissions problem</em> blocking the user from accessing some required resource, or <em>even some faulty piece of hardware</em> acting out. So how can you figure out what’s causing the problem?</p>
<ul>
<li>
<p>The first step is to read the <em><strong>logs</strong></em> available to you. Which logs to read, will depend on the operating system and the application that you’re trying to debug.</p>
</li>
<li>
<p>On <strong>Linux</strong>, you’d read system logs like /var/log/syslog and user-specific logs like the .xsession-errors file located in the user’s home directory.</p>
</li>
<li>
<p>On macOS, on top of the system logs, you’d go through the logs stored in the /Library/logs directory.</p>
</li>
<li>
<p>On <em><strong>Windows</strong></em>, you’d use the <em>Event Viewer</em> tool to go through the event logs.</p>
</li>
</ul>
<p>No matter the operating system, remember to look at the logs when something isn’t behaving as it should. Lots of times, you’ll find an error message that will help you understand what’s going on like, unable to reach server, invalid file format, or permission denied.</p>
<p>But what if you’re unlucky, and there’s no error message, or the error message is super unhelpful like internal system error?</p>
<ul>
<li>The next step is to try to isolate the conditions that trigger the issue. Do other users in the same office also experienced the problem? Does the same thing happen if the same user logs into a different computer? Does the problem happen if the applications config directory is moved away?</li>
</ul>
<p>Let’s say that it’s the config directories file. You ask the user to move it away without deleting it, and now the application starts correctly. So you ask the user to send you the contents of that directory. You copy them onto your computer, and the program fails to start. Bingo, you got your reproduction case. It’s starting the program with that config in place.</p>
<p>Having a clear reproduction case let us do investigate the issue, and quickly see what changes it.</p>
<p>For example, does the problem go away if you revert the application to the previous version? Are there any differences in the <em><strong>strace</strong></em> log, or the <em><strong>ltrace</strong></em> logs when running the application with the bug config and without it?</p>
<p>On top of that, having a clear reproduction case, lets you share with others when asking for help. As long as you aren’t sharing any confidential information of course, you could use it to report a bug to the applications developers, to ask for help from a colleague, or even to ask for help from an Internet forum about the application if it’s publicly available. So when trying to create a reproduction case, we want to find the actions that reproduce the issue, and we want these to be as simple as possible. The smaller the change in the environment and the shorter the list of steps to follow, the better. To get there, we might need to dig deeper into the problem until we have a small enough set of instructions. Once you have a reproduction case, you’re ready to move on to the next step, finding the root cause.</p>
<h3 id="finding-the-root-cause">Finding the Root Cause</h3>
<p>When you first come across these concepts, it might seem that once you have a reproduction case, you already know the root cause of the problem. But more often than not, it’s not true.</p>
<p>In our overloaded server example, we figured out that the backup system was blocking the websites from working, and so we mitigated that immediate problem to unblock the user. But we didn’t really look into the root cause of our server being stuck. This could be because the <em>network bandwidth is saturated, the disk transfer is too slow, the hard drive is faulty, or a bunch of other reasons</em>. We also didn’t do anything to make sure our backups could run successfully in the future.</p>
<p><strong>Understanding the root cause is essential for performing the long-term remediation</strong>. So how do we go about finding the actual root cause of the problem?</p>
<p>We generally follow a <em>cycle</em> of <em>looking at the information we have, coming up with a hypothesis that could explain the problem, and then testing our hypothesis</em>. If we confirm our theory, we found the root cause. If we don’t, then we go back to the beginning and try different possibility. This is where our problem-solving creativity comes into play. We need to come up with an idea of a possible cause, check if it’s correct and if not, come up with a different idea until we find one that explains the problem.</p>
<p>Our ideas don’t come out of a void. To get inspired, we look at information we currently have and gather more if we need. Searching online for the error messages that we get or looking at the documentation of the applications involved can also help us imagine new possibilities of what might be at fault.</p>
<p><em>Whenever possible, we should check our hypothesis in a test environment</em>, instead of the production environment that our users are working with. That way, we avoid interfering with what our users are doing and we can tinker around without fear of breaking something important.</p>
<p>Depending on what you’re trying to fix, this might mean we have to try our code in a newly installed machine, spinning up a test server, using test data, and so on. It can take some time to get the setup, but the extra safety is definitely worth it. Even when it seems that the error might be related to the specific production environment, <em>it’s always a good idea to check if we can reproduce the problem in a test environment before we modify production</em>.</p>
<p>In our overloaded server example, if the problem is with the hardware, we wouldn’t be able to replicate it with a test server. In that case, we would need to either wait until the services aren’t being used or bring up a secondary server, migrate the services there, and only then look at what’s wrong with the computer. On the flip side, if the problem is related to some configuration of either the web services or the backup service, we’d still see it in the test server.</p>
<p>So we’d always start by setting up a test instance of the service and checking if the problem replicates there before touching the production instance. So say we have a test server running the same websites. When we start the backup, we see that the website stop responding. This is great because we have re-production case, and we can debug it properly.</p>
<p>How do we find the root cause? One possible culprit could be too much disk input and output. To get more info on this, we could use <strong>iotop</strong>, which is a tool similar to <strong>top</strong> that lets us see which processes are using the most input and output. Other related tools are <strong>iostat</strong> and <strong>vmstat</strong>, these tools show statistics on the input/output operations and the virtual memory operations. If the issue is that the process generates too much input or output, we could use a command like <strong>ionice</strong> to make our backup system reduce its priority to access the disk and let the web services use it too.</p>
<p>What if the input and output is not the issue? Another option would be that the service is using too much network because it’s transmitting the data to be backed up to a central server and that transmission blocks everything else. We can check this using <strong>iftop</strong>, yet another tool similar to <strong>top</strong> that shows the current traffic on the network interfaces. If the backup is eating all the network bandwidth, we could look at the documentation for the backup software and check if it already includes an option to limit the bandwidth. The <strong>rsync</strong> command, which is often used for backing up data, includes a <strong>-bwlimit</strong>, just for this purpose. If that option isn’t available, we can use a program like <strong>Trickle</strong> to limit the bandwidth being used. But what if the network isn’t the issue either?</p>
<p><em><strong>Remember, we need to put our debugging creativity to work, and come up with other possible reasons for why it’s failing</strong></em>.</p>
<p>Another option could be that the compression algorithms selected is too aggressive, and compressing the backups is using all of the server’s processing power. We could solve this by reducing the compression level or using the <strong>nice</strong> command to reduce the priority of the process and accessing the CPU. If that’s still not the case, we need to keep looking, check the logs to see if we find anything that we missed before. Maybe look online for other people dealing with similar problems related to interactions of the backing up software with the web surfing software, and keep doing this until we come up with something that could be causing our problem. This may sounds like a lot of work, but it’s usually not that bad. In general, by using the tools available to us, we can find enough info to land on the right hypothesis after only a few tries and with experience, we’ll get better at picking up the most likely hypothesis the first time around.</p>
<h3 id="dealing-with-intermittent-issues">Dealing with Intermittent Issues</h3>
<p>Have you ever tried to solve a problem that happened only occasionally? Maybe you’ve dealt with programs that randomly crash, laptops that sometimes fail to suspend, web services that unexpectedly stop replying, or file contents that get corrupted. But only in some cases, bugs that come and go are hard to reproduce, and are extremely annoying to debug?</p>
<p>If you work in IT, you’ve probably had your own dose of frustration while dealing with intermittent and issues. So what can you do if you’re trying to debug an issue like that?</p>
<ul>
<li>The first step is to get more involved in what’s going on, so that you understand when the issue happens and when it doesn’t. If you’re dealing with a bug and a piece of code that you maintain, you can usually modify the program to log more information related to the problem. Since you don’t know exactly when the bug will trigger, you need to be thorough with the information that you log. If you can’t modify the code of the program to get more information, check if there’s a logging configuration that you can change. Many applications and services already include a debugging mode that generates a lot more output then the default mode. By enabling the debug information in advance, you can get a better picture of what’s going on the next time the problem happens. If that’s not possible, you’ll need to resort to <strong>monitoring</strong> the environment when the issue triggers.</li>
</ul>
<p>Depending on what the problem is, you might want to look at different sources of information, like the <em>load on the computer, the processes running at the same time, the usage of the network, and so on</em>. For bugs that occur at random times, we need to prepare our system to give us as much information as possible when the bug happens. This might require several iterations until we get enough information to understand the issue, but don’t lose hope. Most of the time, you can finally get to the point where you can actually understand what’s going on.</p>
<p>Sometimes, the bug goes away when we add extra logging information, or when we follow the code step by step using a debugger. This is an especially annoying type of intermittent issue, nicknamed <em><strong>Heisenbug</strong></em>, in honor of <em>Werner Heisenberg</em>. He’s the scientist that first described the <em><strong>observer effect, where just observing a phenomenon alters the phenomenon</strong></em>. <strong>Heisenbugs</strong> are extra hard to understand, because when we meddle with them, the bug goes away. <em>These bugs usually point to bad resource management</em>.</p>
<p>Maybe the memory was wrongly allocated, the network connections weren’t correctly initialized, or the open files weren’t properly handled. In these cases, we usually need to just spend time looking at the affected code until we finally figure out what’s up.</p>
<p>Yet another type of intermittent issue is the one that goes away when we turn something off and on again. There’s plenty of jokes related to how, in IT, a lot of what we do to solve problems, is just turn things off and on again. Okay, it’s true that in many cases, power cycling a device or restarting a program gets rid of whichever problem we were trying to fix. But why is that?</p>
<p><em>When we reboot a computer or restart a program, a bunch of things change</em>. Going back to a clean slate means <em>releasing all allocated memory, deleting temporary files, resetting the running state of programs, re-establishing network connections, closing open files and more</em>. <em><strong>If a problem goes away by turning it off and on again, there’s almost certainly a bug in the software, and the bug probably has to do with not managing resources correctly</strong></em>. So if an issue goes away after a restart, it’s a good idea to try to figure out why that is, and see if it’s possible to fix it in a way that doesn’t require turning it off and on again. If in the end, we can’t find the actual reason, scheduling a restart at a time that’s not problematic can also be an option.</p>
<p>So we’ve looked at a few ways of getting to the root cause of a problem, like isolating causes, understanding error messages, adding logging information, and generating new ideas for possible failures. We’ve also talked about problems that go away on their own and then pop up again, and looked at how to figure those out.</p>
<h2 id="binary-searching-a-problem">Binary Searching a Problem</h2>
<h3 id="what-is-binary-search">What is binary search?</h3>
<p>Usually when trying to find the <em>root cause of a problem</em>, we’ll be looking for one answer in a list of many. <em>Searching for an element in a list is a common problem in computing</em>. There are a bunch of different algorithms that can help us find the element that we’re looking for. Say for example, you have a list that contains the data of employees that work at your company and you want to find one specific employee.</p>
<ul>
<li>
<p>One possible approach would be to start from the first entry and then check if the name is the one that we’re looking for. If it doesn’t match, move to the second element and check again, and keep going until we find the employee with the name we’re looking for, or we get to the end of the list. This is called a <em><strong>linear search</strong></em>. This type of search works but <em>the longer the list, the longer it can take</em>. In other words, <em>the time it takes to find the result is proportional to the length of the list</em>.</p>
</li>
<li>
<p>If the list is sorted, we can use an alternative algorithm for searching called <em><strong>binary search</strong></em>. Because the list is sorted, we can make decisions about the position of the elements in the list. So the first thing we do is compare the name that we’re looking for with the element in the middle of the list and check if it’s equal, smaller, or bigger. If it’s smaller, we know that the element we’re looking for must be in the first half of the list. On the flip side, if it’s bigger, we know that it’s in the second half of the list. This way, <em>with only one comparison, we’ve eliminated half of the list from possible candidates where the element could have been found, and then we do the same thing again and again until we find the element</em>.</p>
</li>
</ul>
<p>So if the element we were looking for was smaller than the middle element, we look at the element in the middle of the first half. If our element is now bigger, we look at the element in the middle of the second quarter, and so on. Each time we look at the middle element of the section we’re dealing with, until we find the element we’re looking for. Using linear search, going through a list with 1000 elements might take up to 1,000 comparisons if the element we’re looking for is the last one in the list or isn’t present at all. Using binary search for the same list of 1,000 elements, the worst-case is only 10 comparisons. This is calculated as the <em>base two logarithm of the list’s length</em>, and the benefits get more and more significant the longer the list. For a list of 100,000 elements, it would be 17 comparisons instead of 100,000 comparisons.</p>
<p><strong>But remember, that for this to work, the list needs to be sorted</strong>. So <em>if the list isn’t sorted, we would need a sort it first, which takes a chunk of time</em>. It can still make sense to do it if we’re going to search through it several times but it doesn’t make sense to sort the list and then use binary search to only find one element. In that case, using linear search is simpler and faster. If you’re curious about what these two types of search look like when implemented in Python, you can see a possible implementation of linear search and one for binary search <a href="Linear%20and%20Binary%20Search.html">in the next reading</a>. After that, we’ll talk about how we can apply the principles of binary search to troubleshooting.</p>
<h3 id="applying-binary-search-in-troubleshooting">Applying Binary Search in Troubleshooting</h3>
<p>We called out that the binary search algorithm is really efficient when trying to find an element in a sorted list. <em><strong>In troubleshooting, we can apply this idea when we need to go through and test a long list of hypotheses</strong></em>. When doing this, the list of elements contains all the possible causes of the problem and we keep reducing the problem by half until only one option is left.</p>
<p>The list of elements could be <em>entries in a file</em>, <em>extensions enabled</em>, <em>boards connected to a server</em>, <em>or even lines of code added to a faulty release</em>. With each iteration, the problem is cut in half. This approach is sometimes called <em><strong>bisecting</strong></em> which means dividing in two.</p>
<p>In an earlier section, we gave the example of a new version of a program that fail to start when the old configuration directory was present. If the directory contained a bunch of different files in it, we could identify the one causing the failure by bisecting the list of files. Say the old directory contained 12 different config files. We want to identify which of those 12 is causing the failure. To do that, we can create a copy of the directory with just six of the 12 files and then try to start the program again. If it crashes, then the bad file is among those six files. If it doesn’t, it’s among the other six.</p>
<p>In the next step, we would pick three out of the failing group of six. If the program crashes again, it’s one of those three. If it doesn’t, it’s one of the other three. For the last three, we can first check two together or just go one by one. Either way, it’s two checks to get to the failing file. This means that with a total of four attempts, we can find out which of the 12 files is causing the problem.</p>
<p><em>Since things in IT can sometimes be complex and intertwined</em>, before declaring victory, <em>we want to verify that the program crashes with that single file present and doesn’t crash when the single file isn’t present</em>. Once we’ve confirmed that, we’ve reduced the reproduction case of our problem to a single file instead of a whole directory much easier to understand and figure out what’s going on. After that, we can proceed in the same way with the contents of that single file, cutting it in half repeatedly, until we find the specific part of the file that’s causing the problem.</p>
<p><em><strong>The same process can be applied to a large variety of problems</strong></em>. It’s very common for example to use it to figure out which browser extension is causing the browser to crash, disabling half of the extensions then checking if the browser crashes with that subset and so on until we find the faulty extension. We can also use this technique to discover which plug-in in a desktop environment is causing the computer to run out of memory, or which entry in a database is causing the program to raise an exception.</p>
<p><em><strong>We can also apply this to code when trying to find a bug that was introduced in a recent version</strong></em>. If we know the list of changes that were made between one version and the next, we can keep cutting that list in half until we find the one that caused the failure. When using Git for version control, we can use a <strong>Git</strong> command called <strong>bisect</strong>. Bisect receives two points in time in the Git history and repeatedly lets us try the code at the middle point between them until we find the commit that caused the breakage. This doesn’t even need to be your Git repository. If you’re using open source software that’s tracking Git, you can use the bisect commit to find out which command cause the software to stop working on your computer. For example, if the latest release of the Linux kernel causes the sound card on your computer to stop working, you can use Git bisect to find the commit that broke it and report this as a bug to be fixed.</p>
<p>As we called out when we were talking about binary search, <em>the longer the list of items that needs to be checked, the more we’ll gain by cutting our problem in half on each iteration</em>. If it’s just five options that need to be checked, we can simply go one-by-one. It won’t make a lot of difference and it might be easier to keep track of what we tried. But if it’s a 100, we definitely want to bisect the problem so we can find the answer in seven steps, not a 100. When we have to test a bunch of different options to find the one that’s causing a failure, we’ll want a quick and easy way to check it. Even if we’re reducing the amount of attempts by bisecting the problem, we don’t want to spend a long time on each check. Sometimes it’s straightforward. Either the program starts or it fails. But other times, it can take a bunch of manual steps to check what we want to check. So depending on what the problem is that we’re trying to find, <em><strong>it might make sense to spend some time creating a script that checks for the issue</strong></em>.</p>
<h3 id="finding-invalid-data">Finding Invalid Data</h3>
<p>We have discussed how we can quickly find out the reason for a problem in a list of possible reasons by splitting the problem in half and testing each half separately. Let’s see this in action with an example.</p>
<p>We have a program that reads data from a csv file, processes it, and then imports it into a database. One of the users of the system tells us that the file they’re trying to import fails with an obscure import error. They’ve sent us the file so we can try it ourselves. To call the command, we’ll connect the output of <strong>cat contacts.csv</strong>, the file that the user sent us, to the <em><strong><a href="http://import.py">import.py</a></strong></em> command. <em>But before we run the command, it’s a good time to <strong>remember that we shouldn’t test in production</strong></em>. And since this script is going to be trying to import data into a database, <em>we should run it against the test database instead of the production database</em>. To do that, we’ll use the <em>–server flag</em> that takes the name of the database server, and then we’ll pass the test as the parameter.</p>
<p>We see that the file fails with an importing error, and doesn’t give us a lot of information about what’s failing. And how big is that file? We could open it with an editor and check, but we don’t need to. We can use the <strong>wc</strong> command that counts characters, words, and lines in a file. In particular, <strong>wc -l</strong> will print the amount of lines in a file. So our file has 100 lines in it, that’s a lot. We don’t want to have to go looking through that list to find out what could be wrong, especially since we have no idea what that might be. Instead, we can try passing only half of the file to the script and check if it succeeds or fails. If it fails, then we pick up that part of the file and check again with half of it. If our import succeeds, then we take the other half and split it in two. We could edit the file manually to add or remove the parts as needed, but that would be tedious.</p>
<p>Instead, we can use the tools available to us to help us do that with less effort. We can use the <strong>head</strong> command to print the first lines in the file, and the <strong>tail</strong> command to print the last lines. We can pass the amount of lines we want to include as a parameter. So <strong>head -15</strong> will print the first 15 lines, while <strong>tail -20</strong> will print the last 20 lines. Aas we saw earlier, our command reads the file to import from standard input. So we can use <em><strong>pipes</strong></em> to connect the output of our <strong>head</strong> or <strong>tail</strong> commands to it. Let’s try to input the first half of the file now.</p>
<p>The first half failed, let’s split it again. To do that, we’ll use another pipe to take only half of the previous number. This way, in each step, we’ll add a call to <strong>head</strong> <em>or</em> <strong>tail</strong> for the corresponding size. This time, it succeeded, hooray. This means that the failure must be in the second quarter of the file. Let’s verify that that’s the case by giving that part to our command. To do that, we’ll take the first half using <strong>head</strong>, then get the second half of it using <strong>tail</strong>. [SOUND] This fails again, but that’s good, it means we’re on the right track. Let’s split it once again. Great, our test set is getting smaller, let’s split it once more. Okay, we’re down to six entries, and we know that one of them is the bad one. Let’s spin it one more time, and then we can look at the three remaining entries. Let’s look at the three entries left and see if we can find the culprit. Can you see the problem?</p>
<p>This is a comma separated file. This means that each comma is used as a separator between the fields in the file. If a field includes commas, it should be written between quotes. But in the case of the third line we’re looking at here, we can see that there’s a comma instead of a period after the middle initial, and this is not written between quotes. The importing script is then confused because there are too many fields in this line. Let’s edit the file and fix it.</p>
<p>And now let’s run our importer again with the fixed file. Yay, we fixed the problem in the file. Using the <em><strong>bisect method</strong></em>, we very quickly found which line out of 100 lines contained the corrupt data. And then we could fix it and successfully import it. <em>The short-term remediation</em> here is to tell our user about what we found and how to fix it, so that they can import the data into the production database. <em>The long-term remediation</em> is to figure out why the file was generated with the invalid field in the first place, and make sure that it doesn’t happen again.</p>
<h3 id="module-1-review">Module 1 Review</h3>
<p>Congrats on making it to the end of the module. You’re sure learning a lot that’ll open up a lot of doors in your IT career.</p>
<ul>
<li>
<p>Over the past videos, we learned the general principles of debugging and troubleshooting. We looked into the basic process of solving a technical problem like getting information, finding the root cause, and implementing the remediation.</p>
</li>
<li>
<p>We learned about a bunch of different tools and techniques that we can use to better understand what’s going on with our systems and our programs, including how to create a reproduction case, how to find the root cause for problem, and how to deal with issues that only appear occasionally.</p>
</li>
<li>
<p>Finally, we learned about the binary search algorithm, and how we can use it to bisect a problem and quickly find the root cause of a technical problem. All along, we’ve checked out a bunch of real-world examples, and seen how we can apply this to lots of different types of problems, like a bug in our code, a bug in someone else’s code, a configuration issue, or even a hardware problem. It’s been great sharing all these interesting stories and examples with you.</p>
</li>
</ul>
<p>I hope you’re starting to have fun learning more about how to understand problems and find solutions. Next time you need to solve a technical problem, try to use some of these steps we outlined and the ideas we talked about. <strong>Remember</strong> that logs are your best friend, and use all the resources available to you, including looking things up on the Internet, and asking colleagues or friends for help.</p>
<p>Throughout the rest of this course, we’ll keep exploring scenarios that deal with specific problems, like our computer being slow or crushing unexpectedly. We’ll keep applying the techniques we’ve explained in this module to solve those issues. So you can expect a lot more practice.</p>
<h2 id="understanding-slowness">Understanding Slowness</h2>
<h3 id="intro-to-module-2-slowness">Intro to Module 2: Slowness</h3>
<p>Remember that if things don’t work right away, it’s totally fine. The more you try your hand at solving problems, the easier you’ll come up with possible solutions.</p>
<p>A problem that we have to deal with a lot when working in IT, is things being slow. This could be our computer, our scripts, or even complex systems. Slow is a relative term. Modern computers are much faster and can do many more things than computers a couple of decades ago. Still, we always want them to be faster and to do more in less time. With modern day computers it might seem that our resources are unlimited, but if we try hard enough we can hit the limits.</p>
<p>There’s a bunch of different things we can do if our system is too slow. The most obvious one is closing any applications we don’t need at the moment. This works because it helps us free some of the resources in our computer, like CPU time, RAM, or video memory. That way the program that we want to run faster will have access to more of these resources. When closing applications that we don’t need we might even need to look at applets, plugins, extensions, or other small programs that might seem harmless, as they take some resources to run.</p>
<p>On top of that, closing any other elements that take resources, like browser tabs or open files in a document editor, can also help. But this only gets us so far because there’s a ton of other reasons why our devices or programs might be slow. Over the course of the next few sections we’ll do a rundown of the different reasons that can make things run slowly. We’ll look into what causes slow scripts, slow computers, or slow systems. We’ll give you the tools to help you identify the most common causes of slowness, and apply solutions to improve the overall performance. By the end of these videos slowness will rarely slow you down.</p>
<h3 id="why-is-my-computer-slow">Why is my computer slow?</h3>
<p>Our computers execute thousands of millions of instructions per second. Each of those instructions does one small thing, like incrementing a value, comparing two values, or moving a value from one place to another. Still with thousands of millions of instructions per second, there’s a lot that a computer can do in just one second. This allows our computer to seemingly execute a number of different thing at the same time. Say you’re browsing the web while also running an application that plays your favorite music in the background. Even if your computer has a single core to execute those applications, it will seem like the computer is running these two programs at the same time. What’s happening under the hood is that each application gets a fraction of the CPU time, and then the next application gets a turn. Most of the time this works fine. But if you run too many applications or if one of these applications were running needs more CPU time than the fraction it’s getting, things might become frustratingly slow.</p>
<p><strong>The general strategy for addressing slowness is to identify the bottleneck</strong> in our device, our script, or our system to run slowly.</p>
<p>The bottleneck could be the CPU time as we just mentioned. But it could also be time <em>spent reading data from disk waiting for data transmitted over the network, moving data from disk to RAM, or some other resource that’s limiting the overall performance</em>. Pretty often, we can speed things up and getting rid of anything else that’s using resources on the same computer. So if the problem is that your program needs more CPU time, you can close other running programs that you don’t need right then. If the problem is that you don’t have enough space on disk, you can uninstall applications that you don’t use, or delete or move data that doesn’t need to be on that disk. If the problem is that the application needs more network bandwidth, you can try stopping any other processes that are also using the network and so on.</p>
<p>This only helps us if the issue is that there are too many processes trying to use the same resource. If we’ve closed everything that wasn’t needed and the computer is still slow, we need to look into other possible explanations. What if the hardware we’re using just isn’t good enough for the applications we’re trying to run on it?</p>
<p>In cases like these, you will have to upgrade the underlying hardware. But to make a difference in the resulting performance, we need to make sure that we’re actually improving the bottleneck and not just wasting our money on new hardware that will go unused. So how can we tell which piece of hardware needs to be changed?</p>
<p>We need to <strong>monitor</strong> the usage of our resources to know which of them as being exhausted. This means that it’s being used completely and programs are getting blocked by not having access to more of it. Is it the CPU, the memory, the disk IO, the network connection, the graphics card? To find out, we use the tools available in our operating system, monitor the usage of each resource, and then work out which one is blocking our programs for running faster. We’ve already talked about using <strong>top</strong> on Linux systems. This tool lets us see which currently running processes are using the most CPU time. If we start by memory, which ones are using the most memory. It also shows a bunch of other load information related to the current state of the computer, like how many processes are running and how the CPU time or memory is being used. We also called out in earlier visectionsdeos a couple of other programs like <strong>iotop</strong> and <strong>iftop</strong>. They can help us see which processes are currently using the most disk IO usage or the most network bandwidth.</p>
<ul>
<li>
<p>On <strong>macOS</strong>, the OS ships with a tool called <em><strong>Activity Monitor</strong></em> which lets us see what’s using the most CPU, memory, energy, disk, or network.</p>
</li>
<li>
<p>On <strong>Windows</strong>, there’s a couple of OS tools called Resource Monitor and Performance Monitor which also let us analyze what’s going on with the different resources on the computer including CPU, memory, disk and network. So if you’re looking to diagnose what’s causing your computer to run slow, the first step is always to open one of these tools, check out what’s going on, and try to understand which resources the bottleneck and why. Then plan how you’re going to solve the issue.</p>
</li>
</ul>
<p>Of course, not all performance problems are solved by closing applications are getting better hardware. Sometimes, we need to figure out what the software is doing wrong and where it’s spending most of its time to understand how to make it run faster. <em>We need to really study each problem to get to the <strong>root cause</strong> of the slowness</em>.</p>
<h3 id="how-computers-use-resources">How Computers Use Resources</h3>
<p>In the last section, we called out how a computer can be constrained by different resources like <em>CPU, disk, memory, or network</em>. We discussed how we need to eliminate the <em><strong>bottlenecks</strong></em> and get our computer to better use its resources to boost our system’s performance.</p>
<p>To do that, we need to understand how each component interacts with the other and what the limitations are. In particular, when thinking about making things faster, it’s important to understand the different speeds of the parts involved. When an application is accessing some data, the time spent retrieving that data will depend on where it’s located. If it’s a variable that’s currently being used in a function, the data will be in the CPU’s internal memory, and our program will retrieve it really fast. If the data is related to a running program but maybe not the currently executing function, it will likely be in RAM, and our program will still get to a pretty fast. If the data is in a file, our program will need to read it from disk, which is much slower than reading it from RAM, and worse than reading from disk, is reading information from over the network. In this case, we have a lower transmission speed, and we also need to establish the connection to the other endpoint to make the transmission possible, which adds to the total time needed to get to the data. So if you have a process that requires repeatedly reading data over the network, you might want to figure out if you can read it once stored on disk, and then read it from disk afterwards. Or similarly, if you repeatedly reading files from disk, you might see if you can put the same information directly into the process memory and avoid loading it from disk every time. In other words, you want to consider if you can create a cache.</p>
<p><strong>A cache stores data in a form that’s faster to access than its original form</strong>. There’s a ton of examples of caches in IT. A web proxy is a form of cache. It stores websites, images, or videos that are accessed often by users behind the proxy. So they don’t need to be downloaded from the Internet every time. DNS services usually implement a local cache for the websites they resolve. So they don’t need to query from the Internet every time someone asks for their IP address. The operating system also takes care of some caching for us. It tries to keep as much information as possible in RAM so that we can access it fast. This includes the contents of files or libraries that are accessed often, even if they aren’t in use right now. We say that these contents are cached in memory. We call that that if the data is part of a program that’s currently running, it will be in RAM. But RAM is limited. If you run enough programs at the same time, you’ll fill it up and run out of space. What happens when you run out of RAM? At first, the OS will just remove from RAM anything that’s cached, but not strictly necessary. If there’s still not enough RAM after that, the operating system will put the parts of the memory that aren’t currently in use onto the hard drive in a space called <strong>swap</strong>.</p>
<p>Reading and writing from disk is much slower than reading and writing from RAM. So when the swapped out memory is requested by an application, it will take a while to load it back. The swapping implementation varies across the different operating systems, but the concept is always the same. The information that’s not needed right now is removed from RAM and put onto the disk, while the information that’s needed now is put into RAM. This is normal operation, and most of the time, we don’t notice it. But if the available memory is significantly less than what the running applications need, the OS will have to keep swapping out the data that’s not in use right now to move the data currently in use to RAM, and as we called out, our computer can switch between applications very quickly, which means that the data currently in use can also change very quickly. The computer will start spending a lot of time writing to disc to make some space in RAM and then reading from disk to put other things in RAM. This can be super slow.</p>
<p>So what do you do if you find that your machine is slow because it’s spending a lot of time swapping? There are basically three possible reasons for this. We’ve already talked about two of them.</p>
<ol>
<li>
<p>First, if there are too many open applications and some can be closed, close the ones that aren’t needed.</p>
</li>
<li>
<p>Or if the available memory is just too small for the amount that computer is using, add more RAM to the computer.</p>
</li>
<li>
<p>The third reason is that one of the running programs may have a memory leak, causing it to take all the available memory. <strong>A memory leak means that memory which is no longer needed is not getting released</strong>. We’ll talk a bunch more about memory leaks later in the course. For now, let’s just say that if a program is using a lot of memory and this stops when you restart the program, it’s probably because of a memory leak.</p>
</li>
</ol>
<h3 id="possible-causes-of-slowness">Possible Causes of Slowness</h3>
<p>We’ve talked about a few different things that can make our computer slow. But of course, there’s a lot more possible reasons. In this section, we’ll do a rundown of some of the most common ones that you might come across in your IT role. Before we do, a quick reminder that when trying to diagnose why a computer is slow, we should use the <em><strong>process of elimination</strong></em> that we looked at earlier. We first look for the simplest explanations that are the easiest to check. And after eliminating a possible root cause, we go back to the problem and come up with the next possible cause to check.</p>
<ul>
<li>So when trying to figure out what’s making a computer slow, the first step is to look into when the computer is slow. If it’s slow when starting up, it’s probably a sign that there are too many applications configured to start on boot. In this case, fixing the problem is just a question of going through the list of programs that start automatically and disabling any that aren’t really needed. If instead the computer becomes sluggish after days of running just fine, and the problem goes away with a reboot, it means that there’s a program that’s keeping some state while running that’s causing the computer to slow down.</li>
</ul>
<p>For example, this can happen if a program stores some data in memory and the data keeps growing over time, without deleting old values. If a program like this stays running for many days, the data might grow so much that reading it becomes slow and the computer runs out of RAM. This is almost certainly a bug in the program. And the ideal solution for a problem like this is to change the code so that it frees up some of the memory used. If you don’t have access to the code, another option is to schedule a regular restart to mitigate both the slow program and your computer running out of RAM.</p>
<ul>
<li>
<p>A similar problem that can trigger after a long time using an application, and that isn’t solved by a reboot, is that the files that an application is handling have grown too large. So when the program needs to read those files, it gets really slow. Again, this generally points to a bug in the way the program was designed because it didn’t expect the files to grow so large. The best solution in this case is to fix the bug. But what can you do if you can’t modify the code of the program? You can try to reduce the size of the files involved. If the file is a log file, you can use a program like <strong>logrotate</strong> to do this for you. For other formats, you might need to write your own tool to rotate the contents.</p>
</li>
<li>
<p>Another data point that we can use to diagnose what’s going on is whether this happens for all users of the application or just a subset of them. If only some users are affected, we’ll want to know if there’s something that’s configured differently on those computers that might be triggering the slowness.</p>
</li>
</ul>
<p>For example, many operating systems include a feature that tracks the files in our computer so it’s easy and fast to search for them. This feature can be really useful when looking for something on a computer, but can get in the way of everyday use if we have tons of files and not the most powerful hardware.</p>
<ul>
<li>
<p>We’ve called out before that reading from the network is notably slower than reading from disk. It’s common for computers in an office network to use a file system that’s mounted over the network so they can share files across computers. This normally works just fine, but can make some programs really slow if they’re doing a lot of reads and writes on this network-mounted file system. To fix this, we’ll need to make sure that the directory used by the program to read and write most of its data is a directory local to the computer.</p>
</li>
<li>
<p>Hardware failures can also cause our computer to become slow. If your hard drive has errors, the computer might still be able to apply error correction to get the data that it needs, but it will affect the overall performance. And once a hard drive starts having errors, it’s only a matter of time until they’re bad enough that data starts getting lost, so it’s worth keeping an eye out for them. To do this, we can use some of the OS utilities that diagnose problems on hard drives or on RAM, and check if there’s anything that could be causing problems.</p>
</li>
<li>
<p>Yet another source of slowness is malicious software. Of course, we always want to keep your computer clean of any malicious software, but we can feel the effects of malicious software even if they aren’t installed.</p>
</li>
</ul>
<p>For example, you might have come across a website that includes scripts, either in the website’s content or the ads displayed, that use our processor to mine for cryptocurrency. Malicious browser extensions also fall into this category.</p>
<p>As you can see, there’s a lot of possible reasons that could cause our computer to run slowly. Whenever we have to fix an issue like this, we need to look at what the <em><strong>bottleneck</strong></em> is, figure out the root cause behind the resource being used up, and then take appropriate action.</p>
<h3 id="slow-web-server-example">Slow Web Server Example</h3>
<p>A user has alerted us that one of the web servers in our company is being slow, and we need to figure out what’s going on. Let’s start by navigating to the website and loading the page.</p>
<p>Okay. We see that the page loads. It seems to be a little slow but it’s hard to measure this on our own. Let’s use a tool called <strong>ab</strong> which stands for <em><strong>Apache Benchmark tool</strong></em> to figure out how slow it is. We’ll run <strong>ab -n 500</strong> to get the average timing of 500 requests, and then pass our <a href="http://site.example.com">site.example.com</a> for the measurement. This tool is super useful for checking if a website is behaving as expected or not. It will make a bunch of requests and summarize the results once it’s done. Here, were asking for it to do 500 requests to our website. There are a lot more options that we could pass like how many requests we want the program to do at the same time, or if the test to finish after timeout, even if not all requests completed, we’re making 500 requests so that we can get an average of how long things are taking. Once the test finishes, we can look at the data and decide if it’s actually slow or not.</p>
<p>All right. The tool has finished running the 500 requests. We see that the mean time per requests was a 155 milliseconds. While this is not a super huge number, it’s definitely more than what we’d expect for such a simple website. It seems that something is going on with the web server and we need to investigate further. Let’s connect to the web server and check out what’s going on. We’ll start by looking at the output of <strong>top</strong> and see if there’s anything suspicious there. We see that there’s a bunch of ffmpeg processes running, which are basically using all the available CPU. See those load numbers? Thirty is definitely not normal. <em><strong>Remember that the load average on Linux shows how much time the processor is busy at a given minute with one meaning it was busy for the whole minute</strong></em>. This computer has two processors. So any number above two means that it’s overloaded. During each minute, there were more processes waiting for processor time than the processor had to give.</p>
<p>This ffmpeg program is used for video transcoding which means converting files from one video format to another. This is a CPU intensive process and seems like the likely culprit for our server being overloaded. So what can we do? One thing we can try is to change the processes priorities so that the web server takes precedence. <em><strong>The process priorities in Linux are so that the lower the number, the higher the priority. Typical numbers go from 0 to 19</strong></em>. <em>By default, processes start with a priority of zero</em>. But we can change that using the <strong>nice</strong> and <strong>renice</strong> commands. <em>We use <strong>nice</strong> for starting a process with a different priority and <strong>renice</strong> for changing the priority of a process that’s already running</em>.</p>
<p>Okay. Let’s exit <strong>top</strong> with queue and change the priorities. We want to run <strong>renice</strong> for all the ffmpeg processes that are running right now. We could do this one by one. But it would be manual, error-prone, and super boring. Instead, we can use a quick line of shell script to do this for us.</p>
<p>For that, we’ll use the <strong>pidof</strong> command that receives the process name and returns all the process IDs that have that name. We’ll iterate over the output of the pidof command with a for loop and then call renice for each of the process IDs. Renice takes the new priority as the first argument, and the process ID to change as the second one. In our case, we’ll want the lowest possible priority which is 19. So we’ll call:</p>
<pre class=" language-bash"><code class="prism  language-bash"><span class="token keyword">for</span> pid <span class="token keyword">in</span> <span class="token variable"><span class="token variable">$(</span>pidof ffmpeg<span class="token variable">)</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">renice</span> 19 <span class="token variable">$pid</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre>
<p>All right. We see that the priorities for those processes were updated. Let’s run our benchmarking software again and check out if it made any difference.</p>
<p>Okay. It’s running once again. We’ll need to wait until the 500 requests are done and check out the new meantime per request value. This time, the meantime is 153 milliseconds. It doesn’t seem like our renice helped. Apparently, the OS is still giving these ffmpeg processes way too much processor time. Our website is still slow. What else can we do?</p>
<p>These transcoding processes are CPU intensive, and running them in parallel is overloading the computer. So one thing we could do is, modify whatever’s triggering them to run them one after the other instead of all at the same time. To do that, we’ll need to find out how these processes got started.</p>
<p>First, we’ll look at the output of the ps command to get some more information about the processes. We’ll call <strong>ps ax</strong> <em>which shows us all the running processes on the computer</em>, and we’ll connect the output of the command to <strong>less</strong>, to be able to scroll through it. Now we’ll look for the ffmpeg process using slash which is the search key when using less.</p>
<p>Okay. We see that there are a bunch of ffmpeg processes that are converting videos from the webm format to the mp4 format. We don’t know where these videos are on the hard drive. We can try using the <strong>locate</strong> command to see if we can find them. We’ll first exit the less interface with queue and then call locate static/001.webm.</p>
<p>We see that the static directory is located in the server deploy videos directory. Let’s change into that directory and see what we find. There’s a bunch of files here. We could check them all one-by-one to see if one of them contained a call to ffmpeg. But that sounds like a lot of manual work. Instead, let’s use <strong>grep</strong> to check if any of these files contains a call to ffmpeg.</p>
<p>So we see that there’s a couple of mentions in the <em><a href="http://deployed.sh">deployed.sh</a> file</em>. Let’s take a look at that one. Since we’re connecting to the server remotely, we can’t open the file using a graphical editor. We need to use a command line editor instead. We’ll use <strong>vim</strong> in this case.</p>
<p>We see that this script is starting the ffmpeg processes in parallel using a tool called Daemonize that runs each program separately as if it were a daemon. This might be okay if we only need to convert a couple of videos but launching one separate process for each of the videos in the static directory is overloading our server. So we want to change this to run only one video conversion process at a time. We’ll do that by simply deleting the daemonized part and keeping the part that calls ffmpeg, then save and exit.</p>
<p>All right. We’ve modified the file. But this won’t change the processes that are already running. We want to stop these processes but not cancel them completely, as doing so would mean that the videos being converted right now will be incomplete. So we’ll use the <strong>killall</strong> command with the <strong>-STOP</strong> flag which sends a stop signal but doesn’t kill the processes completely. We now want to run these processes one at a time. How can we do that?</p>
<p>We could send the <strong>-CONT</strong> signal to one of them, wait till it’s done, and then send it to the next one. But that’s a lot of manual work. Can be automate it?</p>
<p>Yes! But it’s a little tricky. So pay close attention. We can iterate through the list of processes using the same for loop with the <strong>pidof</strong> command that we used earlier. Inside the for loop, we want to send the cont signal and then wait until the process is done. Unfortunately, there’s no command to wait until the process finishes. But we can create a while loop that sends the cont signal to the process. This will succeed as long as the process exists, and fails once the process goes away. Inside this while loop, we’ll simply add a call to sleep one, to wait one second until the next check.</p>
<pre class=" language-bash"><code class="prism  language-bash"><span class="token keyword">for</span> pid <span class="token keyword">in</span> <span class="token variable"><span class="token variable">$(</span>pidof ffmpeg<span class="token variable">)</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token keyword">while</span> <span class="token function">kill</span> -CONT <span class="token variable">$pid</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">sleep</span> 1<span class="token punctuation">;</span><span class="token keyword">done</span> <span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre>
<p>Okay. Now our server is running one ffmpeg process at a time. Let’s turn our benchmark once more. The mean time is now 33 milliseconds. That’s much lower than before. We’ve managed to get our web server to reply promptly to the request again.</p>
<p>We’ve mentioned a few different approaches that we can take when we can’t fix the code like renicing the processes, or running them one after the other when that doesn’t help.</p>
<h3 id="monitoring-tools">Monitoring Tools</h3>
<p>Check out the following links for more information:</p>
<ul>
<li>
<p><a href="https://docs.microsoft.com/en-us/sysinternals/downloads/procmon">https://docs.microsoft.com/en-us/sysinternals/downloads/procmon</a></p>
</li>
<li>
<p><a href="http://www.brendangregg.com/linuxperf.html">http://www.brendangregg.com/linuxperf.html</a></p>
</li>
<li>
<p><a href="http://brendangregg.com/usemethod.html">http://brendangregg.com/usemethod.html</a></p>
</li>
<li>
<p><a href="https://support.apple.com/en-us/HT201464">Activity Monitor in Mac</a></p>
</li>
<li>
<p><a href="https://www.windowscentral.com/how-use-performance-monitor-windows-10">Performance Monitor on Windows</a></p>
</li>
<li>
<p><a href="https://www.digitalcitizen.life/how-use-resource-monitor-windows-7">https://www.digitalcitizen.life/how-use-resource-monitor-windows-7</a></p>
</li>
<li>
<p><a href="https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer">https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Cache_(computing">https://en.wikipedia.org/wiki/Cache_(computing</a>)</p>
</li>
<li>
<p><a href="https://www.reddit.com/r/linux/comments/d7hx2c/why_nice_levels_are_a_placebo_and_have_been_for_a/">https://www.reddit.com/r/linux/comments/d7hx2c/why_nice_levels_are_a_placebo_and_have_been_for_a/</a></p>
</li>
</ul>
<h2 id="slow-code">Slow Code</h2>
<h3 id="writing-efficient-code">Writing Efficient Code</h3>
<p>In your role as an IT specialist or systems’ administrator, you’ll likely need to write scripts to automate tasks. A piece of code may start as a simple script that does a single thing, but end up growing into a complex program that handles many different tasks, and no matter the size and complexity of our code, we usually want it to perform well.</p>
<p>In this and the next few sections, we’ll discuss some ideas for how to make our code more efficient and how to figure out what needs fixing if it’s slow. <em>One important thing to keep in mind though is that <strong>we should always start by writing clear code that does what it should and only try to make it faster if we realize that it’s not fast enough</strong></em>.</p>
<p>If it takes you 10 minutes to write a script that will run in five seconds, and 20 minutes to write a script that will do the same but takes three seconds, does it make a difference? It all depends on how often you run the script. If you run it once a day, the two seconds deference definitely won’t justify the additional 10 minutes of work. But if you’re going to run the same script for the 500 computers on your network, that small difference means it will take 15 less minutes to run the whole script. So overall, you’re gaining time.</p>
<p>Of course, it’s pretty hard to know in advance how fast your script will be and how long it will take you to make it faster. <em><strong>But as a rule, we aim first to write code that’s readable, easy to maintain and easy to understand, because that lets us write code with less bugs</strong></em>. If there’s something that’s super slow, then yes, it makes sense to fix it, particularly if the script will be executed frequently enough that making it faster will save you more time than the time you spend optimizing it. <em><strong>But remember, trying to optimize every second out of a script is probably not worth your time</strong></em>. With that said, let’s dive into how we can make our code more efficient.</p>
<ul>
<li>The first step is to keep in mind that we can’t really make our computer go faster. <strong>If we want our code to finish faster, we need to make our computer do less work</strong>. And to do this, we’ll have to avoid doing work that isn’t really needed. How? There’s a bunch of different things to do. The most common ones include <em>storing data that was already calculated to avoid calculating it again using the right data structures for the problem and reorganizing the code so that the computer can stay busy while waiting for information from slow sources like disk or over the network</em>.</li>
</ul>
<p>To know what sources of slowness we need to address, we have to figure out where our code is spending most of its time. There’s a bunch of tools that can help us with that called <strong>profilers</strong>. <strong>A profiler is a tool that measures the resources that our code is using, giving us a better understanding of what’s going on</strong>. In particular, they help us see how the memory is allocated and how the time spent. Because of how profilers work, they are specific to each programming language. So we would use <strong>gprof to analyze a C program</strong> but use the <strong>cProfile module to analyze a Python program</strong>.</p>
<p>Using tools like these, we can see which functions are called by our program, how many times each function was called and how much time are programs spent on each of them. This way we can find for example, that our program is calling a function more times than we originally intended or that a function that we thought would be fast is actually slow.</p>
<p>To fix our code, we’ll probably need to restructure it to avoid repeating <strong>expensive actions</strong>. What do we mean by expensive? In this context, <strong>expensive actions are those that take a long time to complete</strong>. Expensive operations include parsing a file, reading data over the network or iterating through a whole list. How do we modify our code to avoid expensive operations?</p>
<h3 id="using-the-right-data-structures">Using the Right Data Structures</h3>
<p>Having a good understanding of the data structures available to us can help us avoid unnecessary expensive operations and create efficient scripts.<br>
In particular, we’ll want to understand the performance of those structures under different conditions. In the introductory course to Python, you learned about a bunch of different data structures available in Python like <em>lists</em>, <em>tuples</em>, <em>dictionaries</em>, and <em>sets</em>. Each of them have their uses, their advantages, and disadvantages. Let’s do a very quick recap of lists and dictionaries.</p>
<ul>
<li>
<p><strong>Lists are sequences of elements. We can add, remove, or modify the elements in them. We can iterate through the whole list to operate on each of the elements</strong>. Different programming languages call them differently. The structure is called <em>ArrayList</em> in Java, <em>Vector</em> in C++, <em>Array</em> in Ruby, and <em>Slice</em> in Go. All these names refer to the same data structure that’s fast to add or remove elements at the end. But adding or removing elements in the middle can be slow because all the elements that follow need to be repositioned. It’s fast to access the element in a specific position in the list, but finding an element in an unknown position requires going through the whole list. This can be super slow if the list is long.</p>
</li>
<li>
<p><strong>Dictionaries store key value pairs. We add data by associating a value to a key. Then, we retrieve a value by looking up a specific key</strong>. They are called <em>HashMap</em> in Java, <em>Unordered Map</em> in C++, <em>Hash</em> in Ruby, and <em>Map</em> in Go. The map part in those names comes from how we’re creating a mapping between a key and a value. The Hash part comes from the fact that to make the structure efficient, a hashing function is used internally to decide how the elements will be stored. The main characteristic of this structure is that it’s super-fast for looking up keys. Once we have our data stored in a dictionary, we can find the value associated to a key in just one operation. If it were stored in a list, we need to iterate through the list.</p>
</li>
</ul>
<p>So as a rule of thumb, <strong>if you need to access elements by position or will always iterate through all the elements, use a list to store them</strong>. This could be a list of all computers in the network, of all employees in the company, or of all products currently on sale for example.</p>
<p>On the flip side, if we need to look up the elements using a <em><strong>key</strong></em>, we’ll use a <em><strong>dictionary</strong></em>. This could be the data associated to a user which we’d look up using their username, the IP associated to a computer using the host name, or the data associated to a product using the internal product code. Whenever we need to do a bunch of these lookup operations, creating a dictionary and using it to get the data will take a lot less time than iterating over a list to find what we’re looking for. But it doesn’t make sense to create a dictionary and fill it with data if we’re only going to look up one value in it. In that case, we’re wasting time creating the structure when we could just iterate over the list and get the element we’re looking for.</p>
<ul>
<li>Another thing that we might want to <em>think twice</em> about is creating <em>copies</em> of the structures that we have in memory. If these structures are big, it can be pretty expensive to create those copies. So we should double-check if the copy is really needed.</li>
</ul>
<p>Now, that we have a better understanding of when to use each data structure and what actions to avoid, we can look into how to deal with expensive loops.</p>
<h3 id="expensive-loops">Expensive Loops</h3>
<p>Loops are what make our computers do things repeatedly. They are an extremely useful tool and let us avoid repetitive work, but we need to use them with caution. In particular, we need to think about what actions we’re going to do inside the loop, and when possible, avoid doing expensive actions. If you do an expensive operation inside a loop, you multiply the time it takes to do the expensive operation by the amount of times you repeat the loop.</p>
<p>Say for example that you’re writing a script to send an email to all the employees at your company asking them to verify that their emergency contact information is still valid. To send this out, you’ll have a loop that sends one email per employee. In the body of the email, you’ll include the current emergency contact data. <em><strong>The interesting part is how you access the data inside the loop</strong></em>. If the data is stored in a file, your script will need to parse the file to fetch it. If the script reads the whole file for every user, you’ll be wasting a lot of time parsing the file over and over unnecessarily. Instead, you could parse the file outside of the loop, put the information into a <strong>dictionary</strong>, and then use the dictionary to retrieve the data inside the loop.</p>
<p><strong>Whenever you have a loop in your code, make sure to check what actions you’re doing, and see if there are operations you can take out of the loop to do them just once</strong>. <em>Instead of making one network call for each element, make one call before the loop. Instead of reading from disk for each element, read the whole thing before the loop</em>. Even if the operations done inside the loop aren’t especially expensive, if we’re going through a list of a thousand elements and we only need five out of them, we’re wasting time on elements we don’t need. Make sure that the list of elements that you’re iterating through is only as long as you really need it to be.</p>
<p>Let’s say you’re running an internal website. As part of the information the site shows, it displays a list of the last five users that logged in. In the code, the program keeps a list of all the users that have logged in since it last started, and when the program needs to display the five latest users, it goes through the whole list and finds out which of those are the five most recent. This wastes a lot of time. If the service has been running for a while, it can take really long to go through the whole list. Instead, you could modify the service to store the user access info in log files that can be read if necessary and only keep the last five logins in memory. Whenever a new user logs in, the oldest entry in the list gets discarded and a new one gets added. That way, the script doesn’t need to go through the whole list every time it needs to display the five most recent users.</p>
<p><em><strong>Another thing to remember about loops is to break out of the loop once you found what you were looking for</strong></em>. In Python, we do this using the keyword <strong>break</strong>. Breaking out of loops means that as soon as the data we’re looking for is found, our script can continue. Of course if the data is at the end of the list, then we need to go through the loop anyway. But when the data is at the beginning of the list and not at the end, it makes sense to have our code break early to make the script faster. Say you’re writing a script that checks if a given username is within the list of authorized entities, and if it is, it grants them access to a particular resource. You can use a for loop to iterate through the list of entities. When the username is found, you can break out of the loop and continue the rest of the script.</p>
<p>One last thing to keep in mind is that <em>the right solution for one problem might not be right for a different problem</em>. Say your service has a total of 20 users. In that case, it’s okay to go over this list whenever you want to check something. It’s short enough that you don’t need to do any special optimization. But if your service has over a thousand users, you’ll want to avoid going through that list unless absolutely necessary. If the service has hundreds of thousands of users, going through that list isn’t even a possibility.</p>
<h3 id="keeping-local-results">Keeping Local Results</h3>
<p>In our last section, we talked about how to avoid having expensive operations inside our loops. So if we have to parse a file, we do it once before we call the loop instead of doing it for each element of the loop. But what if parsing the file is taking a lot of time even when it’s done outside of the loop?</p>
<p>Remember that to make our scripts get to their goal faster, we need to avoid having our computer do unnecessary work. So how can we avoid expensive operations like parsing a file, downloading data over the network, or going through a long list? If the script gets executed fairly regularly, it’s common to create a <strong>local cache</strong>. In an earlier section , we said that <strong>a cache is a way of storing data in a form that’s faster to access than its original form</strong>. So if we’re parsing a large file and only keeping a few key pieces of information from it, we can create a cache to store only that information, or if we’re getting some information over the network, we can keep a local copy of the file to avoid downloading it over and over again.</p>
<p>Creating caches can be super useful to save us time and make our programs faster. But they’re sometimes tricky to get right. <em><strong>We need to think about how often we’re going to update the cache and what happens if the data in the cache is out of date</strong></em>.</p>
<ul>
<li>
<p><em>If we’re looking for some long-term stats, we can generate the cache once per day</em>, and it won’t be a problem. This might be the case for data like <em>how much memory was used on computers across the fleet over the last month? How many employees each department in a company has? Or how many units were sold of each product over the last quarter?</em></p>
</li>
<li>
<p>But <em>if we’re trying to look at data where the value as of right now is super important, we either can’t use a cache or it has to be very short-lived</em>. This could be the case for <em>monitoring the health of computers to alert when something crosses a threshold. Checking the stock levels to see if there’s enough of a product to sell or seeing if a username already exists in the network when trying to create a new one</em>.</p>
</li>
</ul>
<p>Sometimes, we can add a check to validate if we need to recalculate the cache or not. For example, if our cache is based on a file, we could store the modification date of that file when we calculated the cache. Then only recalculate the cache if the modification date of the file is newer than the one we had stored.</p>
<p>If we don’t have a way of checking if our cache is out of date or not, we’ll need to add in logic to our program that tries to make a sensible decision. For that, we’ll take into account <em>how often we expect the data to change, how critical it is that the latest data is used, and how frequently the program that we’re running will be executed</em>. After taking all these factors into account, we might decide that the cache needs to be recreated once per day, once per hour, or even once per minute. Yes, even once per minute might make sense if you have a script that can get executed several times per minute and needs to do an expensive operation that can be cached. That way, only the first execution in a minute will spend time on this operation, the rest will be very fast. But the cache is never more than a minute out of date.</p>
<p>Keep in mind that <em>caches don’t always need to be elaborate structures</em>, storing lots of information with a complex timeout logic. Sometimes, they can be as simple as having a variable that stores a temporary result instead of calculating this result every time we need it.</p>
<p><em>For example, say you’re generating a report that prints how many users there are in each of the different groups in the network. Now, some of these groups may contain other groups in them and some groups may even be part of several groups. For example, the Java release engineers group would be part of the release engineers group and the Java developers group. How can we avoid counting unique users more than once if they show up in multiple groups? We can have a <strong>dictionary</strong> with the group as the key and the amount of users as the value. That way, we only need to count the members of a group <strong>once</strong>, and after that, just use the value in the dictionary.</em></p>
<p>To sum all of this up, remember that you’ll want to look for strategies that let you avoid doing expensive operations. First, check if these operations are needed at all. If they are, see if you can store the intermediate results to avoid repeating the expensive operation more than needed.</p>
<h3 id="slow-script-with-expensive-loop">Slow Script with Expensive Loop</h3>
<p>Remember that meeting reminder script that was having trouble with the dates? The developers has kept working on it. Now, since personalized emails with the name of the person getting email and the greeting. That’s cool. But unfortunately it seems to have made the application pretty slow. The developers are asking for our help in figuring out how we can make the program faster. So let’s get to work.</p>
<p>First, we’ll need to reproduce the problem and figure out what slow means in this case. One user told us that the problem is visible when the list of recipients is long. To avoid spamming our colleagues while we’re testing this issue, we’ll send reminders to a bunch of test users that we’ve created in our mail server. You might remember that the application has two parts. A shell script that pops up a window where we can enter the data of the reminder and a Python script that prepares the email and sends it, the part that’s slow is the sending of the emails. So we won’t interact with the pop-up at all. We’ll just pass the parameters we need to the Python script. We’ll measure the script speed using the <strong>time</strong> command. Let’s first call it with just one test user and see how long it takes.</p>
<pre class=" language-bash"><code class="prism  language-bash">$ <span class="token function">time</span> ./send_reminders.py <span class="token string">"2020-01-13|Example|test1"</span>
$ Successfully sent reminder to test1

real  0m.129s
user  0m.068s
sys   0m.013s
</code></pre>
<p>When we call <strong>time</strong> it runs the command that we pass to it and prints how long it took to execute it. There’s three different values: <strong>real, user, and sys</strong>:</p>
<ul>
<li>
<p><strong>Real</strong> is the amount of actual time that it took to execute the command. This value is sometimes called wall-clock time because it’s how much time a clock hanging on the wall would measure, no matter what the computer’s doing.</p>
</li>
<li>
<p><strong>User</strong> is the time spent doing operations in the user space.</p>
</li>
<li>
<p><strong>Sys</strong> is the time spent doing system level operations.</p>
</li>
</ul>
<p>The values of user and sys won’t necessarily add up to the value of real because the computer might be busy with other processes.</p>
<p>What do we see here? It took our script 0.129 seconds to send the email. That’s not a lot but we only send the message to one user. Let’s try this again with our nine tests users.</p>
<pre class=" language-bash"><code class="prism  language-bash">$ <span class="token function">time</span> ./send_reminders.py <span class="token string">"2020-01-13|Example|test1,test2,test3,test4,test5,test6,test7,test8,test9"</span>
$ Successfully sent reminder to test1,test2,test3,test4,test5,test6,test7,test8,test9
<span class="token punctuation">[</span><span class="token punctuation">..</span>.<span class="token punctuation">]</span>
</code></pre>
<p>We see that it took 0.296 seconds to send the email this time. That’s still not a lot but it does look like it’s taking longer with a longer list of emails. It’s time to try to make this better. How can we find out what’s wrong with the code?</p>
<p>We could always look at the code and see if we find any expensive operations that we can improve. But in this case we want to use a <em><strong>profiler</strong></em> to get some data about what’s going on. So let’s try that. There’s a bunch of different profilers available for Python that work for different use cases. Here, we’ll use the one called <strong>pprofile3</strong>. We use the <strong>-f flag</strong> to tell it to use the <em>callgrind</em> file format and the <strong>-o flag</strong> to tell it to store the output in the <strong>profile.out</strong> file. This generated a file that we can open with any tool that supports the <em>callgrind</em> format. We’re going to use <strong>kcachegrind</strong> to look at the contents, which is a graphical interface for looking into these files.</p>
<p>There’s a lot going on with this program, so don’t get scared if it takes a while to make sense out of it. As with so many other things practicing and tinkering on your own will help you get used to what all the different things here mean. Let’s look at the information we need now.</p>
<p>In the lower right half we see a call graph, which tells us that the main function is calling the send message function one time. This function is calling the message template function, the get name function, and the send message function nine times each. The graph also tells us how many microseconds are spent on each of these calls. We can see that most of the time is being spent in the get name function. That’s probably the one we should optimize. Let’s see what this function is doing using atom.</p>
<p>So we see that the get name function opens a CSV file then goes through the whole file checking if the first field in the line matches the e-mail name and when that’s the case it sets the value of the name variable. There’s a couple of things that are wrong with this function.</p>
<p>First, once it finds the element in the list it should immediately break out of the loop. Right now, it’s iterating through the whole file even if the email was found in the first line. But even if we fixed that it would still open the file and read through it for each e-mail address. This can get really slow if the file has a lot of lines. So how can we make this better?</p>
<p>We can read the file once and store the values that we care about in a <strong>dictionary</strong> and then use that dictionary for the lookups. Let’s do that. We’ll change the get name function and turn it into a read names function that will process the CSV file and store the values we want in the names dictionary. For each line will store the email as the key and the names as the values. Instead of returning one name we’ll return the whole dictionary. All right we have a read names function that stores the data we want in a dictionary. We now need to change the way this is called in the send message function. We see that the get name function is being called once per email. To apply our change we should call the read names function before the for loop so that we do it only once. Then instead of calling get name we’ll just get the values form the dictionary.</p>
<p>All right we’ve made the change. Let’s save our file and profile our script again to see if we manage to make it any faster. The graph looks different now as we’ve changed how the code behaves. See how the read names function is now taking a much smaller portion of time. On the flip side we see that the message template is the one that’s taking the most time now. So if we wanted to keep making our script faster that’s what we look next.</p>
<p>In this section, we saw that we can use the <strong>time</strong> command to check how long it takes to execute a program. We then saw how we can combine a <strong>profiler</strong> and a profile visualizer to figure out where our code is spending most of his time. Finally, we changed our code to avoid doing inexpensive loop over and over by storing the information in a dictionary and then accessing the dictionary instead.</p>
<h3 id="more-about-improving-our-code">More About Improving Our Code</h3>
<p>Check out the following links for more information:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Profiling_(computer_programming">https://en.wikipedia.org/wiki/Profiling_(computer_programming</a>)</li>
</ul>
<h2 id="when-slowness-problems-get-complex">When Slowness Problems Get Complex</h2>
<h3 id="parallelizing-operations">Parallelizing Operations</h3>
<p>We’ve called that a few times already, that reading information from disk or transferring it over the network is a slow operation. In typical scripts while this operation is going on, nothing else happens. The script is blocked, waiting for input or output while the CPU sits idle.</p>
<p>One way we can make this better is to do operations in parallel. That way, while the computer is waiting for the slow IO, other work can take place. The tricky part is dividing up the tasks so that we get the same result in the end. There’s actually a whole field of computer science called <em><strong>concurrency</strong></em>, dedicated to how we write programs that do operations in parallel. We won’t go into a ton of details here but we’ll give you a brief overview of what you can do.</p>
<p>First, we need to understand what the operating system already does for us. Our OS handles the many processes that run on our computer. If a computer has more than one core, the operating system can decide which processes get executed on which core, and no matter the split between cores, all of these processes will be executing in parallel. Each of them has its own memory allocation and does its own IO calls. The OS will decide what fraction of CPU time each process gets and switch between them as needed. So a very easy way to run operations in parallel is just to split them across different processes, calling your script many times each with a different input set, and just let the operating system handle the concurrency.</p>
<p>Let’s say you want to collect statistics on the current load and memory usage for all the computers in your network. You can do this by writing a script that connects to each computer in a list and gets the stats. Each connection takes a while to complete, so the total run-time of the script would be the sum of the time taken by all those connections. Instead, you could split the list of computers into smaller groups and use the OS to call the script many times once for each group. That way, the connections to the different computers can be started in parallel, which minimizes the time but the CPU isn’t doing anything. This is super easy to do and for many scripts, it’ll be the right choice.</p>
<p>Another easy thing to do, is to have a good balance of different workloads that you run on a computer. If you have a process that’s using a lot of CPU while a different process is using a lot of network IO and another process is using a lot of disk IO, these can all run in parallel without interfering with each other.</p>
<p>When using the OS to split the work and the processes, these processes don’t share any memory, and sometimes we might need to have some shared data. In that case, we’d use <em><strong>threads</strong></em>. <em><strong>Threads</strong></em> let us run parallel tasks inside a process. This allows threads to share some of the memory with other threads in the same process. Since this isn’t handled by the OS, we’ll need to modify our code to create and handle the threats. For that, we’ll need to look into how the programming language we’re using implements threading. In <em>Python</em>, we can use the <strong>Threading</strong> or <strong>AsyncIO</strong> modules to do this. These modules let us specify which parts of the code we want to run in separate threads or as separate asynchronous events, and how we want the results of each to be combined in the end.</p>
<p>One thing to watch out for is that depending on the actual threading implementation for the language you’re using, it might happen that all threads get executed in the same CPU processor. In that case, if you want to use more processors, you’ll need to split the code into fully separate processes. If your script is mostly just waiting on input or output, also known as <em><strong>I/O bound</strong></em>, it might matter if it’s executed on one processor or eight. But you might be doing this in parallel because you’re using all of the available CPU time. In other words, your script is <em><strong>CPU bound</strong></em>. In this case, you’ll definitely want to split your execution across processors.</p>
<p>Now, <em>there’s a point where adding more parallel processes means things become even slower</em>, not faster. If we’re trying to read a bunch of files from disk and do too many operations in parallel, the disk might end up spending more time going from one position to another then actually retrieving the data, or if we’re doing a ton of operations that use a lot of CPU, the OS could spend more time switching between them than actually making progress in the calculations we’re trying to do. So when doing operations in parallel, we need to find the right balance of simultaneous actions that let our computers stay busy without starving our system for resources. Some of these concepts may feel a little complex and it’s perfectly normal. Take your time, everyone learns slowness at their own pace.</p>
<h3 id="slowly-growing-in-complexity">Slowly Growing in Complexity</h3>
<p>As we called out in an earlier section, <em>a solution that’s good for one problem might not be so good for a different problem</em>. And as a system becomes more complex and grows in usage, a solution that worked well before may no longer be well-suited. Let’s say you’re writing a secret Santa script where each person gives a secret gift to one other randomly assigned person. The script randomly selects pairs of people and then sends an email to the gift-giver telling them who they’re buying a present for. If you’re doing this for the people working on your floor, you might just store the list of names and emails in a CSV file.</p>
<p>The file will be small enough that the time spent parsing it won’t be significant. Now if this script grows into a larger project that handles everyone at your company and the company keeps hiring more and more people, at some point parsing the file will start taking a lot of time. This is where you might want to consider using a different technology.</p>
<p>For example, you could decide to store your data in a <em><strong>SQLite</strong></em> file. This is a lightweight database system that lets you query the information stored in the file without needing to run a database server. Using SQLite for the data probably works just fine for assigning secret Santas at your company. But imagine that you’ve kept adding features to the service. So it now includes a way to create a wish list, a machine learning algorithm that suggests possible gifts and a tracker that keeps a history of each present given. And since people at your company love the program so much, you’ve made it an external service available to anybody. Keeping all the data in one file would be too slow. So you’ll need to move to a different solution. You have to use a <em><strong>fully-fledged database server</strong></em>. Probably even running on a separate machine than the one running the secret Santa service. And there’s even one more step after that. If the service becomes really really popular, you might notice that your database isn’t fast enough to serve all the queries being requested. In that case, you can add a caching service like <em><strong>memcached</strong></em> which keeps the most commonly used results in RAM to avoid querying the database unnecessarily. So we’ve gone from hosting the data in a CSV file to having it in a SQLite file then moving it to a database server and finally using a dynamic cacher in front of the database server to make it run even faster.</p>
<p>A similar progression can happen on the user facing side of the same project. Initially, we set the Santa service would simply send emails to the people on the list. That’s fine if it’s a small group and there’s one person in charge of the script. But as the project grows more complex, you’d want to have a website for the service to let people do things like check who their assigned person is and create wish lists. Initially, this could just be running on a web server on the same machine as the data. If the website gets used a lot, you might need to add a <em><strong>caching service</strong></em> like <strong>Varnish</strong>. This would speed up the load of dynamically created pages. And eventually, this still might not be enough. So you need to distribute your service across many different computers and use a <em><strong>load balancer</strong></em> to distribute the requests. You could do this in-house with separate computers hosted at your company, but this means that as the application keeps growing you need to add more and more servers. It might be easier to use <em><strong>virtual machines</strong></em> running in the <em><strong>cloud</strong></em> that can be added or removed as the load sustained by the service changes.</p>
<p>These examples show how important it is to find the right solution for each problem. It makes no sense to deploy a multi server web service with a distributed database for storage when you’re only going to have a few dozen users. You just need to pay attention to how the service is growing to know when you need to take the next step to make it work best for the current use case.</p>
<h3 id="dealing-with-complex-slow-systems">Dealing with Complex Slow Systems</h3>
<p>In our last section, we discussed how systems that grow in usage also grow in complexity. In large complex systems, we have lots of different computers involved. Each one doing a part of the work and interacting with the others through the network.</p>
<p>For example, think of an e-commerce site for your company. The web server is the part of the system that directly interacts with external users. Another component is the database server, which is accessed by the code that handles any requests generated from the website, and depending on how the whole system is built, you might have a bunch of other services involved doing different parts of the work. There could be a billing system that generates invoices once orders are placed. A fulfillment system used by the employees preparing the orders for customers. A reporting system that once a day creates a report of all the sales placed and possibly more. On top of this, you should probably have backup, monitoring, testing infrastructure, and so on.</p>
<p>A system like this can be tricky to debug and understand. What do you do if your complex system is slow? As usual, what you want to do is find the bottleneck that’s causing your infrastructure to underperform. Is it the generation of dynamic pages on the web server? Is it the queries to the database? Is it doing the calculations for the fulfillment process? Figuring this out can be tricky. So <em><strong>one key piece is to have a good monitoring infrastructure</strong></em> that lets you know where the system is spending the most time.</p>
<p>Say you notice that getting the web pages is pretty slow. But when you check the web server, you see that it’s not overloaded. Instead, most of the time is spent waiting on network calls, and when looking at your database server, you find that it’s spending a lot of time on Disk I/O. This shows that there’s a problem with how the data is being accessed in the database. One thing to look at is the <em><strong>indexes</strong></em> present in the database. When a database server needs to find data, it can do it much faster if there’s an index on the field that you’re querying for. On the flip side, if the database has too many indexes, adding or modifying entries can become really slow because all of the indexes need updating. So we need to look for a good balance of having indexes for the fields that are actually going to be used. If the problem is not solved by indexing and there are too many queries for the server to reply to all of them on time, you might need to look into either caching the queries or distributing the data to separate database servers.</p>
<p>Now what if when you try to figure out why the service is slow, you see that the CPU on the web serving machine is saturated. The first step is to check if the code of the service can be improved using the techniques that we explained earlier. If it’s a dynamic website, we might try adding caching on top of it. But if the code is fine and the cache doesn’t help because the problem is that there’s just too many requests coming in for one machine to answer all of them, you’ll need to distribute the load across more computers.</p>
<p>To make this possible, you might need to reorganize the code so that it’s capable of running in a distributed system instead of on a single computer. This might take some work, but once you’ve done it, you can easily scale your application to as many requests as needed by adding more computers to the system, and finally, make sure that you actually need to do whatever you’re doing. Lots of times, as projects evolve, we’re left with a scary monster of layer after layer of complex code. If we think about what our system is doing for a few minutes, we might end up discovering that there’s a whole piece that wasn’t needed at all and it was making our servers do unnecessary work all along. If all of this is starting to sound too difficult and scary, don’t worry. <strong>_Remember</strong> that if you ever need to deal with such complex systems, one of your best tools is to ask your colleagues for help_.</p>
<h3 id="using-threads-to-make-things-go-faster">Using Threads to Make Things Go Faster</h3>
<p>Our company has an e-commerce website that includes a bunch of images of the products that are up for sale. There’s a rebranding coming up, which means that all of these images will need to be replaced with new ones. This includes both the full-size images and the thumbnails. We have a script that creates the thumbnails based on the full-size images. But there’s a lot of files to process, and our script is taking a long time to finish. It looks like it’s time to take it up a notch and use something better to do the resizing. We’ll start by trying out the current script as-is using a set of 1,000 test images. There’s more images to convert, but it’ll be easier to test the speed of our script with a smaller batch. We’ll execute our program using the time command to see how long it takes.</p>
<p>It took about two seconds for 1,000 images. This doesn’t seem too slow, but there’s tens of thousands of images that need converting, and we want to make sure that the process is as fast as possible. Let’s try making this go faster by having it process the images in parallel. We’ll start by importing the <em><strong>futures</strong></em> submodule, which is part of the <strong>concurrent</strong> module. This gives us a very simple way of using Python threads.</p>
<p>To be able to run things in parallel, we’ll need to create an <strong>executor</strong>. <strong>An Executor is the process that’s in charge of distributing the work among the different workers</strong>.</p>
<p><strong>The futures module provides a couple of different executors, one for using threads and another for using processes</strong>. We’ll go with the ThreadPoolExecutor for now.</p>
<p>Now the function that does most of the work in this loop is process_file. Instead of calling it directly in the loop, we’ll submit a new task to the executor with the name of the function and its parameters.</p>
<p>Our for loop now creates a bunch of tasks that are all scheduled in the executor. The executor will run them in parallel using threads. An interesting thing that happens when we use threads is that the loop will finish as soon as all tasks are scheduled. But it will still take a while until the tasks complete. So we’ll add a message saying that we’re waiting for all threads to finish, and then call the shutdown function on the executor. This function waits until all the workers in the pool are done, and only then shuts down the executor.</p>
<p>All right, we’ve made the change, let’s save our script and test it out.</p>
<p>Our script now takes 1.2 seconds. That’s a nice improvement over the two seconds we saw before. See how the user time is higher than the real time? By using multiple threads, our script is making use of the different processors available in the computer. And this value shows the time used on all processors combined. What do you think will happen if we try to use processes instead of threads? Let’s try this out by changing the executor that we’re using.</p>
<p>By changing the executor to the ProcessPoolExecutor, we tell the futures module that we want to use processes instead of threads for the parallel operations. Let’s save and try this one out now.</p>
<p>Wow, this is now taking less than a second to finish, and the user time has gone up even more. This is because, by using processes, we’re making even more use of the CPU. The difference is caused by the way threads and processes work in Python. Threads use a bunch of safety features to avoid having two threads that try to write to the same variable. And this means that when using threads, they may end up waiting for their turn to write to variables for a few milliseconds, adding up to the small difference between the two approaches.</p>
<p>In this section, we looked into how we can add threading support to a Python script to make better use of our processor power. There’s still more improvements that we can make to our script, like checking if the thumbnail exists and is up to date before doing the conversion. Or adding a second progress bar while waiting for tasks to finish, to make it clear that our script is doing its job. We won’t go into those here, but if you’re interested, you can explore those possibilities on your own.</p>
<h3 id="more-about-complex-slow-systems">More About Complex Slow Systems</h3>
<p>We only touched briefly on the ways we can use concurrency to improve our programs. If you’re interested in learning more,<a href="https://realpython.com/python-concurrency/">this article</a> from Real Python has a lot of details on the different ways to use concurrency in Python.</p>
<p>Check out the following links for more information:</p>
<ul>
<li>
<p><a href="https://realpython.com/python-concurrency/">https://realpython.com/python-concurrency/</a></p>
</li>
<li>
<p><a href="https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32">https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32</a></p>
</li>
</ul>
<h3 id="module-2-wrap-up-slowness">Module 2 Wrap Up: Slowness</h3>
<p>Over the last few videos we’ve learned about the many different factors that can cause our computer to run slowly. We didn’t cover absolutely every possible cause, but we did check out the most common reasons:</p>
<ul>
<li>
<p>We talked about how the first thing to do when faced with a slow system is to identify the <em><strong>bottleneck</strong></em>. To do this, you’ll need to understand how each component interacts with the system and what resource is being exhausted. Sometimes the root cause is that the hardware isn’t enough, or maybe it’s just that there are too many things happening at the same time. Other times the problem might be in the code itself.</p>
</li>
<li>
<p>When trying to fix a program that’s slow, we should avoid code that does expensive operations. We went over several best practices to help us write better performing code. We discussed when to use the right data structures, how to avoid expensive loops, and how to keep results local, by creating a cache for example.</p>
</li>
<li>
<p>We then spent some time reviewing complex systems and looked at some examples on how to troubleshoot slowness in this type of environment. Next time you have to debug a performance problem, you’ll be able to think about what the bottleneck is, look for what’s exhausting that resource, and come up with ideas for how to make things go faster.</p>
</li>
</ul>
<p>I hope you’re starting to see how the skills you’re picking up can really help you as you face some of these same issues at your workplace. There’s no way you’re going to be able to solve every issue, but hopefully you’re starting to feel even more confident in your skills and abilities.</p>
<h2 id="why-programs-crash">Why Programs Crash</h2>
<h3 id="intro-to-module-3-crashing-programs">Intro to Module 3: Crashing Programs</h3>
<p>In the last module, we learned about a bunch of things that can make our computer, our code, or our systems run slow. We looked into the root causes and the possible remedies that might improve performance. In this module, we’ll look at another area of IT that often keeps us busy. The many things that can cause programs to crash unexpectedly.</p>
<p>If you’ve used computers, you’ve seen software crash at one time or another. A program terminates unexpectedly, a device reboots for no apparent reason, the operating system hangs and we lose all our unsaved work.</p>
<p>Generally, the cause of these crashes is that the software ran into an unexpected situation, a state that the developers didn’t anticipate. Because these are unexpected situations, they can be triggered by very broad range of things.</p>
<p>It could be a hardware problem, like a broken ramjet that causes a program to get invalid data when trying to access the memory. There could be a bug in some part of the code, which does an unsupported operation, like trying to read an element from an empty list.</p>
<p>It could be an issue in the overall system, like if a program expects a certain library to be present or a certain directory to exist, but they don’t or there could be a problem with the input provided by the user. Like if we ask the user to enter a number and they enter a string instead.</p>
<p>The list goes on. There are a ton of things that can cause a crash. Instead of knowing all of them, we need to learn to reduce the scope of the problem so that we can get to the bottom of it. In the next few sections, we’ll learn a bunch of different techniques that we can use to understand the root causes and how to fix them, or at least lessen the damage when fixing is not possible. We’ll first look at how we can understand the problem. We’ll then check out what we can do when we don’t have access to change the program’s code, and what we can do when we do have access to the source code, even if it’s not our own code. Finally, we’ll also look at what to do when the problem isn’t just one computer crashing, but a larger incident affecting complex systems. We’ll also dive into how to document a problem and it’s solutions, and how to learn from our mistakes by writing <strong>postmortems</strong>. As usual, we’ll put all this knowledge into action by solving real-world problems. You’ll have the opportunity to try fixing a complex crashing problem by the end of the module.</p>
<h3 id="systems-that-crash">Systems That Crash</h3>
<p>In this section, we’re going to talk about system crashes. There are a ton of different reasons why applications crash.</p>
<p>When we come across a program that terminates unexpectedly, we go through our usual cycle of gathering information about the crash, digging in until we find the root cause, and then applying the right fix.</p>
<p>Say for example that a user asks for your help with a problem on their computer. When you ask for details, the user tells you that the internal billing application crashed while they were trying to generate an invoice for a customer. Now, this could be caused by lots of different things. So <em><strong>what you need to do is reduce the scope of the problem</strong></em>, and remember, <em><strong>you want to start with the actions that are easier and faster to check</strong></em>.</p>
<p>As a first step, you tried looking at the <strong>logs</strong> to see if there’s any error that may point to what’s happening, but you only find an error saying application terminated and no useful information. So you check if the user can reproduce the problem by doing this same action on a different computer. You ask the user to try this out, and it turns out on a different machine that can generate the invoice just fine. So that means that the problem just has to do with the installation or configuration on that specific computer. Great news! You’ve already reduced the scope to something machine-specific.</p>
<p>Another thing that you might want to check is if this happens reliably. Do all invoice generations fail? Is it confined to one specific product or customer? For this example, let’s say that when you ask the user to try generating other invoices, it works just fine even for the same customer. Okay, you think maybe this problem was with a specific order for that specific customer on that specific computer. That’s rather suspicious, but not so fast. The user tells you that after creating all the invoices for the day, they tried to generate a report, and the application crashed again. But then it worked the next time. You double-check with other users and find out the application isn’t crashing when they use it. So what does this mean? The application seems to be crashing randomly but only on that computer.</p>
<p>To further reduce the scope, you’ll want to know if it’s just that application or the whole system. To check this out, you can try moving away the local configuration for the program and using the default configuration instead, or maybe even reinstalling the application. You might also ask the user if they’ve seen crashes on any other application. For this example, let’s say that reinstalling the application and running it with the default configuration still leads to random crashes. I’m pressed to remember, the user tells you that their web browser also crashed last week when they were using the internal webmail. At this point, the information points to a problem in the overall system, either the hardware or the OS installation. If you have a spare computer available, it might make sense to give one to the user at this point so that they can go back to work while you try to figure out the root cause of the problem. What can you do to further reduce the scope? By now, there’s a high likelihood if the problem being hardware related. So one thing you could do is try taking the hard drive out of the computer and putting it into a different computer. This works best when you already have a spare case that you know works well so that you can use it for tests like these. That way you can quickly check if it’s a problem with the data and the drive or the rest of the computer. Let’s say that after putting the hard drive in the other computer, the applications run without unexpected crashes. This means that some hardware component is at fault. The next step is to find out which one. Given the random crashes, one thing to check would be the RAM.</p>
<p>Memory chips deteriorate over time. When they do, the computer might write data to some part of the memory and then get a totally different value when trying to read it back. To check the health of our RAM, we can use the memtest 86 tool to look for errors. We run this tool on boot instead of the normal operating system so that it can access all of the available memory and verify if the data written to memory is the same when it tries to read it back. If the RAM is fine, you can check if the computer’s overheating by looking at the sensor data provided by the OS. If that’s not the case, check if there’s a problem with external devices like a graphics card or sound card. You can do this by disconnecting or replacing the devices present in the computer and checking if the crashes still occur. So what can you do if when putting the hard drive in a separate computer, you still get the strange caches? This means the problem is in the drive itself or the OS installation. As with RAM, our hard drives age. At some point, the data that the computer reads stops matching what was originally stored. Each OS ships its own battery of hard drive checking tools, and you should familiarize yourself with ones in the OS you’re working with. You’ll want to look at the output of the tools that check the disk for bad sectors, and you’ll also want to use these S.M.A.R.T. tools which can help detect errors and even try to anticipate problems before they affect the computer’s performance. What can you do with the hard-drive turns out to be fine?</p>
<p>You’d need to look into the possible OS issues, but before doing that, ask yourself, <em><strong>is it worth it?</strong></em> Looking to what’s wrong with the installation can take a lot of valuable time. If the installation is easy to replicate, then just reinstalling the OS might be faster and simpler than looking into why it broke.</p>
<p>Alright, so that’s a glimpse of how you can try to diagnose a system that’s unstable and behaving in weird ways. But often, you’ll be dealing with a specific application that’s misbehaving. In this case, it’s almost certainly above in the application’s code that’s not taking into account a situation that, though unexpected, can sometimes occur.</p>
<h3 id="understanding-crashing-applications">Understanding Crashing Applications</h3>
<p>When an application crashes and we don’t know why we’ll want to look for <strong>logs</strong> that might relate to the failure. To look at logs on <em><strong>Linux</strong></em> will open the <em><strong>system log files and /var/log or the user log files and /xsession-errors file</strong></em>. On <em><strong>macOS</strong></em> we generally use the <strong>Console app</strong> to look at logs and the <strong>Event Viewer</strong> on <em><strong>Windows</strong></em>.</p>
<p>So what kind of data should you look for in these logs? Most logs have a date and time for each line locked knowing when the application crashed you can look for a log line around that time. And try to find an error message related to the application that crashed. Sometimes the errors will be self-explanatory like permission denied no such file or directory connection refused. Sometimes it will be a cryptic message and you have no idea what it means. Ever we have an error message no matter how weird it seems we can search for it online to try to figure out its meaning. If we’re lucky, we might find the official documentation of what that error means and what we can do about it. But even if that’s not available, will usually come across posts by others who have tackled a similar error and this additional information can help us understand what’s going on.</p>
<p>If there are no errors or the errors aren’t useful we can try to find out more info by enabling debug logging. Many applications generate a lot more output when debugging logging is enabled. We might need to enable it from a setting in the applications configuration file or a command line parameter to pass when running the application manually. By enabling this extra logging information, we can get a better idea of what’s actually causing the problem. And what do we need to do if there are no logs or error messages at all. In that case we need to use tools that let us see what going on inside the program. We call that a few are ready.</p>
<ul>
<li>On <em><strong>Linux</strong></em> we use <strong>strace</strong> to see what system calls a programs doing. The equivalent tool is called <strong>dtruss</strong> on <em><strong>macOS</strong></em>. <strong>Process monitor</strong> is a <em><strong>Windows</strong></em> tool that can also take a peek inside what’s going on inside a process on Windows. By tracing which system calls a program is doing we can see what files and directories it’s trying open what network connections it’s trying to make and what information it’s trying to read or write. This can give us a better idea of what caused the actual problem.</li>
</ul>
<p>We could find that the problem is caused by a resource not being present that the program expects to be present. Like we saw with the missing directory example in the earlier module or we could find that the program tries to interact with the graphics interface and there isn’t any because it’s a service running on a server. Or the program tries to open a file but the user running the software doesn’t have the necessary permissions.</p>
<p>If the application used to work fine and recently started crashing. It’s useful to look into what changed in between. The first thing is to check if the issue is caused by a new version of the application itself. Maybe there’s a bug in the new version that causes the crash or maybe the way that we’re using the application is no longer supported. But that’s not the only possible change that could trigger crashes.</p>
<p>It could also be that a library or service used by our application changed and they no longer work well together or it could be that there was a configuration change in the overall environment. Like if the user isn’t in a specific group anymore or if the files that the application used are in a different location.</p>
<p>When trying to figure out what changed, logs can also be a useful source of information. In the system log we can check which programs and libraries were recently updated checking for configuration changes might be harder depending on how you manage that configuration. If the settings are managed through a configuration management system and the values are stored in a Version Control System. Then you might be able to look at the history of changes and figure out which one triggered the failure.</p>
<p>We call that a few times already how important it is to have a reproduction case for a problem that we’re trying to solve. When we’re trying to debug an application that crashes finding a reproduction case can help us both understand what’s causing the crash and figure out what we can do to fix it. So it’s valuable to spend some time figuring out the state that triggers the crash. This includes the overall system environment the specific application configuration the inputs to the application the outputs generated by the application the resources that uses and the services it communicates with. When trying create the reproduction case it might be useful to start from a clean slate and slowly put the pieces in place until the crash triggers. This might include trying out the application with the default configuration instead of the local one or on a freshly installed computer instead of the computer where it’s crashing. And remember we want to make the reproduction case as small as possible this lets us better understand the problem and also quickly check if its present or not when we attempt to fix it. And even if we end up unable to fix the issue having a small and simple reproduction case is extremely helpful in reporting a bug to the program’s developers.</p>
<p><strong>So to sum this up to find the root cause of a crashing application will want to look at all available logs figure out what changed trace the system or library calls the program makes and create the smallest possible reproduction case.</strong> After doing all of this, we should have some idea of what the root cause of the issue is and maybe even how to fix it. The strategy for fixing problems will depend on whether we can fix the code or not. In our next video, we’ll check out what you can do when you can’t fix the program and need to work around the issue. And in later sections, we’ll deep dive into strategies for fixing faulty code.</p>
<h3 id="what-to-do-when-you-cant-fix-the-program">What to do when you can’t fix the program?</h3>
<p>One of the great things about working in IT is that we can tell the computer what to do and it will follow our orders. When dealing with unexpected behavior in the software written by other people though, we might not always be so lucky.</p>
<p>It could be that we’re dealing with proprietary software and the source code isn’t available at all, or we might have access to the source code but it’s written in a language that we don’t understand and so we can’t change it.</p>
<p>No matter the reason, what can you do if you need to fix an application that crashes and you can’t change the code. You’ll need to figure out a way of working around the problem and avoiding the crash. The actual workaround will depend on what the issue is that you’re trying to solve. Let’s do a rundown of some of the available options.</p>
<p>Say you figured out that the issue was caused by a specific data input that makes the application crash. The crashes only happen when the input isn’t in the format the code expects. Some of your systems generate data in XML format which used to work fine with the previous version of the software but the new version now requires all data to be in a YAML format. In this case you can write a script that pre-processes the data and make sure that it’s in the format that the program expects.</p>
<p>Similarly if the problem is caused by an external service that the application uses and that’s no longer compatible, we could write a service to act as a proxy and make sure that both sides see the requests and responses they expect. This type of compatibility layer is called a <strong>Wrapper</strong>.</p>
<p><strong>A Wrapper is a function or program that provides a compatibility layer between two functions or programs so they can work well together</strong>. Using Wrappers is a pretty common technique when the expected output and input formats don’t match. So if you’re faced with some sort of compatibility problem don’t be afraid to write a Wrapper to work around it.</p>
<p>Another possibility you might need to look at is if the overall system environment is it working well with the application. In this case, you might want to check what environment the applications developers recommend and then modify your systems to match that. This could be running the same version of the operating system using the same version of the dynamic libraries or interacting with the same back end services. Say the application was developed and tested on Windows 7, if you run into problems while trying to run it under Windows 10, you might want to use Windows 7 instead or if the application was developed and tested for Ubuntu and you’re having trouble running it under Fedora, you might want to try running it on Ubuntu instead.</p>
<p>And what can you do if you can’t make the environment match? This could happen, for example, if there’s another application that requires a different version of the same library or you can’t change a certain configuration setting because it’s required to access a different service. In this case, you might want to consider running the application inside a <em><strong>virtual machine</strong></em> or maybe a <em><strong>container</strong></em>. These are two different things but we won’t go into details of how they are different here. All you need to know right now is they both let you run the affected application in its own environment without interfering with the rest of the system. This is what we need if we want the environment to be different than the one other Applications are using on the same computer.</p>
<p>Sometimes we can’t find a way to stop an application from crashing but we can make sure that if it crashes it starts back again. To do this, we can deploy a <strong>watchdog</strong>. <strong>A watchdog is a process that checks whether a program is running and when it’s not starts the program again</strong>. To implement this, we need to write a script that stays running in the background and periodically checks if the other program is running. Whenever the check fails the watchdog will trigger the program to restart. Doing this won’t avoid the crash itself. But it will at least ensure that the service is available. This works well for services where availability matters more than running continuously and no matter how you work around the issue, <strong>remember to always report the bug to the application developers</strong>.As we called out, if you have a good reproduction case for your issue, it makes it easier for the developers to figure out what’s wrong and how to fix it. So when you report a bug make sure you include as much information as possible, share good reproduction case and answer the questions that we mentioned earlier on:</p>
<ul>
<li>
<p>What were you trying to do?</p>
</li>
<li>
<p>What were the steps you followed?</p>
</li>
<li>
<p>What did you expect to happen?</p>
</li>
<li>
<p>What was the actual outcome?</p>
</li>
</ul>
<h3 id="internal-server-error-example">Internal Server Error Example</h3>
<p>A colleague has alerted us that a webpage on our Web server isn’t working. As we’ve done before, we need to figure out what this means exactly. We asked our colleague for more details and they told us that the failing webpage is at <a href="http://site.example.com/logs">site.example.com/logs</a>. Let’s check out if this is failing for us as well. The server responded with a 500 error. This error usually means that something on the server side of the application crashed, but we have no idea what. We’ll need to investigate to find out more information. Let’s connect to the Web server and try to figure out what’s up.</p>
<p>The first step is looking at logs, as we called out on Linux systems, logs are located in <strong>/var/log</strong>. To do that, we’ll use the <strong>date</strong> command to check the current date. Let’s change into that directory and check out if there are any recent logs about our error and then the <strong>ls -lt</strong> command which <em>sorts the files by the last modified date</em> connecting it to the <strong>head</strong> command to keep the top 10 lines. We just triggered the error but there doesn’t seem to be anything recent in the logs. Just in case, let’s check out the last lines insists log using <strong>tail</strong>.</p>
<p>Nothing interesting here. We need to figure out how we can get more information, but we don’t even know which web surfing software is being used on this computer. But we do know that the Web server is running on <em>port 80</em>, the default web serving port. How can we find which software is listening on port 80? We can use the <strong>netstat</strong> command which can give us a bunch of information about our network connections depending on the flags we pass. This command accesses a bunch of sockets that are restricted to route the administrator user on Linux. So we’ll need to call it with <strong>sudo</strong> which lets us <em>run commands as root</em>, and then we’ll pass a bunch of flags netstat. We’ll use -n to print numerical addresses instead of resolving host names, -l to only check out the sockets that are listening for connection, and -p to print the process ID and name to which each socket belongs. Since we only care about port 80, we’ll connect the output to a grep command checking for colon 80.</p>
<pre class=" language-bash"><code class="prism  language-bash">$ <span class="token function">sudo</span> <span class="token function">netstat</span> -nlp <span class="token operator">|</span> <span class="token function">grep</span> :80
<span class="token punctuation">..</span>.
</code></pre>
<p>Great, we got new information. We see that the process listening in port 80 is called “<strong>nginx</strong>.” One of the popular web serving applications out there. We now want to check out the configuration for our site. Configuration files on Linux are stored in the <em><strong>etc</strong></em> directory. So let’s look at <em>etc/nginx</em>.</p>
<p>There’s a bunch of files here. Lots of different configuration options that you can set in the Web server. We’re looking for the configuration related to a specific site. So let’s look at etc/nginx sites-enabled.</p>
<p>There are two files here one for the default site and one for the <a href="http://site.example.com">site.example.com</a> site that’s the one we want. Let’s open it with the VI editor.</p>
<p>There’s not a lot here, but at the bottom we see that it says uwsgi_pass, and then the local host address followed by a different port number. It seems that this website isn’t being served directly from nginx, instead, the software is passing the control of the connections to uWSGI which is a common solution used to connect a Web server to programs that generate dynamic pages. So let’s see if we can find the configuration for that one. We’ll exit VI with a colon q and then see if there’s anything interesting in etc/uwsgi. Here we only see two directories, apps-available and apps-enabled. Let’s say it’s an apps-enabled. Cool. We found the uWSGI configuration for our site. Let’s check it out.</p>
<p>Nice. This file has a lot more information. We see that the main directory for the application is srv/site.example.com that the applications run as the www-data user and group, that it’s running a Python three script called <a href="http://prod.py">prod.py</a> that the log is stored in var/log/site.log and a bunch of other things. All right. Let’s use this extra information and see if we can find out what’s that. Let’s exit with colon q once more and then check out that log file. Weird, the log file has a size of zero, that doesn’t seem right. Let’s see if we can find out anything else by looking at the Python script that’s executed by uwsgi srv/site.example.com <a href="http://prod.py">prod.py</a>.</p>
<p>There’s a few different webpages configured in this file. It uses bottle which is a Python module to generate dynamic web pages. At the bottom, we see the configuration for the logs page that’s currently failing. Hopefully, a colleague left a comment saying that we can get debugging information by uncommenting the line that calls bottle.debug. That’s exactly what we need. To uncomment this line, we need to have write access to the file though, and VI is open in read only mode currently. Let’s exit an open again with <strong>sudo</strong> to be able to modify it. Okay. We’ve made the change, let’s save it and reload uwsgi as the instructions say. We’ll do this by running <strong>sudo service uwsgi reload</strong>. We’ve added debugging information. Hopefully, that will tell us why the pages failing. Let’s reload the website and see what happens.</p>
<p>Great news, this time we see a trace back of the error and we see that the issue is that the application is getting a permission denied error when trying to open var/log/site.log. Remember that we thought it was weird that the file was empty, it seems that it’s somehow broken. Let’s look at it again, this time let’s check if there are any other files that start with site.</p>
<p>So there’s a site.log file and a site.log.1 file. That’s pretty common when using logrotate to rotate the logs and avoid them getting too big. But there’s something else afoot here. See how one file belongs to the root user and the other belongs to the www-data user. If you look at the permissions of the file, you might notice that they are set to allow the owner to write them and the owner and the group to read them, but the rest of the users can’t access them. We saw earlier that the application is running with the www-data user. So if site.log belongs to the root user, the application won’t be able to either read or write to this log file. Seems like we found the root cause of our issue. Let’s change the owner of the site.log file to fix the immediate problem. Let’s try reloading our page now.</p>
<p>Yes, it works. The log is empty now because the application have not been able to write to it. But if we keep reloading, we’ll see how it populates with our entries. All right. We’ve fixed the immediate problem our Web pages working once again, but we still need to take care of the long-term remediation. Why was the ownership of the file wrong? We suspect that there might be something wrong with the log rotate configuration but we’d need to keep looking to find out what’s up with that.</p>
<p>In this section, we looked into how we can figure out what’s up with an application that’s failing. We checked out a bunch of different tools and ideas that can help us understand what’s going on and get more information until we can find the root cause. I hope we’re starting to see how these lessons provide valuable tools for diagnosing and solving issues that will for sure occur at your job.</p>
<h3 id="resources-for-understanding-crashes">Resources for Understanding Crashes</h3>
<p>There’s a ton of different reasons why a computer might crash. This <a href="https://www.scientificamerican.com/article/why-do-computers-crash/">Scientific American article</a> discusses many of the possible reasons, including hardware problems and issues with the overall operating system or the applications on top.</p>
<p>On Linux or MacOS, the worst kind of crash is called a Kernel Panic. On Windows, it’s known as the <a href="https://en.wikipedia.org/wiki/Blue_Screen_of_Death">Blue Screen of Death</a>. These are situations where the computer completely stops responding and only a reboot can make it work again. They don’t happen often, but it’s good to understand what they mean: the whole OS encountered an error and it can’t recover.</p>
<p>We called out that reading logs is super important. You should know how to read logs on the operating system that you’re using. Here are some resources for this:</p>
<ul>
<li>
<p><a href="https://www.digitalmastersmag.com/magazine/tip-of-the-day-how-to-find-crash-logs-on-windows-10/">How to find logs on Windows 10</a> (Digital Masters Magazine)</p>
</li>
<li>
<p><a href="https://www.howtogeek.com/356942/how-to-view-the-system-log-on-a-mac/">How to view the System Log on a Mac</a> (How-to Geek)</p>
</li>
<li>
<p><a href="https://www.fosslinux.com/8984/how-to-check-system-logs-on-linux-complete-usage-guide.htm">How to check system logs on Linux</a> (FOSS Linux)</p>
</li>
</ul>
<p>You also need to be familiar with the tools available in your OS to diagnose problems. These are the tools we called out, but you don’t need to limit yourself to them:</p>
<ul>
<li>
<p><a href="https://docs.microsoft.com/en-us/sysinternals/downloads/procmon">Process Monitor for Windows</a> (Microsoft)</p>
</li>
<li>
<p><a href="https://www.howtoforge.com/linux-strace-command/">Linux strace command tutorial for beginners</a> (HowtoForge)</p>
</li>
<li>
<p><a href="https://stackoverflow.com/questions/31045575/how-to-trace-system-calls-of-a-program-in-mac-os-x">How to trace your system calls on Mac OS</a> (<a href="http://etcnotes.com/posts/system-call/">etcnotes.com/posts/system-call/</a> link broken)</p>
</li>
</ul>
<h2 id="code-that-crashes">Code that Crashes</h2>
<h3 id="accessing-invalid-memory">Accessing Invalid Memory</h3>
<p>In our earlier sections, we looked into a bunch of different things that can make software crash and what we can do about them when we can’t fix the code. If we’re able to make the application behave correctly though, we’ll have a lot more options for dealing with the crash. Of course to apply these fixes, we’ll need to understand why the crash is even happening.</p>
<p>One common reason a program crashes is it’s trying to access <strong>invalid memory</strong>. To understand what this means, let’s quickly explain how using the memory works on modern operating systems. Each process running on our computer asks the operating system for a chunk of memory. This is the memory used to store values and do operations on them during the program’s execution. The OS keeps a mapping table of which process is assigned which portion of the memory. Processes aren’t allowed to read or write outside of the portions of memory they were assigned. So <strong>accessing invalid memory means that the process tried to access a portion of the system’s memory that wasn’t assigned to it</strong>.</p>
<p>Now, how does this even happen? During normal working conditions, applications will request a portion of the memory and then use the space at the OS assigned to them. But programming errors might lead to a process trying to read or write to a memory address outside of the valid range. When this happens, the OS will raise an error like segmentation fault or general protection fault. What kind of programming error is this? It typically happens with <em>low-level languages like C or C++</em> where the programmer needs to take care of requesting the memory that the program is going to use and then giving that memory back once it’s not needed anymore. In these languages, the variables that store memory addresses are called <em><strong>pointers</strong></em>. They’re just like any other variable and code that can be modified as needed. So if a pointer is set to a value outside of the valid memory range for that process, it will point to invalid memory. If the code then tries to access the memory the pointer points to, the application will crash.</p>
<p>Common programming errors that lead to <em><strong>segmentation faults or segfaults</strong></em> include forgetting to initialize a variable, trying to access a list element outside of the valid range, trying to use a portion of memory after having given it back, and trying to write more data than the requested portion of memory can hold.</p>
<p>So what can you do if you have a program that’s said vaulting? The best way to understand what’s going on is to attach a <em><strong>debugger</strong></em> to the faulty program. This way when the program crashes, you’ll get information about the function where the fault happened. You’ll know the parameters that the function received and find out the address that was invalid. That might already be enough to understand the problem. Maybe a certain variable is being initialized to late or the code is trying to read too many items on a list. If that’s not enough, the debugger can give you a lot more detail on what the application is doing and why the memories invalid. For this to be possible, we’ll need our program to be <em><strong>compiled with debugging symbols</strong></em>. This means that on top of the information that the computer uses to execute the program, the executable binary needs to include extra information needed for debugging, like the names of the variables and functions being used. These symbols are usually stripped away from the binaries that we run to make them smaller. So we’ll need to either recompile the binary to include the symbols, or download the debugging symbols from the provider of the software if they’re available. Linux distributions like Debian or Ubuntu ships separate packages with the debugging symbols for all the packages in the distribution. So to debug and application that’s segfaulting, we download the debugging symbols for that application, attach a debugger to it, and see where the fault occurs.</p>
<p>When doing this, we might find that the crash happens inside a call to a library function. This is separate from the application itself, so we need to install the debugging symbols for that library. We might need to repeat this cycle a few times before we can identify the portion of the code that’s buggy. Microsoft compilers can also generate debugging symbols in a separate PDB file. Some Windows software providers let users download the PDB files that correspond to their binaries to let them properly debug failures.</p>
<p>One of the trickiest things about this invalid memory business is that we’re usually dealing with <em><strong>undefined behavior</strong></em>. This means that the code is doing something that’s not valid in the programming language. The actual outcome will depend on the compiler used, how the operating system assigns memory to processes, and even the version of the libraries in use. A program that runs fine on a computer running Windows triggers a segfault on a computer running Linux and vice versa.</p>
<p>When trying to understand problems related to handling invalid memory, <em><strong>Valgrind</strong></em> can help us a lot. Valgrind is a very powerful tool that can tell us if the code is doing any invalid operations no matter if it crashes are not. Valgrind lets us know if the code is accessing variables before initializing them. If the code is failing to free some of the memory requested, if the pointers are pointing to an invalid memory address, and a ton more things. Valgrind is available on Linux and Mac OS, and <em><strong>Dr. Memory</strong></em> is a similar tool that can be used on both Windows and Linux.</p>
<p>What do we do when we finally discover the cause of the segfaults? You’ll want to either change the code yourself or get the developers to fix the problem in the next version. This might sound scary if you’ve never programmed in the language used by the application. But when you know what’s wrong with the code, it’s usually not that hard to figure out how to fix it. If a variable is initialized too late, fixing the problem can be as easy as moving the initialization to the right part of the code, or if a loop is accessing an item outside of the length of the list, you might solve the issue by checking that there aren’t more iterations than needed.</p>
<p>Throughout this program, we’ve been teaching you these concepts so you can apply them to any piece of code no matter which language the program is using. So don’t be afraid to put this into practice. You’ve got the skills for it. If the program is part of an open source project, you might find that someone else has already done the work, and so you can apply a patch available online. If there’s no patch and you can’t say you’re the bug out yourself, you can always get in touch with the developers and ask a fake and fix the issue and create the necessary patch. In high-level languages like Python, the interpreter will almost certainly catch these problems itself. It will then throw an exception instead of letting the invalid memory access reach the operating system. But still those exceptions can be pretty annoying.</p>
<h3 id="unhandled-errors-and-exceptions">Unhandled Errors and Exceptions</h3>
<p>In our last section, we talked a lot about what happens when a program tries to access invalid memory. Correctly handling memory is a hard problem, and that’s why there’s a bunch of different programming languages like Python, Java, or Ruby that do it for us. But that doesn’t mean programs written in these languages can’t trigger weird problems.</p>
<p>In these languages, when a program comes across an unexpected condition that isn’t correctly handled in the code, it will trigger errors or exceptions. In Python, for example, we could get an index error if we tried to access an element after the end of a list. We might get a type error or an attribute error if we try to take an action on a variable that wasn’t properly initialized or division by zero error if we tried to well, divide by zero. When the code generates one of these errors without handling it properly, the program will finish unexpectedly.</p>
<p>In general, unhandled errors happen because the codes making wrong assumptions maybe the program’s trying to access a resource that’s not present or the code assumes that the user will enter a value but the user entered and empty string instead. Or maybe the application is trying to convert a value from one format to another and the value doesn’t match the initial expectations. When these failures happen, the interpreter that’s running the program will print the type of error, the line that caused the failure, and the traceback. <strong>The traceback shows the lines of the different functions that were being executed when the problem happened</strong>. In lots of cases, the error message and traceback info already gives us enough to understand what’s going on, and we can move on to solving the problem. But sadly, that’s not always the case.</p>
<p>The fact that a piece of code crashes on one function doesn’t mean that the error is necessarily in that function. It’s possible, for example, that the problem was caused by a function called earlier which set a variable to a bad value. So the function where the code crashes is just accessing that variable. So when the error message isn’t enough, we’ll need to debug the code to find out where things are going wrong.</p>
<p>For that, we can use the debugging tools available for the application’s language. For a Python, program we can use the <em><strong>pdb interactive debugger</strong></em> which lets us do all the typical debugging actions like executing lines of code one-by-one or looking at how the variables change values. When we’re trying to understand what’s up with a misbehaving function on top of using debuggers, it’s common practice to add statements that print data related to the codes execution. Statements like these could show the contents of variables, the return values of functions or metadata like the length of a list or size of a file. This technique is called <em><strong>printf debugging</strong></em>. The name comes from the <em><strong>printf function</strong></em> used to print messages to the screen in the C programming language. But we can use this technique in all languages, no matter if we use print, puts, or echo to display the text on the screen.</p>
<p>Let’s take this one step further. When changing code to print messages to the screen, the best approach is to add the messages in a way that can be easily enabled or disabled depending on whether we want the debug info or not. In <em><strong>Python</strong></em>, we can do this using the <strong>logging module</strong>. This module, lets us set how comprehensive we want our code to be. We can say whether we want to include all debug messages, or only info warning or error messages. Then when printing the message, we specify what type of message we’re printing. That way, we can change the debug level with a flag or configuration setting.</p>
<p>So you figured out why the unexpected exception was thrown, what do you do next? The solution might be fixing the programming error like making sure variables are initialized before they’re used or that the code doesn’t try to access elements after the end of a list. Or it could be that certain use cases that hadn’t been considered needs to be added to the code. In general, you’ll want to make the program more resilient to failures. Instead of crashing unexpectedly, you want the program to inform the user of the problem and tell them what they need to do.</p>
<p>For example, say you have an application that crashes with a permission denied error. Rather than the program finishing unexpectedly, you’ll want to modify the code to catch that error and tell the user what the permission problem is so they can fix it. For example, unable to write new files on temp, make sure your user has bright permissions on temp. In some cases, it doesn’t make sense for our program to even run if certain conditions aren’t met. In that case, it’s okay for the program to finish when the error is triggered. But again, it should do so in a way that tells the user what to do to fix the problem. For example, if it’s critical for an application to connect to a database but the database server isn’t responding, it makes sense for the application to finish with an error saying unable to connect to the database server. It also makes sense to include all details of the attempted connection like the host name, the port, or the username used to connect.</p>
<p><strong>To recap, if your program is crashing with an unhandled error, you want to first do some debugging to figure out what’s causing the issue. Once you figured it out, you want to make sure that you fix any programming errors and that you catch any conditions that may trigger an error</strong>. This way, you can make sure the program doesn’t crash and leave your users frustrated.</p>
<h3 id="fixing-someone-elses-code">Fixing Someone Else’s Code</h3>
<p>In our IT jobs, it’s pretty common to have to fix problems and code that we didn’t write ourselves. It might be because we’re working with a program that’s open-source or with a program that was developed by someone else inside the company. When this happens, we need to spend some time getting acquainted with the code so that we can understand what’s going on. Let’s do a rundown of some things that can help us with that.</p>
<p>If the code has comments and the functions are well-documented, reading these is a great place to start when trying to figure out what’s going on. Remember way back in the course when we first introduced Python, we talked about the importance of developing good habits when we’re writing code. <strong>Writing good comments is one of those good habits</strong> that pays off when trying to understand code written by others and also your past self.</p>
<p>Unfortunately, a lot of code doesn’t include enough comments, leaving us to try to understand it without enough context. If that’s the case, you can improve things by adding comments as you read the code and figure out what it’s doing. Writing these comments help you solidify your understanding. If you contribute those comments back to the original developers, you can help anybody else trying to understand the code.</p>
<p>Another thing that can help to understand someone else’s code is <strong>reading the tests</strong> associated to the code. Well-written tests can tell us what each function is expected to do. Looking at the existing tests can show us which use cases weren’t taken into account.</p>
<p>But what if there aren’t enough tests? Just like with writing extra comments, writing some tests of your own can help you better see what the code is supposed to do and improve overall quality of the code. This can also be really useful when modifying the original code, to ensure that changes you make don’t break the rest of the functionality.</p>
<p>In my job, I need to make changes to code written by other people a lot. I definitely read the comments and sometimes reference the tests too. But in the end, to really understand what’s going on, I just have to read through the code. But how do you even start reading through someone else’s code?</p>
<p>This depends a bit on personal preference and the size of the project. If there are only a couple of 100 lines of code, it’s feasible to read all of them. But when the project has thousands or tens of thousands of lines of code, you can’t really read the whole thing. You’ll need to focus on the functions or modules that are part of the problem that you’re trying to fix. One possible approach in this case, would be to start with the function where the error happened, then the function or functions that call it, and so on until you can grasp the contexts that led to the problem.</p>
<p>While this is of course much easier if it’s in a programming language that you’re familiar with, you don’t need to be an expert in the language to fix a bug in the program. If you’ve come across an error and debug the issue well enough to understand what’s going on, you might be able to fix the problem even if you’ve never seen that language before.</p>
<p>This is one of those skills that gets better with practice. So it might make sense to you to start practicing before you need to fix a problem in the code. Take a program that you both use and have access to its code and figure out how it does a specific action. Follow the code until you really understand what’s going on. For example, you could take the web server software you’re using and check out how it parses its configuration files, or take a look at one Python module you like, like Python Request for example, and figure out how it processes the data it receives. Doing this, you can get used to reading code written by others and understanding what it’s doing.</p>
<p>Another option is to pick an open-source project that you use. Look at the list of open issues and to have a go at fixing an easy one. To do that, you’ll need to find your way around the code, understand what it’s doing and what to change. By practicing doing this, you’ll improve your ability to quickly figure out what the code does and what needs to be changed, while helping improve the project’s overall quality.</p>
<h3 id="debugging-a-segmentation-fault">Debugging a Segmentation Fault</h3>
<p>Over the past sections, we’ve discussed a bunch of different types of crashes. Let’s now check out what a segmentation fault looks like in action. We have a simple example program that crashes with a seg fault. When an application crashes like this, it’s useful to have a <em><strong>core file</strong></em> of the crash. <strong>Core files store all the information related to the crash so that we or someone else can debug what’s going on</strong>. It’s like taking a snapshot of the crash when it happens to analyze it later. We need to tell the OS that we want to generate those core files. We do that by running the <strong>ulimit</strong> command, then using the <strong>-c</strong> flat for core files, and then saying <strong>unlimited</strong> to state that we want core files of any size. Once we’ve done that, we can try executing our example again.</p>
<p>All right, our crashing program has generated a core file. Let’s check it out using ls -l. This file contains all the information of what was going on with the program when it crashed. We can use it to understand why the program crashed by passing it to the <strong>GDB debugger</strong>. We’ll call it <strong>gdb -c core</strong> to give it a core file and then <strong>example</strong> to tell it where the executable that crashed is located.</p>
<p>When it starts, GDB shows a bunch of messages including its version, license, and how to get help. It then tells us that the program finished with a segmentation fault. It shows that the crash happened inside the strlen function in a file that’s part of the system libraries. The no such file or directory error that we’re seeing here means that we don’t have the debugging symbols for that system library, but that’s okay. We trust the strlen function to work correctly. It’s our code that’s buggy. Let’s look at the full backtrace of the crash by using the <strong>backtrace</strong> command.</p>
<p>The first element in the list is the function where the crash occurred. The second element is the function that called the function and so on. In this case, we see that the strlen function that failed was called by the copy parameters function in our code which was called by the main function. We can use the <strong>up</strong> command to move to the calling function in the backtrace and check out the line and copy parameters that caused the crash. We see that the faulty line is calling the strlen function, but it’s not clear why that would fail. We can get more contexts for the code that failed by calling the <strong>list</strong> command that shows the lines around the current one.</p>
<p>Here, we see a chunk of C code. If this is the first time you look at C code, it might seem a bit confusing. That’s okay. There are some similarities with Python, but also, some things that are pretty different. We see that the faulty line, line 10, is in the body of a for loop. The variable that the for loop uses to iterate is called i. Let’s check out the value of i using the print command.</p>
<p>GDB uses the dollar sign followed by a number to give separate identifiers to each result it prints. In this case, the result is one. In other words, when the crash happened, I had the value of one. Since this variable is being used to access an array called argv, let’s print the contents of the first element argv 0, and then the second element argv 1.</p>
<p>What are those weird numbers starting with 0x? Those are _<strong>hexadecimal number</strong>_s, and <em>they are used to show addresses in memory where some data is stored</em>. Here, GDB is telling us that the first element in the argv array is a pointer pointing to the./example string. The second element is a pointer to zero also known as a <em><strong>NULL pointer</strong></em>. <strong>Zero is never a valid pointer</strong>. It usually signals the end of data structures in C. So our code is trying to access the second element in the array, but the array only has one valid element. In other words, the for loop is doing one iteration to many. This is known as an <em><strong>off-by-one error</strong></em>, and it’s a <em>super common error</em>. In this case, the fix is really simple. We need to change the less than or equal sign to be a strictly less than sign so that the iteration stops one element before. In this video, we’ve got a sneak peek at what it’s like debugging C applications that crashed with a segmentation fault.</p>
<h3 id="debugging-a-python-crash">Debugging a Python Crash</h3>
<p>In our last section, we looked into an application that was crashing with a segmentation fault. That kind of problem is common when dealing with applications written in languages like <em><strong>C</strong></em> or <em><strong>C++</strong></em>. On the flip side, when using languages like <em><strong>Python</strong></em>, we usually need to deal with <em><strong>unexpected exceptions</strong></em> making our program crash. Let’s look at one example of that. We have a script that updates the descriptions of some products in our company’s database. It’s a pretty simple script that takes a CSV file as a parameter, which includes the data that needs to be imported using the product code and description. Our script simply reads through a file and then updates the database. Most of the time it works just fine. But when the file with the new descriptions is generated by one specific user, the program fails with an exception. The user has sent us a file that’s failing so that we can try to figure out what’s going on. Let’s first check out the contents of the file.</p>
<p>Okay, this seems harmless enough. Let’s try executing the program.</p>
<p>The program failed with an exception. Let’s have a look at this <em><strong>traceback</strong></em> to understand it a bit better. At the bottom, we see the name of the exception. In this case, KeyError and the message in this case, product code, which is the name of the key that’s failing. Above that, we see a list of function calls with two lines per function. The first line tells us the Python file that contains the function, the line number, and the name of the function. The second line shows us the contents of that line. This information is similar to the <em><strong>backtrace</strong></em> that we saw in our last section. But the order of the functions is reversed. The function at the bottom, update data, is the one where the exception occurred. Above it, we see that update data was called by main, and on top of that we see that main was called by the line at the module level. So what’s going on here? The update data function is trying to access the product code fields in a variable named row. But for some reason this is failing with a KeyError. Frequently, knowing the exception message and the line where the exception happened, is already enough to understand what’s going on. But in some cases like this one, that’s not enough. It’s time to try our hand at using a Python debugger. We’ll start the debugger by running <strong>pdb3</strong> and then passing the script that we want to run and <em><strong>any parameters that our script needs</strong></em>. In our case, we’ll call <em><strong>pdb3 update <a href="http://products.py">products.py</a> new products.csv</strong></em>. When we start the debugger it gets positioned at the first line of our script and waits for us to tell it what to do. We could run each of the instructions in the file one by one using the <strong>next</strong> command. But there’s a lot going on here. So we need to go through a lot of lines until we reach the failure. Alternatively, we can tell the debugger to <strong>continue</strong> the execution until it either finishes or crashes. Let’s do that now.</p>
<p>So the program failed in the same way we’d seen before. But now we can use the debugger to get a better idea of why we’re getting this pesky key error. Let’s print the contents of row.</p>
<p>That’s really weird. What are those characters appearing before product code? If we search online for the sequence of characters, will find that they represent the <em><strong>Byte Order Mark or BOM</strong></em> which is used in UTF-16 to tell the difference between a file stored using Little-endian and Big-endian. Our file is in UTF-8 so it doesn’t need the BOM. But some programs still include it and this is tripping up our script. So what can we do? Fortunately, others have already faced the same issue and figured out a solution. There is a special value called UTF-8-sig that we can set as the encoding parameter of the open function. Setting this encoding means that Python will get rid of the BOM when files include it and behave as usual when they don’t. Let’s change the code of our script to use that encoding instead of the default. We’ll look for the place where it’s opening the file, then add the encoding parameter with UTF-8-sig as the value.</p>
<p>All right. We’ve made the change. Will they work now? Let’s check it out.</p>
<p>Yeah. We’ve fixed the problem. Our script can now work with users generating files with and without the Byte Order Mark. In the last two sections, we looked briefly at GDB and PDB. We’ve barely discussed the surface of the many operations that we can do with debuggers. There are ton more advanced debugging features. Like setting breakpoints the letter code run until certain line of code is executed or watchpoints that let the code run until a variable or expression changes. We can also step through the code instruction by instruction to check when a problem happens and much more. We won’t look into any of these advanced techniques here. But as usual, we’ll put more information about this in the next reading in case you want to learn more.</p>
<h3 id="resources-for-debugging-crashes">Resources for Debugging Crashes</h3>
<p>Check out the following links for more information:</p>
<ul>
<li>
<p><a href="https://realpython.com/python-concurrency/">https://realpython.com/python-concurrency/</a></p>
</li>
<li>
<p><a href="https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32">https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32</a></p>
</li>
<li>
<p><a href="https://stackoverflow.com/questions/33047452/definitive-list-of-common-reasons-for-segmentation-faults">https://stackoverflow.com/questions/33047452/definitive-list-of-common-reasons-for-segmentation-faults</a></p>
</li>
<li>
<p><a href="https://sites.google.com/a/case.edu/hpcc/home/important-notes-for-new-users/debugging-segmentation-faults">https://sites.google.com/a/case.edu/hpcc/home/important-notes-for-new-users/debugging-segmentation-faults</a></p>
</li>
</ul>
<p>Readable Python code on GitHub:</p>
<ul>
<li>
<p><a href="https://github.com/fogleman/Minecraft">https://github.com/fogleman/Minecraft</a></p>
</li>
<li>
<p><a href="https://github.com/cherrypy/cherrypy">https://github.com/cherrypy/cherrypy</a></p>
</li>
<li>
<p><a href="https://github.com/pallets/flask">https://github.com/pallets/flask</a></p>
</li>
<li>
<p><a href="https://github.com/tornadoweb/tornado">https://github.com/tornadoweb/tornado</a></p>
</li>
<li>
<p><a href="https://github.com/gleitz/howdoi">https://github.com/gleitz/howdoi</a></p>
</li>
<li>
<p><a href="https://github.com/bottlepy/bottle/blob/master/bottle.py">https://github.com/bottlepy/bottle/blob/master/bottle.py</a></p>
</li>
<li>
<p><a href="https://github.com/sqlalchemy/sqlalchemy">https://github.com/sqlalchemy/sqlalchemy</a></p>
</li>
</ul>
<h2 id="handling-bigger-incidents">Handling Bigger Incidents</h2>
<h3 id="crashes-in-complex-systems">Crashes in Complex Systems</h3>
<p>Up to now we’ve talked about how to diagnose and fix errors that are confined to one computer. That’s a common case for computers that are used by a single user. But once we start going into complex systems that involve many different services, <em><strong>we’ll need to take a look at the bigger picture and have different computers interact with each other</strong></em>.</p>
<p>Say you’re in charge of the e-commerce site for your company. The page as seen by the users recently started responding with internal server error to about 20% of all requests. How do you figure out what’s going on? You want to apply the same principles that we saw for troubleshooting a problem on one computer, but this time at a larger scale. So you’ll want to <em>check the log messages in the servers providing the service</em>, and see if you find any additional information pointing to what’s causing the issue. You’ll want to find any log specific to the service that’s failing, and also look at the general system logs to see if there’s a problem affecting the server in general.</p>
<p>For this example, let’s say you find a bunch of entries in the logs that say, invalid response from server. That’s not a great error message. You don’t know what the request was or what the response was, but it’s at least a clue that whatever’s happening is related to some other service in the overall system. We said that this started failing recently, so it might make sense to figure out what changed between what it was working correctly and when it started to fail. Was there a new version of the system deployed? Were there any relevant changes regarding the requests? Let’s say this is happening on a Tuesday morning, and the latest release of this service was the previous week. Things were working fine until today, and the requests seemed normal, nothing out of the ordinary. So the service itself is probably okay, but <em>what about the other services involved in the system?</em> Was there a new version of one of the underlying systems, like the database, the authentication service, or some other back-end server like the inventory, billing, or procurement systems?</p>
<p>Looking at recent changes, you see that there were a bunch of changes made earlier in the day to the load balancer used between the front-end and the back-end services. Since the only clue you have is that the response from the service was invalid, you’re not sure that these changes are at fault, but they sure seem suspicious. <em><strong>Whenever possible, the best strategy is to roll back the changes that you suspect are causing the issue, even if you aren’t 100% sure if this is the actual cause</strong></em>. If your infrastructure allows easy rollbacks, try that before doing any further investigation. Why? Because that way, <em>you’ll restore the service back to health if it was the cause, or you’ll eliminate this change as a possible cause if doing the rollback doesn’t help</em>.</p>
<p>Whether you do the rollback or not, when coming across unhelpful error messages, it’s a good idea to improve them. Instead of the error just saying that the response is invalid, change it to include what the request and the response were, and why the response was invalid. That way, the next time you’re trying to debug a similar issue you already have more information to work with.</p>
<p>For this example, if the error had included this information you’d have seen that the invalid response was a 404 error. This was caused by having a server added to the pool as part of the inventory system, but the server actually belonged to the procurement system. Now, say a couple of weeks later you see that again, there are a bunch of internal server errors in the same service. It might be tempting to assume that it’s the load balancer’s fault once again, but by now you know that you should always look at the logs first and see what you find. There’s no reason why the error should be the same this time. When looking at the logs you may notice, for example, that only one of the front-end servers is actually affected by the problem. All the other machines are serving their content successfully. In a case like this, you’d start by first removing the machine from the pool of servers that can provide this service. That way, you avoid users getting any more errors. Well, you can investigate what’s going on with the broken machine. As you’ve probably realized by now, when dealing with complex systems like these having good logs is essential to understanding what’s going on. On top of that, you’ll want to have good monitoring of what the service is doing and use version control for all changes so that you can quickly check what’s changed and roll back when needed. It’s also important that you can very quickly deploy new machines when necessary. This could be achieved by either keeping standby servers, in case you need to use them, or by having a tested pipeline that allows you to deploy new servers on demand.</p>
<p>A lot of companies today have automated processes for deploying services to virtual machines running in the cloud. This can take a bit of time to set up, but once you’ve done that you can very easily increase or reduce the amount of servers you’re using. This can help a lot when investigating and solving problems. But one thing to take into account when the servers are running as virtual machines, especially if they’re running in the cloud, is that there might be external limits apply to these services. Resources, like the available CPU time, RAM, or network bandwidth, might be artificially capped. And not only that, the use of certain external services can also be limited, like how many database connections you can have at the same time or how much data you can store. If these limits are causing problems with your application, you might need to rethink how you use your resources.</p>
<p>We’ve covered a bunch of techniques that you can use when facing a problem in a complex system: <strong>looking at the available logs, figuring out what changed since the system was last working, rolling back to a previous state, removing faulty servers from the pool, or deploying new servers on demand</strong>.</p>
<h3 id="communication-and-documentation-during-incidents">Communication and Documentation During Incidents</h3>
<p>Until now, we’ve discussed how we can troubleshoot computers or systems with a specific issue. We’ve covered how we can get enough information so we can identify the root cause, and then apply the necessary remediation.</p>
<p>There’s another aspect to all of this, that is related to how we handle the communication with those affected by the issue and how we distribute tasks when addressing large issues as a team. Armed with what you’ve learned so far and your past experience, you might do a great job troubleshooting a problem. But if you drop the ball when it comes to communicating what you’re doing, you could end up with a bunch of frustrated users calling you to find out what’s going on. If you don’t <em><strong>write down what you’ve tried or how you fix the problem</strong></em>, you risk for getting some important details and wasting a lot of valuable time when you need to revisit an issue.</p>
<p>When working on a problem, it’s always a good idea to <em><strong>document</strong></em> what you’re doing in a bug or ticket. If there’s no such system at your company, then use a doc, a text file, or Wiki, or whatever you have access to. Documenting what you do lets you keep track of what you’ve tried and what the results were. This might seem unnecessary. But after a whole day of troubleshooting a problem, it’s pretty common for us to forget what we’ve tried or what was the outcome of a specific action. On top of that, having all this info available in some electronic forum lets you easily share all the data you’ve collected with other team members. If for example, you brought something back which turned out to be unrelated, having the whole process document it, helps you remember to roll forward again.</p>
<p>While you’re working on a problem, it’s important to <strong>communicate clearly</strong> with those affected by the issue. They want to know w_<strong>hat you figured out about the problem, what the available workarounds are, and when they can expect the next update</strong>_. If you don’t know what the problem is, it’s hard to give an estimation of when you’ll have it fixed. But you can still provide timely updates about the work you’re doing. This kind of regular communication is helpful no matter the size of the incident. But the more people affected, the more you’ll want to provide regular updates with clear instructions of what users can do and what they can expect as a solution.That way, they can better plan and organize their time. If access to the Internet is down, you want to let people know if they can expect to fix in one or two hours or if it’s going to take the whole day. This info can make a difference between people choosing to discuss issues in person for a couple of hours or deciding to work from home.</p>
<p>If the issue is big enough that you’re involving more people in finding a solution, you should agree on who’s going to work on which tasks.</p>
<p>For example, you could have someone working on finding out a temporary workaround, while someone else is in charge of understanding the root cause of the problem and finding the long-term remediation. Or if there are lots of possible causes for the issue, you could divide the causes among the team members and have them work on those in parallel. On top of people looking for the root cause and a solution, you want to have a person in charge of communicating with the people affected. This lets the team avoid forgetting to update the tracking issue or even worse providing contradictory information. <strong>This communications lead needs to know what’s going on and provide timely updates on the current state and how long until the problem’s resolved</strong>. They can act as a shield for questions from users letting the rest of the team focus on the actual problem. Similarly, there should be one person in charge of delegating the different tasks to the team members. This person sometimes called <strong>the Incident Commander or Incident Controller needs to look at the big picture and decide what’s the best use of the available resources</strong>. They can make sure that there’s no duplication of work among team members and that only one person is modifying the production system at a time. Having multiple people make overlapping changes to the system could lead to confusing results, making the outage even longer.</p>
<p>Of course, this division of roles makes the most sense when there’s a large incident and there’s a big team working on figuring out the solution. If it’s only two or three people working on the problem, it’s still important to agree who will work on what but you probably don’t need to use any special role names to do that. Once the issue has been resolved, it’s super-important to <strong>sum up</strong> the information that was helpful. The most important information that you’ll want to include are:</p>
<ul>
<li>
<p>The root cause,</p>
</li>
<li>
<p>How you diagnosed the problem and found that root cause,</p>
</li>
<li>
<p>What you did to fix the issue and</p>
</li>
<li>
<p>What needs to be done to prevent the problem from happening again.</p>
</li>
</ul>
<p>Depending on the size of the issue and the number of people affected, this summary could just be the last update to the bug or ticket that you use to keep track of your work, or it could be a full <strong>postmortem</strong>.</p>
<h3 id="writing-effective-postmortems">Writing Effective Postmortems</h3>
<p>In our last section, we talked about <em>the importance of communication and documentation when troubleshooting incidence</em>. We called out that if the issue is big enough, we might want to document what happened in a <em><strong>postmortem</strong></em>. P<strong>ostmortems are documents that describe details of incidence to help us learn from our mistakes</strong>. When writing a postmortem, the goal isn’t to blame whoever caused the incident, but <em>to learn from what happened to prevent the same issue from happening again</em>. To do this, we usually document <em>what happened, why it happened, how it was diagnosed, how it was fixed</em>, and finally figure out <em>what we can do to avoid the same event happening in the future</em>.</p>
<p>Remember the main goal is to learn from our mistakes. Writing a postmortem isn’t about getting someone fired but about making sure that next time we do better. Writing postmortems after dealing with incidence is important because it helps us avoid dealing with them again or at least learn how to deal with the next incident better.</p>
<p>While Postmortems are super useful with large incidence, you don’t need to wait until something huge happens to write your first postmortem. You can practice riding them for any kind of event where there’s something to be learned no matter how small. That way, when you need to write a postmortem after a big incident, you know how to concentrate on the things that matter the most. What you can learn from the problem and how you can prevent it in the future.</p>
<p>So what should you write in a postmortem? The exact structure might vary depending on preference and the type of incident that you’re dealing with. In general, you’ll want to include the details of</p>
<ul>
<li>
<p>What caused the issue,</p>
</li>
<li>
<p>what the impact of the issue was,</p>
</li>
<li>
<p>how it got diagnosed,</p>
</li>
<li>
<p>the short-term remediation you applied,</p>
</li>
<li>
<p>and the long-term remediation you recommend.</p>
</li>
</ul>
<p>If the document is long and you’re going to share it with a lot of people, you want to include a <em><strong>summary that highlights the root cause, the impact, and what needs to be done to prevent the issue from happening again</strong></em>.</p>
<p>It’s useful to include what went well in postmortems too. When working on a problem, we might realize that it would have been much worse if we didn’t have certain tools or systems available.</p>
<p>For example, we might say that we were able to solve the problem quickly by doing a roll back to the previous version or that we caught the issue before users even noticed it because we had good monitoring and alerting. Noting the things that went well helps us show that our systems are effective and justifies keeping those systems running. Writing a postmortem can sometimes help you understand the services that you’re working with much better.</p>
<p>You can even practice writing postmortems outside of the IT context. Like, if you bake cookies and they don’t turn out as great as you wanted them to, document what you did, what went wrong, what went right, and how you can improve the results in the future. You can do this with any hobby that you have. Maybe photography, 3D printing or brewing your own beer. You don’t always need to write the whole thing down. Sometimes a mental note is enough.</p>
<p>Once again, <strong>remember that the most important part of the postmortem is what we can learn for the future</strong>. So if instead of writing a whole document you’re creating a one paragraph summary of the incident, remember to focus that paragraph on what you can do better, not on whatever mistake caused the incident.</p>
<h3 id="module-3-crashing-apps-wrap-up">Module 3 Crashing Apps Wrap-up</h3>
<p>Congratulations on making it all the way here. You’ve learned a lot about all kinds of crashes, errors, and incidents over the past sections.</p>
<p>We’ve covered examples of what you can do to work around a problem and you don’t have access to the code. We’ve looked at ways to solve the issues when you do have access to the code, and checked out how to deal with larger incidents that involve complex systems and large teams of people.</p>
<p>Learning about the different ways computers and software can fail can seem scary at first. But remember that all of these tools help us get better at coming up with solutions. One of the things In IT, there are so many different ways to accomplish any task. So if you get stuck, you can always try a new angle. There’s nothing like the feeling of solving a tricky problem.</p>
<p>Throughout our examples, we’ve been applying the same techniques that we learned the beginning of the course. <strong>We gathered information until we understood the problem. We found the root cause and then worked on the short-term and the long-term remediatio</strong>n. By now, you have a better idea of what you can do when an application crashes unexpectedly, how to use <strong>elimination</strong> to figure out what’s wrong, and what kind of solutions you can apply depending on the failure.</p>
<h2 id="managing-computer-resources">Managing Computer Resources</h2>
<h3 id="intro-to-module-4-managing-resources">Intro to Module 4: Managing Resources</h3>
<p>Welcome back. We’re almost to the end of the course. Congratulations on making it all the way here. I hope you’re starting to see just how practical these lessons are in a real-world IT environment, and that you’re feeling empowered by your new troubleshooting skills.</p>
<p>In past modules, you’ve learned how to troubleshoot and debug a bunch of situations. We saw how reducing scope and isolating problems can lead us to the root cause of programs that are running slowly or crushing unexpectedly. We also learned how to understand different error messages and use the tools available in the OS to diagnose what’s going on.</p>
<p>Sometimes the problem we face isn’t that something doesn’t work, but that it doesn’t work as well as it should. Usually, this comes down to not making the best use of the available resources in the system. If our program uses too much memory for example, we might be able to work around it by adding more RAM to the computer. But wouldn’t it be better if it didn’t use that much memory in the first place? <em><strong>All resources in our computer are limited</strong></em>. So we need to make sure that the applications we run make the best use of them. We need to check that the software we run doesn’t waste memory for things that aren’t needed, or that the space on our disks is actually used by data that matters, or that the information transmitted over the network is actually the info we care about. There’s always something to declutter.</p>
<p>In the next few sections, we’ll explore how we can figure out what’s going on with programs that exhaust resources on our computer. Whether that’s memory, disk, or even network link. Then will talk about managing <em><strong>our most valuable resource of all, time</strong></em>. We’ll learn how we can look at the never-ending list of tasks that needs to be done, and make sure that we’re spending our time wisely by prioritizing our work and avoiding unnecessary interruptions.</p>
<p>After that, we’ll discuss how we can apply all our new knowledge to try to avoid future problems. Being proactive could help us mitigate issues when things don’t go according to plan. Hint, they rarely do, and even if would problems altogether by catching them in the test infrastructure. Finally, you’ll have another opportunity to try your hand at solving a real world challenge, to put your skills into practice.</p>
<h3 id="memory-leaks-and-how-to-prevent-them">Memory Leaks and How to Prevent Them</h3>
<p>Most applications need to store data in memory to run successfully. We called out earlier, how processes interact with the OS to request chunks of memory, and then release them when they’re no longer needed. When writing programs in languages like C, or C++, the programmer is in charge of deciding how much memory to request, and when to give it back. Since we’re human, we might sometimes forget to free memory that isn’t in use anymore, this is what we call a Memory leak. <strong>A memory leak happens when a chunk of memory that’s no longer needed is not released</strong>.</p>
<p>If the memory leak is small, we might not even notice it, and it probably won’t cause any problems. But, when the memory that’s leaked becomes larger and larger over time, it can cause the whole system to start misbehaving. When a program uses a lot of RAM, other programs will need to be swapped out and everything will run slowly. If the program uses all of the available memory, then no processes will be able to request more memory, and things will start failing in weird ways. When this happens, the OS might terminate processes to free up some of the memory, causing unrelated programs to crash.</p>
<p>You might be thinking why should I care if I don’t plan to code in C or C++? It’s true, languages like Python, Java, or Go manage memory for us, but things can still go wrong if we don’t use the memory correctly.</p>
<p>To understand how this works, let’s look into what these languages do. First, they request the necessary memory when we create variables, and then they run a tool called <strong>Garbage collector, that’s in charge of freeing the memory that’s no longer in use</strong>. To detect when that’s the case, the garbage collector looks at the variables in use and the memory assigned to them and then checks if there any portions of the memory that aren’t being referenced by any variables.</p>
<p>Say for example, you create a dictionary inside a function, use it to process a text file, calculate the frequency of the words in the file, and then return the word that was used the most frequently. When the function returns, the dictionary is not referenced anymore. So the garbage collector can detect this and give back the unused memory, but if the function returns the whole dictionary, then it’s still in use, and the memory won’t be given back until that stops being the case. When our code keeps variables pointing to the data in memory, like a variable in the code itself, or an element in a list or a dictionary, the garbage collector won’t release that memory.</p>
<p>In other words, even when the language takes care of requesting and releasing the memory for us, we could still see the same effects of a memory leak. If that memory keeps growing, the code could cause the computer to run out of memory, just like a memory leak would. The OS will normally released any memory assigned to a process once the process finishes. So memory leaks are less of an issue for programs that are short lived, but can become especially problematic for processes that keep running in the background.</p>
<p>Even worse than these are memory leaks caused by a device driver, or the OS itself. In these cases, only a full restart of the system releases the memory. Say you notice that your computer seems to run out of memory a lot, you look at the running programs over the course of some time, and realize that there’s a process that keeps using more and more memory as the hours pass. If you reset that process, it begins with a very small amount of memory, but quickly requires more and more. If that’s the case, it’s pretty likely that this program has a memory leak.</p>
<p>What can we do if we suspect a program has a memory leak? We can use a <strong>memory profiler</strong> to figure out how the memory is being used. As what debuggers will have to use the right profiler for the language of the application. For profiling C and C++ programs, we’ll use Valgrind which we mentioned in an earlier video. For profiling a Python, there are bunch of different tools that are disposal, depending on what exactly we want to profile. We can be as detailed as profiling the memory usage of a single function, or as big picture as monitoring the total memory consumption over time. Using profilers, we can see what structures are using the most memory at one in time or take snapshots at different points in time and compare them. The goal of these tools is to help us identify which information we’re keeping in memory that we don’t actually need. <em><strong>It’s important that we measure the use of memory first before we try to change anything, otherwise we might be optimizing the wrong piece of code</strong></em>. Sometimes we need to keep data in memory, and that’s fine, but you want to make sure that you’re only keeping the data that you actually need, and that you’ve let go of anything you won’t be using, that way the garbage collector can give that memory back to the OS. Of course, if you check that you’re using the memory correctly, but still find that your exhausting available RAM, it might be time for an upgrade.</p>
<p>There’s a lot more to say about memory profiling than we have time to cover it, but we’ve included links to more information about some of these profiling tools in the next reading.</p>
<h3 id="managing-disk-space">Managing Disk Space</h3>
<p>Another resource that might need our attention is the <strong>disk usage</strong> of our computer. Programs may need disk space for lots of different reasons: Installed binaries and libraries, data stored by the applications, cached information, logs, temporary files or even backups. If our computers running out of space, it’s possible that we’re trying to store too much data in too little space. Maybe we have too many applications installed, or we’re trying to store too many large files in the drive.</p>
<p>But it’s also possible that programs are misusing the space allotted to them, like by keeping temporary files or caching information that doesn’t get cleaned up quickly enough or at all. It’s common for the overall performance of the system to decrease as the available disk space gets smaller. Data starts getting fragmented across the disk, and operations become slower. When a hard drive is full, programs may suddenly crash, while trying to write something into disk and finding out that they can’t. A full hard drive might even lead to data loss, as some programs might truncate a file before writing an updated version of it, and then fail to write the new content, losing all the data that was stored in it before. If it gets to this point, we’ll probably see some error, like no space left on device when running our applications or in the logs.</p>
<p>So what do you do if a computer runs out of disk space? If it’s a user machine, it might be easily fixed by uninstalling applications that aren’t used, or cleaning up old data that isn’t needed anymore. But if it’s a server, you might need to look more closely at what’s going on. Is the issue that you need to add an extra drive to the server to have more available space, or is it that some application is misbehaving and filling the disk with useless data? To figure this out, you want to look at how the space is being used and what directories are taking up the most space, then drill down until you find out whether large chunks of space are taken by valid information or by files that should be perched. For example, on a database server, it’s expected that the bulk of the disc space is going to be used by the data stored in the database. A mail server, it’s going to be the mailboxes of the users of that service. But if you find that most of the data is stored in logs or in temporary files, something has gone wrong. One common pattern of misbehavior is a program that keeps logging error messages to the system log over and over. This can happen for lots of different reasons.</p>
<p>For example, the OS might keep trying to start a program that fails because of a configuration problem. This will generate a new log entry with every retry, and can take up a lot of space if there are several retries per second, or it could be that the server has a lot of activity and the logs are real. But there are just too many of them. In that case, you might want to look on the tweaking, the configuration of the tools that rotate the logs more frequently, to make sure that you’re keeping only what you need.</p>
<p>In other cases, the disk might get full due to a program generating large temporary files, and then failing to clean those up. For example, an application might clean up temporary files when shutting down cleanly, but leave them behind if it crashes. Or it could simply be a programming error of creating temporary files and never cleaning them up. In a case like this, you’ll ideally have some housekeeping to fix the program, and delete those files correctly. But if that’s not possible, you might need to write your own script that gets rid of them.</p>
<p>A situation that might be tricky to debug is when the files taking up the space or deleted files. I’m sure you’re wondering, <em><strong>how can deleted files take up space</strong></em>? Great question. Well, if a program opens a file, the OS lets that program read and write in the file regardless of whether the file is marked as deleted or not. So lots of programs delete the temporary files they create right after opening to avoid issues with failing to clean them up later. That way, the process can read from and write to the file while the file is open. Then when the process finishes, the file gets closed and actually deleted. Now, this system is widely used and works fine for most processes. But if for some reason, this temporarily deleted file starts becoming super large, it can end up taking up all the available disk space. If that happens, we’ll be left scratching our heads when trying to figure out where most of the data went, since we won’t see these deleted files. To check for the specific condition, we need to list the currently opened files, and comb for the ones that we know are deleted:</p>
<pre class=" language-bash"><code class="prism  language-bash"><span class="token function">lsof</span> <span class="token operator">|</span> <span class="token function">grep</span> deleted
</code></pre>
<p>We include pointers to the commands for how that works in the next reading. Of course, there are all kinds of other reasons why the disk may be getting too full. Just remember that whenever this happens, your process will remain the same. You’ll need to spend some time looking into what’s using the disk. Check to see if it’s expected or an anomaly, figure out how to solve it, and most important of all, how to prevent it from happening again.</p>
<h3 id="network-saturation">Network Saturation</h3>
<p>When you work in IT, you interact with services all over the Internet. At one moment, you might connect to a service running on your local network and the next use another service running in a data center located on a different continent. If your network connection is good, you might not be able to tell the difference where the website you’re browsing is hosted. But if you’re dealing with a network service that isn’t exactly up to speed, you might need to get more details about the connection you’re using. The two most important factors that determine the time it takes to get the data over the network are the <strong>latency</strong> and the <strong>bandwidth</strong> of the connection.</p>
<ul>
<li>
<p><strong>The latency is the delay between sending a byte of data from one point and receiving it on the other</strong>. This value is directly affected by the physical distance between the two points and how many intermediate devices there are between them.</p>
</li>
<li>
<p><strong>The bandwidth is how much data can be sent or received in a second</strong>. This is effectively the data capacity of the connection.</p>
</li>
</ul>
<p>Internet connections are usually sold by the amount of bandwidth the customer will see. But it’s important to know that the usable bandwidth to transmit data to and from a network service will be determined by the available bandwidth at each endpoint and every hop between them.</p>
<p>To understand how latency and bandwidth interact, think about what happens when you try to visit a website over the Internet. If the web server is hosted somewhere across the ocean, the latency might be a 100 milliseconds or so. That’s the time it takes for your request to reach the server. The server will then generate a response and send it back to you. The first bytes of the response will again take a 100 milliseconds to zap across the pond to your computer. Once the response is on its way, the time it takes for the rest of the data to arrive is determined by the bandwidth. If the available bandwidth between the two points is 10 megabits per second, you’ll be able to receive 1.25 megabytes every second.</p>
<ul>
<li>So for a website of about one megabyte of content, that large initial latency will be noticeable, since it’s an extra 20 percent on top of the total time to download it. But if the content is 10 megabytes or more, the initial latency will be less than 5 percent of the total time to download it. So it matters less.</li>
</ul>
<p>Let’s say you’re trying to figure out why a network connection isn’t going as fast as you want. <em><strong>Remember that if you’re transmitting a lot of small pieces of data, you care more about latency than bandwidth</strong></em>. <em>In this case, you want to make sure that the server is as close as possible to the users of the service</em>, aiming for a latency of less than 50 milliseconds if possible, and up to a 100 milliseconds in the worst-case.</p>
<ul>
<li>On the flip side, if you’re transmitting large chunks of data, you care more about the bandwidth than the latency. <em>In this case, you want to have as much bandwidth available as possible regardless of where the server is hosted</em>.</li>
</ul>
<p>What do we mean by available bandwidth? Computers can transmit data to and from many different points of the Internet at the same time, but all those separate connections share the same bandwidth. <em>Each connection will get a portion of the bandwidth, but the split isn’t necessarily even</em>. If one connection is transmitting a lot of data, there may be no bandwidth left for the other connections. When these traffic jams happen, the latency can increase a lot because packets might get held back until there’s enough bandwidth to send them. You’ve probably experienced this already on your own computer. If you’ve ever run several applications using the same network at once, the overall connection speed may have seem slower.</p>
<p>You can check out which processes are using the network connection by running a program like <strong>iftop</strong>. This shows how much data each active connection is sending over the network.</p>
<p>You might also have noticed that the more users sharing the same network, the slower the data comes in. This is true for home connections and office connections alike. No matter how much bandwidth you have, it’s a limited resource. So you’ll need to be careful with how you share it among its users. If some applications are using so much bandwidth that others can’t transmit anymore data, it’s possible to restrict how much each connection takes by using <em><strong>traffic shaping</strong></em>. <strong>This is a way of marking the data packets sent over the network with different priorities</strong>, to avoid having huge chunks of data use all the bandwidth. By prioritizing accordingly, processes that send and receive small packets can keep working fine, while processes that need the most bandwidth can use the rest.</p>
<p>There’s also a limit to how many network connections can be established on a single computer. This isn’t usually a problem, but there could be bugs in the software that causes it to open way too many connections, or keep old connections open even if they’re no longer in use. If this happens on a server, no new users will be able to connect to it until whatever is keeping those connections open closes them.</p>
<h3 id="dealing-with-memory-leaks">Dealing with Memory Leaks</h3>
<p>There’s a ton of reasons why an application may request a lot of memory. Sometimes, it’s what we need for the program to complete it’s task. Sometimes, it’s caused by a part of the software misbehaving. First, let’s trigger the misbehavior ourselves to see what this looks like. We’ll use a terminal called <strong>uxterm</strong> for that.</p>
<p>We’ve configured this terminal to have a really long scroll buffer. The scroll buffer is that nifty feature that lets us scroll up and see the things that we executed and their output. The contents of the buffer are kept in memory. So if we make it really long and we managed to fill it, will cause our computer to run out of memory. With normal use, it might take ages until it happens, but if we run a command that keeps generating a lot of output, we could manage to fill that buffer pretty quickly. Say we run a command like:</p>
<pre class=" language-bash"><code class="prism  language-bash">od -cx /dev/urandom
</code></pre>
<p>This command will take the random numbers generated by the urandom device and show them as both characters and hexadecimal numbers. Since the urandom device keeps giving more and more random numbers, it will just keep going. Our command is filling up the scroll buffer, making a computer require more and more memory. In a different terminal, let’s open <strong>top</strong> and check out what’s going on.</p>
<p>Pressing “Shift M” we tell ton that we want to order the programs by how much memory they are using. We see that the percentage of memory used by uxterm is going up super quickly. Let’s stop the process, it’s filling up the buffer by pressing “Control C”.</p>
<p>With that, we stopped the command that was filling the buffer, but the terminal still has that memory allocated, storing all the lines in the scroll buffer. Let’s look at the output of top in a bit more detail. There’s a bunch of different columns with data about each process.</p>
<ul>
<li>
<p>The column labeled <em><strong>RES</strong></em> is the dynamic memory that’s reserved for the specific process.</p>
</li>
<li>
<p>The one labeled <em><strong>SHR</strong></em> is for memory that’s shared across processes,</p>
</li>
<li>
<p>The one labeled <em><strong>VIRT</strong></em> lists all the virtual memory allocated for each process. This includes; process specific memory, shared memory, and other shared resources that are stored on disk but maps into the memory of the process.</p>
</li>
</ul>
<p>It’s usually fine for a process to have a high value in the VIRT column. The one that usually indicates a problem is the RES column. Let’s close the other terminal so that it releases all the memory that it reserved.</p>
<p>In this example, we saw what a program that keeps requesting more and more memory looks like. This was a super extreme example. Most memory leaks don’t happen at the speed. It can usually take a long while until we notice that a program is taking more memory than it should, and it might be hard to tell the difference between memory that’s actually needed and memory that’s being wasted. But <em>looking at the output of <strong>top</strong> and comparing it to what it used to be a while back is usually how any investigation into a memory leak starts</em>. Let’s look at a different example. We have a script that analyzes the frequency of words in web pages. This script works fine when it’s just a few web pages, but if we try to give it all the Wikipedia contents, it starts using up all the memory. Let’s run it first and see what happens.</p>
<p>This is running and it will take a long while to finish. It’s processing a huge amount of articles after all. While this is running, let’s look at the output of <strong>top</strong> in a different terminal and see what we find. We see that there’s a bunch of different content stats processes running. That’s because our script is using the multiprocessing techniques that we saw in an earlier video to parallelize the processing of the information and get the results as fast as possible. It seems like these scripts are taking a lot of memory. So let’s sort it out to see the details.</p>
<p>We see that the memory used by one of the processes in particular keeps growing and growing. The application is processing a bunch of data and generating a dictionary with it. So it’s expected that it will use some memory but not this much. This looks like the program is storing more than it should in memory. This program is pretty complex. So we could use the help of a <em><strong>memory profiler</strong></em> here to figure out what the problem is. Let’s stop it now and use a profiler to figure out where our computer’s memory is going. To do that, we’ll need to use a simplified version of our code as profiling the memory of a multi-process application is extra hard, and instead of processing all the articles, we’ll just handle a few so that we can check up the memory consumption quickly. Let’s open our simplified script and have a look.</p>
<p>We’ll be using a module called memory <strong>profiler</strong>. This is one of the many different memory profilers available for Python. We’ve added this app profile label before the main function definition to tell the profiler that we wanted to analyze the memory consumption of it. This type of label is called a <em><strong>decorator</strong></em> and it’s used in Python to add extra behavior to functions without having to modify the code. In this case, the extra behavior is measuring the use of memory. The rest of the code is basically the same as the original one, it just uses a single process and is limited to 50 articles instead of the thousands of articles that the other script was going through.</p>
<p>We’re running the script with the <em>memory profiler enabled</em>. This is just reading through 50 articles but it takes a bunch of time because all that memory profiling makes our script slower. Once the program finishes, the memory profiler gives us information about which lines are adding or removing data from the memory used by the program. The first column shows us the amount of memory required when each line gets executed. The second one shows the increase in memory for each specific line. We see here that after going through 50 articles, the program already took 130 megabytes, no wonder we ran out of memory when we were trying to process all the articles. We can see that the variables that require the most memory are article and text, with about four and of three megabytes respectively. Those are the articles we’re processing, and it’s fine for them to take space while we’re counting the words in the article. But once were done processing one article, we shouldn’t keep that memory around. Can you spot the problem?</p>
<p>Right at the end, the code is storing the article to keep a reference to it, but it’s storing the whole article. If we want to keep a reference to all the articles that include a word, we could store the titles or the index entries, definitely not the whole contents.</p>
<p>There’s a ton more to say about memory management and memory profiling that we don’t have time to cover here. In the next reading, we’ve gathered a bunch of interesting links to information about managing scarce resources.</p>
<h3 id="more-about-managing-resources">More About Managing Resources</h3>
<p>Check out the following links for more information:</p>
<ul>
<li>
<p><a href="https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python">https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python</a></p>
</li>
<li>
<p><a href="https://www.linuxjournal.com/content/troubleshooting-network-problems">https://www.linuxjournal.com/content/troubleshooting-network-problems</a></p>
</li>
</ul>
<h2 id="managing-our-time">Managing our Time</h2>
<h3 id="getting-to-the-important-tasks">Getting to the Important Tasks</h3>
<p>In earlier sections, we discussed how to make better use of the resources available on our computers and systems, like the CPU, the memory, the disk, the network, and so on. But, there’s another resource that’s even more valuable in our day to day, <em><strong>our time</strong></em>.</p>
<p>As humans, we want to make sure that <em><strong>we spend our time doing meaningful activities</strong></em>, like work that we enjoy, and earning the satisfaction of a job well done. When working, we need to optimize the time we spend to bring the most value to the company. Finding the right balance is hard, but that’s what we’re here for. From updated calendars, to social media detoxes, there’s lots of different ways to optimize our time.</p>
<p>One that’s super effective when working in IT is <strong>the Eisenhower Decision Matrix</strong>. When using this method, we split tasks into two different categories: <strong>urgent</strong> and <strong>important</strong>.</p>
<ul>
<li>
<p>There are tasks that are <em><strong>important and urgent</strong></em>. Draw alarm bells around them if you’d like because these need to be done right away. For example, if the company’s Internet connection is down, it’s both urgent and important to get it back up as soon as possible.</p>
</li>
<li>
<p>Some tasks are <em><strong>important, but not urgent</strong></em>, so they need to get done at some point even if it takes a while to complete them. For example, as a follow-up to the network being down, it would be important to make sure that there’s a backup network connection so that if the existing one is ever down again, the company can stay connected using the backup.</p>
</li>
<li>
<p>Other tasks might <em><strong>seem urgent, but aren’t really important</strong></em>. A lot of the <em>interruptions that we <strong>need</strong> to deal with</em> are in this category. Answering email, phone calls, texts, or instant messages feel like something that we need to do right away. But most of the time are not really the best use of our time.</p>
</li>
<li>
<p>Finally, there’s a whole category of tasks that are <em><strong>neither important nor urgent</strong></em>. These are <em>distractions</em> and time wasters, they shouldn’t be done at all. These include: meetings where nothing useful is being discussed, email threads that lead to nowhere, office gossip, no thanks, and any other tasks that eat up our time without giving anything valuable in return.</p>
</li>
</ul>
<p>In general, to make the most of our time, we need to make sure that we’re spending the majority of it on tasks that are important. Of course we’ll want to get to the urgent tasks as soon as possible, but <em>we need to block some time for long-term planning and execution</em>. Spending time on long-term tasks might not bear fruit right away, but it can be critical when dealing with a large incident. For example, setting up your infrastructure so that you can easily roll back changes or deploy new servers when needed takes a large chunk of time.</p>
<p>But investing in the future can save you even more time and user frustration when responding to a problem. <em><strong>Researching new technologies</strong></em> is another task in this category. <em><strong>IT is always evolving and it’s important to have time set aside to stay up to date</strong></em>. Figure out if it’s time to migrate the web server to different software, update the mail server to a new OS version, or deploy voice-over IP throughout your company.</p>
<p>Another important task that might not necessarily be urgent is <em><strong>solving technical debt</strong></em>. When you work in IT, you waste time a lot. What does it mean? <strong>Technical debt is the pending work that accumulates when we choose a quick-and-easy solution instead of applying a sustainable long-term one</strong>. We call that a few times already though in solving the problem we might apply a short-term remediation to fix it right away, and then plan for a long-term solution to prevent it from happening in the future. Until we have fixed the sticks, <em>the workaround we created is technical debt</em> because we need to spend time keeping it in place even if it’s not the best solution. Whenever we go for short-term solution and leave the long-term solution for later, we’re creating technical debt.</p>
<p>This might be the right decision in the moment to get us out of a crisis and let our users get back to work, but we need to schedule time to apply the long-term solution that will make our future lives easier.</p>
<p><em><strong>Technical debt can also be generated by external parties</strong></em>. For example, when a new version of the software we’re using is released, will need to schedule time to upgrade it. Until we do, that pending upgrade is technical debt.</p>
<p>So we’ve made it clear that we need to focus our work on tasks that are important, but what can we do about interruptions which are urgent but not important? <em><strong>If you work in IT support, you have to be interrupted, it’s part of the role</strong></em>. So you’ll need to plan to deal with those interruptions effectively. If you work on a team, you can rotate the person dealing with those interruptions. Maybe someone takes care of them in the morning and a different person in the afternoon, or you take turns each day. If you work independently, you can try to establish a set of hours when users can expect to reach you for a normal requests, and the rest of the time only be available for emergencies. The key here is to have a window of time reserved when you’re not going to be interrupted.</p>
<p>That’s the time when you can get the most important tasks done when you can fully concentrate on dealing with complex issues and finding solutions for tricky problems. Depending on your role and how the company works, you might need to get this work done in a different location to avoid people walking up to your desk, or actively silencing any of your notifications to avoid getting interrupted and distracted by unimportant conversations. So assuming that you’ve managed to set some time aside to work on these important but not urgent tasks, how do you make sure that you work on the right things with the right priorities?</p>
<h3 id="prioritizing-tasks">Prioritizing Tasks</h3>
<p>In our last section, we talked about how we need to make sure that we have the time available to work on tasks that are important, but not necessarily urgent. But sometimes it feels like everything is important, and everything is urgent.</p>
<p>Say you need to deploy a new computer for the person that’s starting tomorrow. Upgrade the VPN service to the latest version because the old one has a security vulnerability. Fix a permissions problem that’s preventing a group of users from accessing the inventory data. Check out a problem with the mail system that’s causing some emails to get randomly rejected. And so many other things that by now you’ve lost track of them. What can you do to figure out how to spend the limited time that you have?</p>
<p>There’s a lot to say about this, and everyone works a little differently. So you’ll need to find the system that works best for you. But let’s cover the basic structure that can help us get organized and prioritize our tasks.</p>
<ol>
<li>
<p>The <strong>first</strong> step is to <em><strong>make a list of all of the tasks that need to get done</strong></em>. You can make this list on a piece of paper, a text file in your computer, a bug tracking system, or a ticket management system. Whatever works for you. The point is to have all the tasks listed in one place to avoid depending on your not always perfect memory later.</p>
</li>
<li>
<p>Once you have the list, you can <em><strong>check the real urgency of the tasks</strong></em>. Ask yourself, <em><strong>if any items don’t get done today will something bad happen?</strong></em> If yes, then those should be worked on first.</p>
</li>
<li>
<p>Once you’re done with the most critically urgent tasks, you can look at the rest of the list and assess the importance of each issue. Even when it looks like everything is important, you should be able to tell that some things are more important than others. For example, a task that will benefit more people is more important than a task that will benefit less people. If there are a bunch of different tasks that depend on you completing one, that roadblock is more important to clear than the rest. If it still seems like everything is on fire, you can try dividing the tasks into groups of most important, important, and not so important. And then sort the tasks inside each group, but <em>don’t spend too much time doing this sorting</em>. In the end, the exact order isn’t what matters. <em><strong>What matters is that you spend most of your time working on the most important tasks</strong></em>. And if you work with a team of people, it’s a good idea to share both the list of tasks and the standard of prioritization among team members. This helps you avoid having to do the work multiple times and coming out with different priorities.</p>
</li>
<li>
<p>Once you have a list of the most important tasks to work on, you’ll want to have a rough idea of how much effort they’ll take. We’ll talk more about estimating times in the next section. This isn’t about exact timing, it’s about assigning rough sizes. <em>One common technique is to use small, medium, and large</em>. And when the range of sizes is big enough, include extra small or extra large if needed. Once you identify the most important tasks and how big they are, you can start working on them. <em><strong>If possible, try to start with the larger, most important tasks to get those out of the way first</strong></em>. But as we called out, when our work involves IT support, we know that we’ll have to deal with interruptions. And working on complex tasks while getting interrupted can be very frustrating. I hear that. <em><strong>One strategy that can help us with that is saving the most complex tasks for the moments when we’re less likely to get interrupted</strong></em>. If you know that you get busiest in the morning, and you tend to have more quiet time during the afternoon, it makes sense to work on easy and quick tasks early in the day. Save the most complex tasks for later, when you’ll have more time to concentrate on them. But when your focused time starts, you should make sure that you work on those large complex tasks and not on the easy ones. Otherwise, the complex tasks will never get done. <em><strong>The key here is to always work on important tasks. If a task is not important, it shouldn’t be done at all</strong></em>. Really, we live by this rule here at Google. Then select which task you’re going to deal with depending on urgency and how much time you can devote to it, starting with the biggest tasks that you can fit in the time you have available.</p>
</li>
</ol>
<p>But keep in mind, this shouldn’t stop you from taking a break or working on experimental projects. <em><strong>Taking breaks is important because it allows our creative minds to stay fresh</strong></em>, and working on a fun side project can help us research emerging technologies and come up with new ideas. Did you know that this very certificate program got its start as a side project at Google?</p>
<p>Okay, but what if the unthinkable happens? What can we do if after all of this prioritizing, sizing, and ordering there’s just too much work to be done and too few hours in the day? The first thing to know is this is normal, most people working in IT have too much to do and can’t get all the things they want done. Unfortunately, us humans can’t multiply ourselves on command yet and working extra hours is not sustainable long-term. <em><strong>Which means there are basically two options, either you get extra help from other team members or you decide that some tasks weren’t really that important, and they won’t get done. For both of these options, you’ll need to involve other people, like your manager, and make sure that expectations get clearly communicated</strong></em>. Some tasks, like fixing the permissions in a directory, changing a faulty keyboard, or installing a new application on a single computer, can be self-contained and completed in a small amount of time. Other tasks, like upgrading the database software to a new version, automating the creation of user accounts, or writing a wrapper to adapt to incompatible programs, are larger projects that can take several days or maybe even weeks to complete. When that’s the case, it’s important to have a rough estimate of how long the tasks will take to be completed and to clearly communicate expectations to those affected.</p>
<h3 id="estimating-the-time-tasks-will-take">Estimating the Time Tasks Will Take</h3>
<p>As we’ve called that before, when deciding whether a manual task needs to be automated, we should consider two things:</p>
<ul>
<li>
<p>how many times we’ll do the task over a period of time</p>
</li>
<li>
<p>and how long it takes to do it manually.</p>
</li>
</ul>
<p>From there, we can figure out if that number is larger than the time it will take us to write the automation.</p>
<p>This sounds great in theory, but the problem is that we don’t know how long it’ll take us to write the automation until we’ve actually written it. All we can do is estimate it, and most of us are really bad at estimating how long tasks will take. We tend to be too optimistic about the amount of time that a certain piece of code will take us to write or a certain infrastructure will take us to set up. Usually, our first instinct is to consider how much we can get done with an ideal amount of focus on the work and a full grasp of the problem we’re trying to solve. We forget to take into account the many obstacles that we might face like finding a bug that we don’t know how to fix, being interrupted by a problem that needs more urgent attention, or discovering that a new tool doesn’t work well with the rest of the tools we have in place. So if you’re trying to estimate how long it will take you to complete a project, big or small, you need to be realistic. Avoid being overly optimistic with your time estimates.</p>
<p>The best way to do this is to compare the task that you’re trying to do with similar tasks that you’ve done before. This way, you’re not estimating based on how long you would like the project to take but on how long other similar projects actually took in the past. If the task at hand is large, it might be hard to find something similar enough to use for comparison. So to make a better estimate of a bigger than average project, you’ll want to chop it up. Split the task into smaller steps. Compare each step to a similar task that you’ve done in the past and assign an estimated amount of time to each step based on that. If one smaller step is still too large, then split it into even smaller pieces until you can compare each piece of the puzzle was something that you’ve done before. Once you’ve got all those estimated times, just add them up and you’ll have a rough estimate of how long the whole task will take. But even that’s going to be optimistic since putting all the pieces together will take additional time.</p>
<p>So once you have a rough estimate of the total time of all the steps, you want to factor in some extra time for integration. This should also come from prior experience. Think about how long it took you to integrate the pieces of a project before, and you’ll have a rough idea of how much to add to the previous value.</p>
<p>Knowing how optimistic we humans are, even after basing those estimations on previous experience, the number you come up with is going to be very close to the best possible scenario. Even if you’re prepared for something to go wrong, it’s impossible to anticipate new unknown bumps in the road, so take this estimation and multiply it by a factor. Once again, this factor works best when it’s based on previous experience. So if the last time you did this exercise, it took you three times longer to complete the task than you’d planned, go ahead and multiply your estimation by three. This might seem like we’re inflating the numbers, but remember, you want to have a grounded estimate of how long it will take you to complete the task. That means taking into account the obstacles that you’ll certainly run into but haven’t come across yet. No matter how detailed we are, the final estimation won’t ever exactly match the time it takes, but it will give us a rough idea of whether we can complete the task in a few hours, days, weeks, or months.</p>
<p>Once you’ve made your estimation, you want to note it down somewhere so that you can check later to see how close you were to the original number. You can adjust future estimations based on that. You will also want to communicate with those affected and let them know when they can expect the task to be done.</p>
<h3 id="communicating-expectations">Communicating Expectations</h3>
<p>When you’re dealing with an issue that’s affecting one or more users, you might feel rushed to meet expectations, support the people you’re helping. Everyone’s got their own ideas of how long it will take you to solve the problem and when they can expect a solution. They might be wondering, what’s the holdup?</p>
<p>If the issue is that the users spilled coffee on their keyboard and needs a new one, the expectation is that replacing the keyboard will be a very small task that will take almost no time. If the issue is that there’s a new employee starting soon and they need a new computer setup for them, the expectation is that it will take much longer than replacing a keyboard, even if we have an automated process for setting up new computers, which means there’s very little manual work.</p>
<p><em>To have successful interactions with our users, it’s important to understand these implicit expectations and let users know if fixing the problem will take longer than they expect</em>. Users will be happy if the issue is resolved within their expectations, but will become frustrated if it takes much longer than they thought. But as long as we communicate with them early about the circumstances, they will be able to understand this and manage their time accordingly.</p>
<p>Say you have to replace a keyboard but you have no spares available. This means, you’ll need to buy a new one or maybe even a whole batch to have a spare available next time. It’s important to communicate upfront to the user that in this case, the replacements will take more time and then figure out how critical getting the replacement is. If the user is an accountant, working on the salary deposit order, which needs to be sent to the bank in one hour or nobody gets their payment on time, you might opt for giving them your keyboard while you go and buy another one at the closest hardware store. On the flip side, if the user can work on their laptop until the next day when a new batch of keyboards is scheduled to arrive, you don’t need to go out of your way to get early replacement.</p>
<p>It’s also important to let users know if there are any conflicting priorities that might delay the response to whatever they need. Say a user calls you to request access to a shared resource, but you’re in the middle of dealing with an issue that’s causing the company’s database to be offline. Even if the user request is quick and easy to solve, fixing the database is critical and affects the whole company, so it should take priority. In this case, make sure you tell the user that you’re dealing with a crisis and that you’ll help with their request once the crisis is resolved, give them an idea of when they can expect their issue to be fixed so they can plan what to do next. So as a general rule, communication is key.</p>
<p><strong>Try to be clear and upfront about when you expect the issue will be resolved, and if for any reason the issue isn’t solved by then, explain why and what the new expectation should be</strong>.</p>
<p>Unfortunately, when the issue you’re trying to solve involves troubleshooting and debugging, it’s usually very hard to give an accurate estimate of how long it will take you to fix the problem. Notice a theme, estimating the time it takes to perform more complicated work is tough. A lot of your time will be spent investigating looking into what’s going on and figuring out what should be happening. <em><strong>In that case, make sure to let users know when they can expect an update on their issue and give them timely updates</strong></em>, if possible it’s a really good idea to have users filed the requests through a ticket tracking system.</p>
<p>Using a system like this has a ton of advantages. Having all the work you need to do in one place lets you organize your tasks by priority as we discussed before. Receiving reports of issues through a system instead of a phone or chat, lets you make better use of your time. You can now decide when you’ll look at the list of issues instead of getting interrupted in the middle of a task, and when you have an update for an issue that you’ve been working on, you can easily update the ticket with news without having to track down users to let them know what’s up with their request.</p>
<p>Finally, try out some practical shortcuts when dealing with users. It makes sense to take some time to think about the work you do and figure out ways to avoid interruptions and save time. For example, if a user tells you that their mouse doesn’t work, your first instinct might be to go check it yourself and then bring a new one if necessary, but that’s a lot of back and forth, instead you could ask the user to bring the faulty mouse to you so that you can test it at your computer and if it’s broken change it with a new one or even better, if you trust your users, you might even leave a set of mice, keyboards, and other accessories available for users to take when the one they’re using breaks down. Along the same lines, if your company’s budget allows, you could have a couple of spare computers ready to be used. That way, when a computer breaks down, you can get the user back to work as quickly as possible, then you can debug the faulty computer at your own pace. But not every issue can be solved by having spare devices.</p>
<p>Sometimes spending time on improving your infrastructure can help you get more done in less time. Automating processes like installing new computers, setting up new user accounts, deploying virtual machines, or rolling back changes to previous versions can help save you a lot of time when you’re responding to an incident.</p>
<h3 id="more-about-making-the-best-use-of-our-time">More About Making the Best Use of Our Time</h3>
<p>Check out the following link for more information:</p>
<ul>
<li><a href="https://blog.rescuetime.com/how-to-prioritize/">https://blog.rescuetime.com/how-to-prioritize/</a></li>
</ul>
<h2 id="making-our-future-lives-easier">Making Our Future Lives Easier</h2>
<h3 id="dealing-with-hard-problems">Dealing with Hard Problems</h3>
<p>You might be wondering, why is debugging so hard that we need an entire course on it? <a href="https://en.wikipedia.org/wiki/Brian_Kernighan">Brian Kernighan</a>, one of the first contributors to the Unix operating system and co-author of the famous C programming language book, among many other things, once said:</p>
<blockquote>
<p>"Everyone knows that debugging is twice as hard as writing a program in the first place. So if you’re as clever as you can be when you write it, how will you ever debug it?</p>
</blockquote>
<p><em>This is a warning against writing complicated programs. If the code is clear and simple, it will be much easier to debug than if it’s clever but obscure</em>. The same applies to IT systems. If the system is engineered very cleverly, it will be extremely hard to understand what’s going on with it when something fails.</p>
<p>It’s important to focus on building systems and applications that are simple and easy to understand. So that when something goes wrong, we can figure out how to fix them quickly. So how do we do this?</p>
<ul>
<li>
<p>One piece of advice I found really valuable is to <em><strong>develop code in small, digestible chunks</strong></em>. Every so often, I stop and test what I’ve written. The hardest thing to do is to try to debug something if I’m running it for the first time only after I’ve completed it. There are so many places things could have gone wrong.</p>
</li>
<li>
<p>Another lesson that’s super useful is to <em><strong>keep your goal clear</strong></em>. If you’re writing code, <em>try writing the tests for the program before the actual code to help you keep focus on your goal</em>. If you’re building a system or deploying an application, having documentation that states what the end goal should be, and the steps you took to get there can be really helpful. To both keep you on track, and figure out any problems that might turn up along the way.</p>
</li>
</ul>
<p>We called out at the beginning of this course that solving technical problems is a bit of an art, and that it can be fun when things finally click together.</p>
<p>On the flip side, the worst part of troubleshooting and debugging is when we get stuck. When we can’t think of any other reasons why the program is failing, or we can’t figure out what else we can do to fix it.</p>
<p>In this course, we’ve given you a bunch of tools and processes to follow that can hopefully help you avoid getting stuck on a lot of these situations, but we can’t cover absolutely everything. <em><strong>You might still find yourself facing an issue that you have no idea what to do about, and that’s okay</strong></em>.</p>
<p><em><strong>If you’re in a sticky situation, the main thing to do is to remain calm</strong></em>. We need our creative skills to solve problems, and the worst enemy of creativity is anxiety. So if you feel that you’re out of ideas, it’s better to take your mind off the problem for a while. Maybe grab a cup of coffee, or take a walk outside.</p>
<p>Sometimes a change of scenery is all we need for a new idea to pop up and help us figure out what we’re missing, true in coding and in life.</p>
<p>If the problem you’re trying to solve is complex and affects a lot of people, it can get really stressful to try to fully debug it with everyone waiting on you. That’s why it’s better to focus first on the short-term solution, and then look for the long-term remediation once those affected are able to get back to work.</p>
<p>And don’t be afraid to ask for help. Sometimes just the act of explaining the problem to someone else can help us realize what we’re missing. There’s a technique called <em><strong>rubber duck debugging</strong></em>, which is simply explaining the problem to a rubber duck. It sounds whimsical, and you may look like a quack, but it can really work. Because when we force ourselves to explain a problem, we already start thinking about the issue differently.</p>
<p>And remember that no one knows absolutely everything. Sometimes the best way to learn new skills and techniques is to ask others for help. We’re all in this thing together. There are times when I know that if I spend enough hours on a problem, I’ll probably figure out a solution, but is that the best use of my time? Usually, the better answer is to ask someone who has done it before, to save time and frustration. And then use the problem at hand as an opportunity to keep learning, so that the next time, I can do it on my own. When you ask a colleague for their help with debugging a problem, be careful not to tell them what you think the root cause of the issue might be. Instead, tell them about the symptoms, and see what questions they ask and what possibilities they probe. They might come up with completely different paths to explore.</p>
<p>Of course, our lives as IT specialists would be much easier if we could avoid problems altogether…</p>
<h3 id="proactive-practices">Proactive Practices</h3>
<p>Something that IT specialists and exterminators have in common is dealing with bugs… It can be bugs in our software or someone else’s software. But we’ll come across lots of bugs that trigger lots of different failures in our programs. There’s a bunch of strategies we can adopt to make our lives easier, by catching issues before they affect our users or making troubleshooting simpler by having better information. We’ve touched upon some of them here and there but now, it’s time to deep dive.</p>
<p>To avoid having to scramble to fix things when there’s an outage, <em>it’s really helpful to have infrastructure that lets us test changes in advance so that we can check that things are working as expected before they reach our users</em>. <strong>If we’re the ones writing the code, one thing we can do is to make sure that our code has good unit tests and integration tests</strong>. If our tests have good coverage of the code, we can rely on them to catch a wide array of bugs whenever there’s a change that may break things. <em>For these tests to be really meaningful, we need to run them often, and make sure we know as soon as they fail</em>. <em><strong>Setting up continuous integration can help with that</strong></em>.</p>
<p>Another step in this direction is to <em>have a test environment</em>, where we can deploy new code before shipping it to the rest of our users. This serves two purposes:</p>
<ol>
<li>
<p>First, we can do a thorough check of the software as it will be seen by the users. Depending on the software and how often we update it, we can do both automated and manual tests in this environment.</p>
</li>
<li>
<p>Second, we can use this test environment to troubleshoot problems whenever they happen. We can try possible solutions and new features without affecting the production environment.</p>
</li>
</ol>
<p>Taking this even further, another recommended practice when managing a fleet of computers is to deploy software in phases or canaries. What this means is that instead of upgrading all computers at the same time and possibly breaking all of them at the same time, you upgrade some computers first and check how they behave. If everything goes fine, you can upgrade a few more, and so on until you’re confident enough to upgrade the remaining part of the fleet. As the saying goes, like a canary in a coal mine.</p>
<p>To make the best use of this practice, we’ll need to be able to easily roll back to the previous version. Depending on the software, this might require more or less infrastructure. But trust me, it’s worth spending the time setting up that additional infrastructure. If you deploy to software version that was broken and suddenly a bunch of your computers aren’t working correctly, you’ll want to roll them back to a previous state as fast as possible.</p>
<p>Now, even with all these preventative measures, bugs will still filter through and problems will occur. We can make our troubleshooting easier by <em><strong>including good debug logging in the code</strong></em>. That way, whenever we have to figure out an issue, we can look at the logs and get a pretty good idea of what’s going on.</p>
<p>Another method that can help us is having <em><strong>centralized logs collection. This means there’s a special server that gathers all the logs from all the servers or even all the computers in the network</strong></em>. That way, when we have to look at those logs, we don’t need to connect to each machine individually, we can comb through all the logs together in a centralized server.</p>
<p>Similarly, having a good monitoring system can be super helpful. We can use it to catch issues early before they affect too many users. During a debugging session, we can look at the collected data to try to determine if there’s anything out of the ordinary going on.</p>
<p>We called out ticketing systems a few times already, because we can’t stress their importance enough. Making good use of them can help us save a lot of time when trying to get to the bottom of a problem. <em><strong>If we ask users to provide the needed information up front, we don’t have to waste time and go back and forth</strong></em>. Even here, we can look at opportunities for automation.</p>
<p>Say you almost always want some specific info from the users computers, you can automate getting it by creating a script that gathers all the data you want and have the users attach it to the ticket.</p>
<p>Finally, remember to <em><strong>spend time writing documentation</strong></em>. Just as importantly, store the documentation in a well-known location. Even if writing documentation isn’t especially fun, having good instructions on how to solve a specific problem, knowing how to diagnose what’s going on with the server, or tracking the known issues in a system can be real time savers. At Google, we have a bunch of docs called Playbooks where we detail what a person who’s on call can do to diagnose and mitigate a ton of different problems. By keeping this information updated, we make sure that no matter who the person on call is, everybody has access to the knowledge base accumulated by the whole team.</p>
<p>It doesn’t stop there. If we’re dealing with systems that change and grow, we can proactively plan for the additional capacity that we’ll need in the future.</p>
<h3 id="planning-future-resource-usage">Planning Future Resource Usage</h3>
<p>We discussed in earlier sections what we can do when our programs are misusing resources like memory, disk, network, or CPU. But sometimes, it’s not a question of misusing resources, but rather missing resources.</p>
<p>A database server is expected to use more disk storage as more data gets stored. Or a web server is expected to use more network bandwidth as the service grows in popularity. If you’re dealing with a service that’s expected to grow and will acquire more resources in the future, it makes sense to spend some time thinking about what that might look like.</p>
<p>Planning ahead will prepare you for when you need additional resources, instead of having to scramble for them at the last minute.</p>
<p>Lets say the database growth is expected to be one megabyte per day, and you have 500 megabytes of free-space. You can use that storage for almost two years. But if the growth is expected to be 10 megabytes per day and you only have those 500 megabytes available, then you need to start figuring out a plan that will allow your database to keep growing at that pace. Otherwise, you run out of space in a couple of months.</p>
<p>Once you’ve figured out the current usage and the expected growth, don’t forget to <em><strong>write this down so you can refer to it in the future and check if anything has changed</strong></em>. If you find that you’ll soon be running out of space, what you’ll do next will depend on what the system does, and the importance of the data. You might decide that you don’t really want to store all that data, and instead clean up anything that’s not really necessary, or you might decide that you really need to have a lot more storage available. In that case, you might opt for buying a network attached storage or NAS that can be attached to your server for additional disk space.</p>
<p>Migrating to a different type of storage takes time, and can be tricky to do right under pressure. So it’s important to do this kind of planning in advance and not wait until the disk is completely full. This means monitoring the usage growth of the computer to see if there are any trends that need attention.</p>
<p>If the service using the database suddenly becomes very popular, the daily growth can increase so much that we’ll need to find a better solution sooner than we thought. Our monitoring system should trigger an automatic alert when that’s the case.</p>
<p>An interesting strategy for making the best possible use of resources, is to mix and match the processes that run on the computers, so they make use of all the available resources. If you have a process that’s CPU intensive and takes almost all the available CPU on a computer, you can still run processes that are IO intensive, reading and writing a lot of data to the hard drive. Or if you have a service that requires a lot of RAM, you can pair it with another one that uses very little memory, and mostly sends and receives data over the network.</p>
<p>An alternative for having to deal with all these resources like figuring out when to buy more and how to distribute them, is to migrate those systems to the Cloud. Setting up your service to run on the cloud will require some initial setup time, as well as an ongoing cost for the Cloud resources you’re using. But while this is more expensive than what you’d pay when running the service on premise, you’re basically delegating all your capacity planning needs to your Cloud provider. That way, if the initial setup doesn’t have enough space, you can simply attach a bigger hard drive. Or if the program needs more RAM, you can just deploy the service in a virtual machine with more memory assigned.</p>
<p>If you decide that moving to the Cloud is a good way to go for your company, remember that you’ll also need to plan for that. Migrating your services to run fully or even partially in the Cloud, requires work on your side. So you’ll need to decide if and when to make the leap, to avoid your service having an outage because it ran out of resources.</p>
<p>Still with us, you’re doing great. We’re coming towards the end of the module, and I hope you are as proud as we are of the progress you’ve made. Your thinking ahead and learning some seriously important skills that will serve you well up there in the wild, which</p>

    </div>
  </div>
</body>

</html>
